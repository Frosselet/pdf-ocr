{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf-ocr: Capabilities Walkthrough\n",
    "\n",
    "You have documents. You need data.\n",
    "\n",
    "This notebook walks through the complete transformation pipeline — from raw PDF/DOCX\n",
    "to clean, typed DataFrames — one step at a time. Each section builds on the previous,\n",
    "showing how the library turns messy, unstructured documents into structured records.\n",
    "\n",
    "**Story arc:** The Problem → Spatial Reconstruction → Compression → Structure Detection → Classification → Schema Definition → LLM Interpretation → Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — file paths and imports\n",
    "from pathlib import Path\n",
    "\n",
    "# PDF inputs\n",
    "PDF_2857439    = \"inputs/2857439.pdf\"\n",
    "PDF_CBH        = \"inputs/CBH Shipping Stem 26092025.pdf\"\n",
    "PDF_BUNGE      = \"inputs/Bunge_loadingstatement_2025-09-25.pdf\"\n",
    "PDF_GRAINCORP  = \"inputs/shipping-stem-2025-11-13.pdf\"\n",
    "\n",
    "# DOCX inputs\n",
    "DOCX_HSPAN     = \"inputs/docx/synthetic/hspan.docx\"\n",
    "DOCX_MULTI     = \"inputs/docx/synthetic/multi_category.docx\"\n",
    "DOCX_JULY      = \"inputs/docx/input/2025-07-17_10-16-25.Russian weekly grain EOW July 11-12 2025-1.docx\"\n",
    "\n",
    "print(\"Paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers — render document pages and side-by-side comparisons\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG image.\n",
    "\n",
    "    Supports PDF (via fitz) and DOCX (via LibreOffice → PDF → fitz).\n",
    "    Returns (base64_png, page_count).\n",
    "    \"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: The Problem\n",
    "\n",
    "Standard text extraction tools produce jumbled output from tabular documents.\n",
    "PDFs store text in arbitrary file order. DOCX merged cells create duplicate values.\n",
    "The result: unusable text that can't be parsed into structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF: fitz.get_text() produces text in file storage order — not reading order\n",
    "import fitz\n",
    "\n",
    "doc = fitz.open(PDF_2857439)\n",
    "raw = doc[0].get_text()\n",
    "doc.close()\n",
    "\n",
    "print(f\"Raw text length: {len(raw)} chars\")\n",
    "print(\"First 600 chars (note the jumbled column order):\")\n",
    "print(raw[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX: python-docx returns duplicate _tc references for horizontally merged cells\n",
    "from docx import Document\n",
    "\n",
    "doc = Document(DOCX_HSPAN)\n",
    "table = doc.tables[0]\n",
    "\n",
    "print(f\"Table: {len(table.rows)} rows x {len(table.columns)} columns\")\n",
    "print(\"\\nRaw cell text (note duplicates from merged cells):\")\n",
    "for i, row in enumerate(table.rows):\n",
    "    cells = [c.text for c in row.cells]\n",
    "    print(f\"  Row {i}: {cells}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Spatial Reconstruction\n",
    "\n",
    "The first step is format-specific: reconstruct the visual layout from the raw document.\n",
    "\n",
    "- **PDF**: Project every text span onto a monospace character grid using (x, y) coordinates\n",
    "- **DOCX**: Deduplicate merged `_tc` references and stack multi-row headers with ` / ` separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF: Spatial text preserves the visual layout as a monospace grid\n",
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "spatial = pdf_to_spatial_text(PDF_2857439)\n",
    "print(f\"Spatial text: {len(spatial)} chars, {len(spatial.splitlines())} lines\")\n",
    "print()\n",
    "# Show first 30 lines — columns are aligned as they appear in the PDF\n",
    "for line in spatial.splitlines()[:30]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX: Extract tables with proper merge handling and compound headers\n",
    "from pdf_ocr import extract_tables_from_docx\n",
    "\n",
    "tables = extract_tables_from_docx(DOCX_HSPAN)\n",
    "t = tables[0]\n",
    "print(f\"hspan.docx: {len(t.column_names)} columns, {len(t.data)} data rows\")\n",
    "print(f\"Source format: {t.source_format}\")\n",
    "print(f\"\\nCompound column names (stacked with ' / '):\")\n",
    "for col in t.column_names:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF page image vs spatial text reconstruction\n",
    "img_b64, _ = render_document_page(PDF_2857439, page=0)\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Compression\n",
    "\n",
    "Spatial text is verbose — it preserves every whitespace character. Compression analyzes\n",
    "the page structure and renders each region in its optimal format:\n",
    "\n",
    "- **Tables** → pipe-delimited markdown\n",
    "- **Text blocks** → flowing paragraphs\n",
    "- **Key-value pairs** → `key: value` lines\n",
    "\n",
    "From this point forward, both PDF and DOCX produce the **same pipe-table format**.\n",
    "All downstream processing (classification, interpretation, serialization) is format-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF compression without header refinement (deterministic, no LLM call)\n",
    "from pdf_ocr import compress_spatial_text\n",
    "\n",
    "compressed_raw = compress_spatial_text(PDF_2857439, refine_headers=False)\n",
    "print(f\"Compressed (no refinement): {len(compressed_raw)} chars\")\n",
    "print()\n",
    "for line in compressed_raw.splitlines()[:20]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF compression WITH header refinement (calls LLM to fix stacked headers)\n",
    "compressed = compress_spatial_text(PDF_2857439, refine_headers=True)\n",
    "print(f\"Compressed (refined): {len(compressed)} chars\")\n",
    "print(f\"Compression ratio: {len(spatial) / len(compressed):.1f}x vs spatial text\")\n",
    "print()\n",
    "for line in compressed.splitlines()[:20]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX compression produces the SAME pipe-table format\n",
    "from pdf_ocr import compress_docx_tables\n",
    "\n",
    "docx_results = compress_docx_tables(DOCX_HSPAN)\n",
    "docx_md, docx_meta = docx_results[0]\n",
    "print(f\"DOCX pipe-table: {docx_meta['row_count']} rows x {docx_meta['col_count']} cols\")\n",
    "print()\n",
    "print(docx_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression ratio comparison across 4 PDFs\n",
    "pdfs = {\n",
    "    \"Newcastle (2857439)\": PDF_2857439,\n",
    "    \"CBH\": PDF_CBH,\n",
    "    \"Bunge\": PDF_BUNGE,\n",
    "    \"GrainCorp (3pp)\": PDF_GRAINCORP,\n",
    "}\n",
    "\n",
    "print(f\"{'Document':<22s} {'Spatial':>8s} {'Compressed':>11s} {'Ratio':>6s}\")\n",
    "print(\"-\" * 50)\n",
    "for name, path in pdfs.items():\n",
    "    sp = pdf_to_spatial_text(path)\n",
    "    cp = compress_spatial_text(path, refine_headers=False)\n",
    "    ratio = len(sp) / len(cp) if len(cp) > 0 else 0\n",
    "    print(f\"{name:<22s} {len(sp):>7,d}c {len(cp):>10,d}c {ratio:>5.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Structure Detection Heuristics\n",
    "\n",
    "Before compression, the library runs three layers of heuristics to detect table structure:\n",
    "\n",
    "- **Text heuristics (TH)**: Cell type classification, header row estimation\n",
    "- **Visual heuristics (VH)**: Grid lines, header fills, zebra striping, section separators\n",
    "- **Font heuristics (FH)**: Size hierarchy, bold patterns, monospace detection\n",
    "\n",
    "Results are cross-validated: agreements strengthen confidence, contradictions trigger fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual and font heuristics on CBH (has bold headers, visual grid)\n",
    "from pdf_ocr import compress_spatial_text_structured\n",
    "from pdf_ocr.compress import _analyze_visual_structure, _analyze_font_structure\n",
    "from pdf_ocr.spatial_text import _extract_page_layout\n",
    "import fitz\n",
    "\n",
    "# Get the page layout with visual elements and font spans\n",
    "doc = fitz.open(PDF_CBH)\n",
    "layout = _extract_page_layout(doc[0], extract_visual=True)\n",
    "doc.close()\n",
    "\n",
    "# Run visual heuristics (VH1-VH4)\n",
    "row_y = list(layout.row_y_positions.values()) if layout.row_y_positions else []\n",
    "vis = _analyze_visual_structure(layout.visual, row_y)\n",
    "\n",
    "print(\"=== Visual Heuristics (CBH) ===\")\n",
    "if vis.grid:\n",
    "    print(f\"  VH1 Grid: {vis.grid.h_line_count} h-lines, {vis.grid.v_line_count} v-lines, has_grid={vis.grid.has_grid}\")\n",
    "if vis.header:\n",
    "    print(f\"  VH2 Header fill: {len(vis.header.header_fill_rows)} rows highlighted\")\n",
    "if vis.zebra:\n",
    "    print(f\"  VH3 Zebra: rows {vis.zebra.zebra_start_row}-{vis.zebra.zebra_end_row}\")\n",
    "if vis.separators:\n",
    "    print(f\"  VH4 Separators: {len(vis.separators.separator_y_positions)} section breaks\")\n",
    "\n",
    "# Run font heuristics (FH1-FH6)\n",
    "font = _analyze_font_structure(layout.font_spans, row_y, header_row_estimate=2)\n",
    "\n",
    "print(\"\\n=== Font Heuristics (CBH) ===\")\n",
    "if font.hierarchy:\n",
    "    print(f\"  FH1 Size tiers: {len(font.hierarchy.size_tiers)} tiers\")\n",
    "    print(f\"       Header size: {font.hierarchy.header_size}, Body size: {font.hierarchy.body_size}\")\n",
    "if font.bold:\n",
    "    print(f\"  FH2 Bold: header={font.bold.header_bold_ratio:.0%}, data={font.bold.data_bold_ratio:.0%}\")\n",
    "if font.monospace:\n",
    "    print(f\"  FH4 Monospace columns: {font.monospace.monospace_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Classification\n",
    "\n",
    "When a document contains multiple tables, classification assigns each table to a\n",
    "category using keyword matching against header text. This is format-agnostic — both\n",
    "PDF and DOCX tables are classified using the same `classify_tables()` function\n",
    "operating on pipe-table markdown + metadata tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX classification: multi_category.docx has 4 different table types\n",
    "from pdf_ocr import classify_docx_tables\n",
    "\n",
    "categories = {\n",
    "    \"shipping\": [\"cargo\", \"port\", \"vessel\"],\n",
    "    \"hr\": [\"employee\", \"salary\", \"department\"],\n",
    "    \"finance\": [\"revenue\", \"expenses\"],\n",
    "    \"inventory\": [\"stock\", \"warehouse\"],\n",
    "}\n",
    "\n",
    "classes = classify_docx_tables(DOCX_MULTI, categories)\n",
    "print(f\"multi_category.docx: {len(classes)} tables classified\")\n",
    "for c in classes:\n",
    "    print(f\"  Table {c['index']}: {c['category']:12s} title={c['title'] or '(none)':25s} {c['rows']}r x {c['cols']}c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF classification: CBH has side-by-side tables → StructuredTable.to_compressed()\n",
    "from pdf_ocr import classify_tables\n",
    "\n",
    "structured_tables = compress_spatial_text_structured(PDF_CBH)\n",
    "compressed_tuples = [t.to_compressed() for t in structured_tables]\n",
    "\n",
    "shipping_cats = {\n",
    "    \"shipping\": [\"vessel\", \"ship\", \"cargo\", \"commodity\", \"eta\", \"port\", \"loading\"],\n",
    "}\n",
    "\n",
    "cbh_classes = classify_tables(compressed_tuples, shipping_cats)\n",
    "print(f\"CBH PDF: {len(structured_tables)} tables found, {len(cbh_classes)} classified\")\n",
    "for c in cbh_classes:\n",
    "    print(f\"  Table {c['index']}: {c['category']:12s} {c['rows']}r x {c['cols']}c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world DOCX: Russian July report — harvest, planting, and export tables\n",
    "ag_categories = {\n",
    "    \"harvest\":  [\"area harvested\", \"yield\", \"collected\", \"bunker\", \"centner\"],\n",
    "    \"planting\": [\"spring crops\", \"moa target\", \"sown area\", \"planting\", \"sowing\"],\n",
    "    \"export\":   [\"export\", \"shipment\", \"ports\", \"fob\"],\n",
    "}\n",
    "\n",
    "july_classes = classify_docx_tables(DOCX_JULY, ag_categories)\n",
    "counts = {}\n",
    "for c in july_classes:\n",
    "    counts[c[\"category\"]] = counts.get(c[\"category\"], 0) + 1\n",
    "print(f\"Russian July report: {len(july_classes)} tables\")\n",
    "print(f\"  By category: {dict(sorted(counts.items()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Schema Definition\n",
    "\n",
    "A `CanonicalSchema` tells the LLM what output structure you want. Each `ColumnDef`\n",
    "specifies a column name, type, description, and aliases (alternative names the LLM\n",
    "should recognize in the source table). The schema is domain-specific — you define it\n",
    "once and reuse it across any number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import CanonicalSchema, ColumnDef\n",
    "\n",
    "# Define a shipping schema — works for any Australian shipping stem PDF\n",
    "shipping_schema = CanonicalSchema.from_dict({\n",
    "    \"description\": \"Vessel loading records by port\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\",    \"type\": \"string\", \"description\": \"Loading port name\",\n",
    "         \"aliases\": [\"Port\", \"PORT\", \"Loading Port\"]},\n",
    "        {\"name\": \"vessel_name\",  \"type\": \"string\", \"description\": \"Name of the vessel\",\n",
    "         \"aliases\": [\"Ship Name\", \"NAME OF SHIP\", \"Vessel Name\", \"Vessel\"]},\n",
    "        {\"name\": \"shipper\",      \"type\": \"string\", \"description\": \"Exporting company\",\n",
    "         \"aliases\": [\"Exporter\", \"EXPORTER\", \"Client\", \"Shipper\"]},\n",
    "        {\"name\": \"commodity\",    \"type\": \"string\", \"description\": \"Type of commodity\",\n",
    "         \"aliases\": [\"Commodity\", \"COMMODITY\", \"Cargo\"]},\n",
    "        {\"name\": \"tons\",         \"type\": \"int\",    \"description\": \"Quantity in metric tonnes\",\n",
    "         \"aliases\": [\"Quantity (tonnes)\", \"QUANTITY (TONNES)\", \"Volume\", \"Tonnes\"]},\n",
    "        {\"name\": \"eta\",          \"type\": \"string\", \"description\": \"Estimated time of arrival\",\n",
    "         \"aliases\": [\"ETA\", \"Date ETA of Ship To\"]},\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(f\"Schema: {shipping_schema.description}\")\n",
    "print(f\"Columns ({len(shipping_schema.columns)}):\")\n",
    "for col in shipping_schema.columns:\n",
    "    print(f\"  {col.name:15s} {col.type:6s}  aliases={col.aliases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Section 7: Interpretation\n\nThe interpretation pipeline uses a **deterministic-first** architecture:\n\n1. **Deterministic mapping** (no LLM): Split each header on ` / `, match parts against\n   schema aliases. If every part matches, build records via string matching — zero LLM calls.\n   Handles unpivoting, sections, and dimension/measure classification purely from aliases.\n\n2. **LLM fallback** (2-step pipeline): Only for pages with unmatched columns.\n   - **Step 1 — Parse**: Analyze table structure, detect headers, merge multi-row records\n   - **Step 2 — Map**: Map parsed columns to canonical schema using aliases and descriptions\n\nMulti-page documents are auto-split and processed concurrently. Step 2 is batched\n(default 20 rows per batch) to prevent LLM truncation on dense pages.\n\n**When does deterministic mapping fire?** Whenever the schema aliases fully cover\nevery header part. This is typical for DOCX compound headers (`spring crops / 2025`)\nand PDF hierarchical headers (`BATTERY ELECTRIC / Dec-25`) when the contract has\ncomplete aliases."
  },
  {
   "cell_type": "code",
   "source": "# Deterministic mapping demo — zero LLM calls\nfrom pdf_ocr.interpret import _try_deterministic\n\n# A Russian-style compound header table\ndemo_text = \"\"\"## PLANTING\n| Region | spring crops / MOA Target 2025 | spring crops / 2025 | spring grain / MOA Target 2025 | spring grain / 2025 |\n|---|---|---|---|---|\n| Belgorod | 100 | 90 | 50 | 45 |\n| Bryansk | 80 | 70 | 40 | 35 |\"\"\"\n\ndemo_schema = CanonicalSchema(columns=[\n    ColumnDef(\"region\", \"string\", \"Region\", aliases=[\"Region\"]),\n    ColumnDef(\"area\", \"float\", \"Target area\", aliases=[\"MOA Target 2025\"]),\n    ColumnDef(\"value\", \"float\", \"Actual area\", aliases=[\"2025\"]),\n    ColumnDef(\"crop\", \"string\", \"Crop type\", aliases=[\"spring crops\", \"spring grain\"]),\n])\n\ndet_result = _try_deterministic(demo_text, demo_schema)\nprint(f\"Deterministic result: {len(det_result.records)} records (0 LLM calls)\")\nprint(f\"Model: {det_result.metadata.model}\")\nprint(f\"Strategy: {det_result.metadata.table_type_inference.mapping_strategy_used}\")\nprint()\nfor r in det_result.records:\n    print(f\"  region={r.region:10s} crop={r.crop:15s} area={r.area:>4s} value={r.value}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single PDF: interpret_table() on Newcastle shipping stem\n",
    "import time\n",
    "from pdf_ocr import interpret_table, to_records, to_pandas\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "result = interpret_table(compressed, shipping_schema, model=\"openai/gpt-4o\")\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "records = to_records(result)\n",
    "print(f\"Newcastle: {len(records)} records in {elapsed:.1f}s\")\n",
    "print(f\"Pages: {sorted(result.keys())}\")\n",
    "print(f\"\\nFirst 3 records:\")\n",
    "for r in records[:3]:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF page | compressed text | DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df_newcastle = to_pandas(result, shipping_schema)\n",
    "img_b64, _ = render_document_page(PDF_2857439, page=0)\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=compressed, dataframe=df_newcastle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-page PDF: GrainCorp (3 pages, ~180 records, concurrent processing)\n",
    "gc_compressed = compress_spatial_text(PDF_GRAINCORP, refine_headers=True)\n",
    "pages = gc_compressed.split(\"\\f\")\n",
    "print(f\"GrainCorp: {len(pages)} pages\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "gc_result = interpret_table(gc_compressed, shipping_schema, model=\"openai/gpt-4o\")\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "gc_records = to_records(gc_result)\n",
    "print(f\"Total records: {len(gc_records)} in {elapsed:.1f}s (all pages concurrent)\")\n",
    "for page_num, mapped in sorted(gc_result.items()):\n",
    "    print(f\"  Page {page_num}: {len(mapped.records)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCX: interpret multiple harvest tables from Russian July report concurrently\n",
    "from pdf_ocr import interpret_tables, extract_pivot_values\n",
    "\n",
    "harvest_idx = [c[\"index\"] for c in july_classes if c[\"category\"] == \"harvest\"]\n",
    "july_results = compress_docx_tables(DOCX_JULY, table_indices=harvest_idx)\n",
    "\n",
    "# Build harvest schema with dynamic year aliases\n",
    "pivot_years = extract_pivot_values(july_results[0][0])\n",
    "harvest_schema = CanonicalSchema.from_dict({\n",
    "    \"description\": \"Harvest progress by region, metric, and year\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"region\",  \"type\": \"string\", \"aliases\": [\"Region\"]},\n",
    "        {\"name\": \"metric\",  \"type\": \"string\", \"aliases\": [\"Area harvested\", \"collected\", \"Yield\"]},\n",
    "        {\"name\": \"year\",    \"type\": \"int\",    \"aliases\": pivot_years[-2:]},\n",
    "        {\"name\": \"value\",   \"type\": \"float\",  \"aliases\": []},\n",
    "    ],\n",
    "})\n",
    "print(f\"Year aliases from headers: {pivot_years[-2:]}\")\n",
    "\n",
    "texts = [md for md, _ in july_results]\n",
    "titles = [meta[\"title\"] or \"Unknown\" for _, meta in july_results]\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "mapped_tables = interpret_tables(texts, harvest_schema, model=\"openai/gpt-4o\")\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "frames = []\n",
    "for title, mapped in zip(titles, mapped_tables):\n",
    "    df = to_pandas(mapped, harvest_schema)\n",
    "    df[\"crop\"] = title\n",
    "    frames.append(df)\n",
    "    print(f\"  {title}: {len(df)} records\")\n",
    "\n",
    "df_july = pd.concat(frames, ignore_index=True)\n",
    "print(f\"\\nTotal July harvest: {len(df_july)} records ({elapsed:.1f}s concurrent)\")\n",
    "df_july.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Serialization\n",
    "\n",
    "Typed export to CSV, TSV, Parquet, pandas, and polars. The serializer validates records\n",
    "against the schema using Pydantic, with automatic coercion of OCR artifacts\n",
    "(e.g., `\"1,234\"` → `1234`, `\"(500)\"` → `-500`). Output formatting is driven by\n",
    "`ColumnDef.format` — date patterns, number patterns, string case transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## Summary\n\n| Step | Function | Input | Output | Format-Specific? |\n|------|----------|-------|--------|-------------------|\n| 1. Extract | `pdf_to_spatial_text()` / `extract_tables_from_docx()` | Raw document | Spatial text / StructuredTable | Yes |\n| 2. Compress | `compress_spatial_text()` / `compress_docx_tables()` | Spatial text / StructuredTable | Pipe-table markdown (` / ` headers) | Yes (entry point) |\n| 3. Classify | `classify_tables()` / `classify_docx_tables()` | Pipe-tables + keywords | Category assignments | No |\n| 4. Define | `CanonicalSchema.from_dict()` | Column definitions | Schema object | No |\n| 5. Interpret | `interpret_table()` / `interpret_tables()` | Pipe-table + schema | MappedTable records | No |\n| 6. Serialize | `to_csv()` / `to_pandas()` / `to_parquet()` | MappedTable + schema | CSV / DataFrame / Parquet | No |\n\nSteps 1-2 are format-specific. Steps 3-6 are completely format-agnostic — the same\ncode works identically for PDF, DOCX, XLSX, PPTX, and HTML inputs.\n\nStep 5 uses a **deterministic-first** architecture: when schema aliases fully cover\nevery ` / `-separated header part, records are built via string matching with zero LLM\ncalls. The LLM pipeline activates only as a fallback for unmatched columns."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | Function | Input | Output | Format-Specific? |\n",
    "|------|----------|-------|--------|-------------------|\n",
    "| 1. Extract | `pdf_to_spatial_text()` / `extract_tables_from_docx()` | Raw document | Spatial text / StructuredTable | Yes |\n",
    "| 2. Compress | `compress_spatial_text()` / `compress_docx_tables()` | Spatial text / StructuredTable | Pipe-table markdown | Yes (entry point) |\n",
    "| 3. Classify | `classify_tables()` / `classify_docx_tables()` | Pipe-tables + keywords | Category assignments | No |\n",
    "| 4. Define | `CanonicalSchema.from_dict()` | Column definitions | Schema object | No |\n",
    "| 5. Interpret | `interpret_table()` / `interpret_tables()` | Pipe-table + schema | MappedTable records | No |\n",
    "| 6. Serialize | `to_csv()` / `to_pandas()` / `to_parquet()` | MappedTable + schema | CSV / DataFrame / Parquet | No |\n",
    "\n",
    "Steps 1-2 are format-specific. Steps 3-6 are completely format-agnostic — the same\n",
    "code works identically for PDF, DOCX, XLSX, PPTX, and HTML inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}