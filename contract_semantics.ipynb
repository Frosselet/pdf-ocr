{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Semantics Toolkit\n",
    "\n",
    "Ontology-grounded contract authoring and validation for docpact. Demonstrates\n",
    "document analysis, contract recommendation, alias resolution (AGROVOC + GeoNames),\n",
    "contract materialization, and SHACL validation — all with real documents and contracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Design\n\nThe `contract-semantics` toolkit sits upstream of docpact: it helps **author** contracts,\nnot execute them. The contract JSON remains the sole interface between the two packages.\n\n**Authoring workflow**: analyze raw documents → recommend a contract skeleton → enrich with\nontology aliases (AGROVOC for crops/metrics, GeoNames for regions) → diff alias\ncoverage → materialize the final contract → validate output records against SHACL shapes.\n\n**Runtime workflow**: `build_semantic_context()` resolves all concept URIs in an annotated\ncontract and produces a `SemanticContext` — a plain dataclass that the docpact pipeline\nconsumes for runtime alias enrichment, pre-flight header checks, and post-extraction\nvalue validation. docpact never imports from `contract_semantics` — the `SemanticContext`\nis the sole interface.\n\nThis notebook walks through each step using the Russian agriculture contract\n(`ru_ag_ministry.json`) and Australian shipping stem contract (`au_shipping_stem.json`)\nas real-world examples. Ontology adapters use small in-memory fixtures for offline\nreproducibility — in production, use `contract-semantics fetch-agrovoc` and\n`contract-semantics fetch-geonames` to download full datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport json\nimport shutil\nfrom pathlib import Path\n\nsys.path.insert(0, \"tools/contract-semantics/src\")\n\n# Ensure dependencies are available (uv-managed venv — no pip)\ndef _ensure_pkg(import_name, pip_name):\n    try:\n        __import__(import_name)\n    except ImportError:\n        import subprocess\n        uv = shutil.which(\"uv\")\n        if uv:\n            subprocess.check_call([uv, \"pip\", \"install\", pip_name])\n        else:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n\n_ensure_pkg(\"rdflib\", \"rdflib>=7.0,<8\")\n_ensure_pkg(\"docx\", \"python-docx\")\n_ensure_pkg(\"pyshacl\", \"pyshacl>=0.26\")\n\nimport rdflib\nprint(f\"rdflib {rdflib.__version__}\")\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build small AGROVOC + GeoNames fixtures for offline testing.\n",
    "# In production: `contract-semantics fetch-agrovoc` / `contract-semantics fetch-geonames`\n",
    "\n",
    "from rdflib import Graph, Literal, Namespace\n",
    "from rdflib.namespace import SKOS\n",
    "\n",
    "AGROVOC = Namespace(\"http://aims.fao.org/aos/agrovoc/\")\n",
    "g = Graph()\n",
    "\n",
    "# All crops referenced in ru_ag_ministry.json\n",
    "for uri_frag, en, ru, alts in [\n",
    "    (\"c_8373\",  \"wheat\",      \"\\u043f\\u0448\\u0435\\u043d\\u0438\\u0446\\u0430\",      [\"common wheat\"]),\n",
    "    (\"c_631\",   \"barley\",     \"\\u044f\\u0447\\u043c\\u0435\\u043d\\u044c\",        [\"barleycorn\"]),\n",
    "    (\"c_12332\", \"maize\",      \"\\u043a\\u0443\\u043a\\u0443\\u0440\\u0443\\u0437\\u0430\",      [\"corn\"]),\n",
    "    (\"c_6599\",  \"rice\",       \"\\u0440\\u0438\\u0441\",           []),\n",
    "    (\"c_1094\",  \"buckwheat\",  \"\\u0433\\u0440\\u0435\\u0447\\u0438\\u0445\\u0430\",       []),\n",
    "    (\"c_7440\",  \"sunflowers\", \"\\u043f\\u043e\\u0434\\u0441\\u043e\\u043b\\u043d\\u0435\\u0447\\u043d\\u0438\\u043a\",  [\"sunflower\"]),\n",
    "    (\"c_14477\", \"soyabeans\",  \"\\u0441\\u043e\\u0435\\u0432\\u044b\\u0435 \\u0431\\u043e\\u0431\\u044b\",   [\"soya\", \"soybean\"]),\n",
    "    (\"c_6476\",  \"rapeseed\",   \"\\u0440\\u0430\\u043f\\u0441\",          [\"canola\", \"rape\"]),\n",
    "]:\n",
    "    c = AGROVOC[uri_frag]\n",
    "    g.add((c, SKOS.prefLabel, Literal(en, lang=\"en\")))\n",
    "    g.add((c, SKOS.prefLabel, Literal(ru, lang=\"ru\")))\n",
    "    for alt in alts:\n",
    "        g.add((c, SKOS.altLabel, Literal(alt, lang=\"en\")))\n",
    "\n",
    "# Wheat narrower: durum wheat\n",
    "durum = AGROVOC[\"c_33556\"]\n",
    "g.add((AGROVOC[\"c_8373\"], SKOS.narrower, durum))\n",
    "g.add((durum, SKOS.prefLabel, Literal(\"durum wheat\", lang=\"en\")))\n",
    "g.add((durum, SKOS.prefLabel, Literal(\"\\u0442\\u0432\\u0451\\u0440\\u0434\\u0430\\u044f \\u043f\\u0448\\u0435\\u043d\\u0438\\u0446\\u0430\", lang=\"ru\")))\n",
    "\n",
    "# Harvest metrics\n",
    "for uri_frag, en, ru, alts in [\n",
    "    (\"c_330918\", \"area harvested\", \"\\u0443\\u0431\\u0440\\u0430\\u043d\\u043d\\u0430\\u044f \\u043f\\u043b\\u043e\\u0449\\u0430\\u0434\\u044c\", []),\n",
    "    (\"c_5765\",   \"crop yield\",     \"\\u0443\\u0440\\u043e\\u0436\\u0430\\u0439\\u043d\\u043e\\u0441\\u0442\\u044c\",     [\"yield\"]),\n",
    "    (\"c_3486\",   \"harvesting\",     \"\\u0443\\u0431\\u043e\\u0440\\u043a\\u0430 \\u0443\\u0440\\u043e\\u0436\\u0430\\u044f\",   [\"collected\"]),\n",
    "]:\n",
    "    c = AGROVOC[uri_frag]\n",
    "    g.add((c, SKOS.prefLabel, Literal(en, lang=\"en\")))\n",
    "    g.add((c, SKOS.prefLabel, Literal(ru, lang=\"ru\")))\n",
    "    for alt in alts:\n",
    "        g.add((c, SKOS.altLabel, Literal(alt, lang=\"en\")))\n",
    "\n",
    "agrovoc_graph = g\n",
    "print(f\"AGROVOC fixture: {len(g)} triples\")\n",
    "\n",
    "# GeoNames mock: 3 Russian ADM1 regions\n",
    "geonames_features = {\n",
    "    524894: {\"geoname_id\": 524894, \"name\": \"Moscow Oblast\", \"asciiname\": \"Moscow Oblast\",\n",
    "             \"alternatenames\": \"Moskovskaya Oblast\", \"lat\": 55.75, \"lng\": 37.62,\n",
    "             \"feature_class\": \"A\", \"feature_code\": \"ADM1\", \"country_code\": \"RU\",\n",
    "             \"admin1_code\": \"48\", \"population\": 7500000},\n",
    "    472039: {\"geoname_id\": 472039, \"name\": \"Voronezh Oblast\", \"asciiname\": \"Voronezh Oblast\",\n",
    "             \"alternatenames\": \"Voronezhskaya Oblast\", \"lat\": 51.67, \"lng\": 39.21,\n",
    "             \"feature_class\": \"A\", \"feature_code\": \"ADM1\", \"country_code\": \"RU\",\n",
    "             \"admin1_code\": \"86\", \"population\": 2300000},\n",
    "    511180: {\"geoname_id\": 511180, \"name\": \"Penza Oblast\", \"asciiname\": \"Penza Oblast\",\n",
    "             \"alternatenames\": \"Penzenskaya Oblast\", \"lat\": 53.12, \"lng\": 44.10,\n",
    "             \"feature_class\": \"A\", \"feature_code\": \"ADM1\", \"country_code\": \"RU\",\n",
    "             \"admin1_code\": \"57\", \"population\": 1300000},\n",
    "}\n",
    "geonames_alt_names = {\n",
    "    524894: [(\"ru\", \"\\u041c\\u043e\\u0441\\u043a\\u043e\\u0432\\u0441\\u043a\\u0430\\u044f \\u043e\\u0431\\u043b\\u0430\\u0441\\u0442\\u044c\"), (\"en\", \"Moscow Region\")],\n",
    "    472039: [(\"ru\", \"\\u0412\\u043e\\u0440\\u043e\\u043d\\u0435\\u0436\\u0441\\u043a\\u0430\\u044f \\u043e\\u0431\\u043b\\u0430\\u0441\\u0442\\u044c\")],\n",
    "    511180: [(\"ru\", \"\\u041f\\u0435\\u043d\\u0437\\u0435\\u043d\\u0441\\u043a\\u0430\\u044f \\u043e\\u0431\\u043b\\u0430\\u0441\\u0442\\u044c\")],\n",
    "}\n",
    "print(f\"GeoNames fixture: {len(geonames_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from contract_semantics.agrovoc import AgrovocAdapter\nfrom contract_semantics.geonames import GeoNamesAdapter\nfrom contract_semantics.models import ConceptRef, ResolveConfig\nfrom contract_semantics.resolve import resolve_column\nfrom contract_semantics.diff import diff_aliases, diff_all\nfrom contract_semantics.materialize import materialize_contract\nfrom contract_semantics.context import build_semantic_context\nfrom contract_semantics.validate import generate_shapes, validate_records, records_to_graph\n\nagrovoc = AgrovocAdapter(graph=agrovoc_graph)\ngeonames = GeoNamesAdapter(features=geonames_features, alternate_names=geonames_alt_names)\n\nprint(\"Contract-semantics modules loaded.\")\nprint(f\"  AgrovocAdapter: offline ({len(agrovoc_graph)} triples)\")\nprint(f\"  GeoNamesAdapter: offline ({len(geonames_features)} regions)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Ontology Resolution: AGROVOC\n",
    "\n",
    "AGROVOC is FAO's multilingual agricultural thesaurus. The contract-semantics toolkit\n",
    "resolves `concept_uris` in contracts to multilingual labels (`skos:prefLabel`,\n",
    "`skos:altLabel`) with optional `skos:narrower` traversal.\n",
    "\n",
    "The Russian agriculture contract uses AGROVOC for two column types:\n",
    "- **Metric** (harvest output): `area harvested`, `crop yield`, `harvesting`\n",
    "- **Crop** (planting output): wheat, barley, maize, rice, buckwheat, sunflowers, soyabeans, rapeseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve wheat — multilingual labels from AGROVOC\n",
    "wheat_aliases = agrovoc.resolve_concept(\n",
    "    \"http://aims.fao.org/aos/agrovoc/c_8373\",\n",
    "    languages=[\"en\", \"ru\"],\n",
    "    label_types=[\"prefLabel\", \"altLabel\"],\n",
    ")\n",
    "\n",
    "print(\"Wheat (c_8373) — resolved aliases:\")\n",
    "for a in wheat_aliases:\n",
    "    print(f\"  [{a.language}] {a.label_type}: {a.alias}\")\n",
    "\n",
    "# With narrower traversal — discovers durum wheat\n",
    "wheat_deep = agrovoc.resolve_concept(\n",
    "    \"http://aims.fao.org/aos/agrovoc/c_8373\",\n",
    "    languages=[\"en\", \"ru\"],\n",
    "    label_types=[\"prefLabel\", \"altLabel\"],\n",
    "    include_narrower=True,\n",
    "    narrower_depth=1,\n",
    ")\n",
    "\n",
    "print(f\"\\nWith narrower: {len(wheat_deep)} aliases (vs {len(wheat_aliases)} without)\")\n",
    "known = {a.alias for a in wheat_aliases}\n",
    "for a in wheat_deep:\n",
    "    if a.alias not in known:\n",
    "        print(f\"  [narrower] [{a.language}] {a.label_type}: {a.alias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve the Crop column from the planting contract.\n",
    "# prefix_patterns: [\"spring {label}\", \"{label}\"] multiply each resolved label,\n",
    "# generating \"spring wheat\", \"spring barley\" etc. alongside bare \"wheat\", \"barley\".\n",
    "\n",
    "crop_refs = [\n",
    "    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_8373\", label=\"wheat\"),\n",
    "    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_631\", label=\"barley\"),\n",
    "    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_12332\", label=\"maize\"),\n",
    "]\n",
    "\n",
    "crop_config = ResolveConfig(\n",
    "    languages=[\"en\", \"ru\"],\n",
    "    label_types=[\"prefLabel\", \"altLabel\"],\n",
    "    prefix_patterns=[\"spring {label}\", \"{label}\"],\n",
    ")\n",
    "\n",
    "crop_manual = [\"spring wheat\", \"spring barley\", \"corn\", \"grains and grasses\"]\n",
    "\n",
    "crop_result = resolve_column(\"Crop\", crop_refs, crop_manual, crop_config, agrovoc)\n",
    "\n",
    "print(f\"Crop column resolution:\")\n",
    "print(f\"  Resolved: {len(crop_result.resolved_aliases)} aliases\")\n",
    "print(f\"  Manual:   {len(crop_result.manual_aliases)} aliases\")\n",
    "print(f\"  Matched:  {crop_result.matched}\")\n",
    "print(f\"  Manual-only: {crop_result.manual_only}\")\n",
    "print(f\"  Coverage: {crop_result.coverage:.0%}\")\n",
    "\n",
    "print(f\"\\nResolved aliases (first 15):\")\n",
    "for a in crop_result.resolved_aliases[:15]:\n",
    "    pattern = f\" (pattern: {a.pattern})\" if a.pattern else \"\"\n",
    "    print(f\"  {a.alias}{pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Ontology Resolution: GeoNames\n",
    "\n",
    "GeoNames provides geographic feature metadata. The Russian agriculture contract\n",
    "uses GeoNames for the **Region** column — 78 Russian ADM1 administrative divisions.\n",
    "Resolution produces multilingual alternate names (English + Russian), and enrichment\n",
    "adds lat/lng coordinates for downstream GIS joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve alternate names for Moscow Oblast\n",
    "moscow = geonames.resolve_geoname(524894, languages=[\"en\", \"ru\"])\n",
    "\n",
    "print(\"Moscow Oblast (524894) — alternate names:\")\n",
    "for a in moscow:\n",
    "    print(f\"  [{a.language}] {a.alias}\")\n",
    "\n",
    "# Resolve via concept URI (as used in contracts)\n",
    "voronezh = geonames.resolve_concept(\n",
    "    \"https://sws.geonames.org/472039/\",\n",
    "    languages=[\"en\", \"ru\"],\n",
    ")\n",
    "print(f\"\\nVoronezh Oblast (via URI): {[a.alias for a in voronezh]}\")\n",
    "\n",
    "# Geographic enrichment — lat/lng/admin codes for GIS\n",
    "print(\"\\nGeo enrichment:\")\n",
    "for gid in [524894, 472039, 511180]:\n",
    "    e = geonames.enrich_geoname(gid)\n",
    "    print(f\"  {e.name}: lat={e.lat}, lng={e.lng}, \"\n",
    "          f\"admin1={e.admin1_code}, pop={e.population:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Alias Diff\n",
    "\n",
    "`diff_aliases()` compares ontology-resolved aliases against the manual aliases in the\n",
    "contract. This is the key feedback loop for contract authoring: it shows which manual\n",
    "aliases the ontology can cover automatically, which manual aliases have no ontology\n",
    "equivalent, and which ontology labels are available but not yet in the contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Diff for the Metric column (harvest output)\nmetric_refs = [\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_330918\", label=\"area harvested\"),\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_5765\", label=\"crop yield\"),\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_3486\", label=\"harvesting\"),\n]\nmetric_config = ResolveConfig(languages=[\"en\", \"ru\"], label_types=[\"prefLabel\", \"altLabel\"])\nmetric_manual = [\"Area harvested\", \"collected\", \"Yield\"]\n\nmetric_result = resolve_column(\"Metric\", metric_refs, metric_manual, metric_config, agrovoc)\n\nprint(diff_aliases(metric_result))\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Diff for the Crop column (planting output) — prefix patterns generate \"spring wheat\" etc.\ncrop_refs = [\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_8373\", label=\"wheat\"),\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_631\", label=\"barley\"),\n    ConceptRef(uri=\"http://aims.fao.org/aos/agrovoc/c_12332\", label=\"maize\"),\n]\ncrop_config = ResolveConfig(\n    languages=[\"en\", \"ru\"],\n    label_types=[\"prefLabel\", \"altLabel\"],\n    prefix_patterns=[\"spring {label}\", \"{label}\"],\n)\ncrop_manual = [\"spring wheat\", \"spring barley\", \"corn\", \"grains and grasses\"]\ncrop_result = resolve_column(\"Crop\", crop_refs, crop_manual, crop_config, agrovoc)\n\n# Multi-column diff — Metric + Crop together\nprint(diff_all([metric_result, crop_result]))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Contract Materialization\n",
    "\n",
    "`materialize_contract()` reads an annotated contract (with `concept_uris` and `resolve`\n",
    "blocks), resolves all concept URIs via the appropriate adapters, merges resolved aliases\n",
    "into the alias lists (union/resolved_only/manual_priority), strips annotation fields,\n",
    "and writes a clean contract consumable by docpact's `load_contract()`.\n",
    "\n",
    "For GeoNames columns with `enrich_fields`, it also produces a **geo sidecar** JSON\n",
    "mapping region names to coordinates — enabling post-extraction GIS joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the Russian contract with union merge strategy\n",
    "materialized = materialize_contract(\n",
    "    \"contracts/ru_ag_ministry.json\",\n",
    "    agrovoc=agrovoc,\n",
    "    geonames=geonames,\n",
    "    merge_strategy=\"union\",\n",
    ")\n",
    "\n",
    "# Compare alias counts before/after\n",
    "with open(\"contracts/ru_ag_ministry.json\") as f:\n",
    "    original = json.load(f)\n",
    "\n",
    "print(\"Alias count comparison (harvest output):\")\n",
    "for col_name in [\"Metric\", \"Region\"]:\n",
    "    orig_col = next(c for c in original[\"outputs\"][\"harvest\"][\"schema\"][\"columns\"] if c[\"name\"] == col_name)\n",
    "    mat_col = next(c for c in materialized[\"outputs\"][\"harvest\"][\"schema\"][\"columns\"] if c[\"name\"] == col_name)\n",
    "    orig_n = len(orig_col.get(\"aliases\", []))\n",
    "    mat_n = len(mat_col.get(\"aliases\", []))\n",
    "    new = sorted(set(mat_col.get(\"aliases\", [])) - set(orig_col.get(\"aliases\", [])))\n",
    "    print(f\"  {col_name}: {orig_n} -> {mat_n} aliases (+{len(new)})\")\n",
    "    if new:\n",
    "        print(f\"    new: {new[:8]}{'...' if len(new) > 8 else ''}\")\n",
    "\n",
    "print(\"\\nAlias count comparison (planting output):\")\n",
    "for col_name in [\"Crop\"]:\n",
    "    orig_col = next(c for c in original[\"outputs\"][\"planting\"][\"schema\"][\"columns\"] if c[\"name\"] == col_name)\n",
    "    mat_col = next(c for c in materialized[\"outputs\"][\"planting\"][\"schema\"][\"columns\"] if c[\"name\"] == col_name)\n",
    "    orig_n = len(orig_col.get(\"aliases\", []))\n",
    "    mat_n = len(mat_col.get(\"aliases\", []))\n",
    "    new = sorted(set(mat_col.get(\"aliases\", [])) - set(orig_col.get(\"aliases\", [])))\n",
    "    print(f\"  {col_name}: {orig_n} -> {mat_n} aliases (+{len(new)})\")\n",
    "    if new:\n",
    "        print(f\"    new: {new[:10]}{'...' if len(new) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect materialized contract structure — concept_uris and resolve blocks are stripped\n",
    "print(\"Materialized contract:\")\n",
    "print(f\"  Provider: {materialized['provider']}\")\n",
    "print(f\"  Outputs: {list(materialized['outputs'].keys())}\")\n",
    "\n",
    "for name, spec in materialized[\"outputs\"].items():\n",
    "    cols = spec[\"schema\"][\"columns\"]\n",
    "    print(f\"\\n  {name} ({len(cols)} columns):\")\n",
    "    for c in cols:\n",
    "        aliases = c.get(\"aliases\", [])\n",
    "        src = c.get(\"source\", \"\")\n",
    "        if aliases:\n",
    "            print(f\"    {c['name']} ({c['type']}): {len(aliases)} aliases\")\n",
    "        elif src:\n",
    "            print(f\"    {c['name']} ({c['type']}): source={src}\")\n",
    "\n",
    "# Verify annotation fields are stripped\n",
    "has_concept_uris = any(\n",
    "    \"concept_uris\" in col\n",
    "    for spec in materialized[\"outputs\"].values()\n",
    "    for col in spec[\"schema\"][\"columns\"]\n",
    ")\n",
    "has_resolve = any(\n",
    "    \"resolve\" in col\n",
    "    for spec in materialized[\"outputs\"].values()\n",
    "    for col in spec[\"schema\"][\"columns\"]\n",
    ")\n",
    "print(f\"\\nconcept_uris stripped: {not has_concept_uris}\")\n",
    "print(f\"resolve blocks stripped: {not has_resolve}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Document Structural Analysis\n",
    "\n",
    "`profile_document()` examines a raw PDF or DOCX and extracts structural signals:\n",
    "column names, data types, cardinalities, table layout (flat/transposed/pivoted),\n",
    "section labels, temporal patterns, and unit annotations. These signals feed into\n",
    "the recommendation engine to produce a contract skeleton.\n",
    "\n",
    "This section profiles a real Russian agricultural DOCX report — the same documents\n",
    "that `pipeline.ipynb` processes through the extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contract_semantics.analyze import profile_document, merge_profiles, format_analysis_report\n",
    "\n",
    "# Profile one Russian weekly grain report\n",
    "docx_files = sorted(Path(\"inputs/docx/input\").glob(\"*.docx\"))\n",
    "print(f\"Found {len(docx_files)} DOCX reports:\")\n",
    "for p in docx_files:\n",
    "    print(f\"  {p.name}\")\n",
    "\n",
    "docx_path = docx_files[0]\n",
    "print(f\"\\nProfiling: {docx_path.name}\")\n",
    "ru_profile = profile_document(str(docx_path))\n",
    "\n",
    "print(format_analysis_report(ru_profile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed look at the first 3 table profiles\n",
    "for tp in ru_profile.tables[:3]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Table {tp.table_index}: {tp.title or '(no title)'}\")\n",
    "    print(f\"  Layout: {tp.layout}, {tp.row_count} rows x {tp.col_count} cols\")\n",
    "    if tp.section_labels:\n",
    "        print(f\"  Section labels: {tp.section_labels[:5]}\")\n",
    "    print(f\"  Columns:\")\n",
    "    for cp in tp.column_profiles:\n",
    "        unit = f\" [{cp.unit_annotations[0]}]\" if cp.unit_annotations else \"\"\n",
    "        year = \" (YEAR)\" if cp.year_detected else \"\"\n",
    "        print(f\"    {cp.header_text}: {cp.inferred_type}{unit}{year} \"\n",
    "              f\"({cp.unique_count} unique / {cp.total_count} total)\")\n",
    "        if cp.sample_values:\n",
    "            print(f\"      samples: {cp.sample_values[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Contract Recommendation\n",
    "\n",
    "`recommend_contract()` takes a `DocumentProfile` (or `MultiDocumentProfile`) and\n",
    "produces a draft contract JSON with inline `_recommendation` and `_detected_*`\n",
    "guidance fields. The draft includes:\n",
    "\n",
    "- Category keywords extracted from table header tokens\n",
    "- Output schemas with columns, inferred types, and alias candidates\n",
    "- Year template aliases (`{YYYY}`) for year columns\n",
    "- Section labels as dimension column aliases\n",
    "- `report_date` configuration from temporal patterns\n",
    "\n",
    "`strip_recommendations()` removes all annotation fields, producing a clean\n",
    "contract ready for `load_contract()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contract_semantics.recommend import recommend_contract, strip_recommendations, compare_contract\n",
    "\n",
    "# Generate draft contract from the DOCX profile\n",
    "draft = recommend_contract(ru_profile, provider_name=\"ru_ag_ministry\")\n",
    "\n",
    "print(\"Draft contract:\")\n",
    "print(f\"  _analyzer_version: {draft.get('_analyzer_version')}\")\n",
    "print(f\"  _source_documents: {draft.get('_source_documents')}\")\n",
    "print(f\"  Categories: {list(draft.get('categories', {}).keys())}\")\n",
    "print(f\"  Outputs: {list(draft.get('outputs', {}).keys())}\")\n",
    "\n",
    "if draft.get(\"report_date\"):\n",
    "    print(f\"  report_date: {draft['report_date']}\")\n",
    "\n",
    "# Show first output schema with inline recommendations\n",
    "first_out_name = next(iter(draft.get(\"outputs\", {})))\n",
    "first_out = draft[\"outputs\"][first_out_name]\n",
    "print(f\"\\n--- {first_out_name} schema ---\")\n",
    "for col in first_out[\"schema\"][\"columns\"][:6]:\n",
    "    print(f\"  {col['name']} ({col['type']})\")\n",
    "    if \"_recommendation\" in col:\n",
    "        print(f\"    rec: {col['_recommendation']}\")\n",
    "    if col.get(\"aliases\"):\n",
    "        print(f\"    aliases: {col['aliases'][:4]}{'...' if len(col['aliases']) > 4 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare profile against the existing hand-authored contract\n",
    "report = compare_contract(ru_profile, \"contracts/ru_ag_ministry.json\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip recommendation fields -> clean contract\n",
    "clean = strip_recommendations(draft)\n",
    "\n",
    "def count_underscore_keys(obj, count=0):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k.startswith(\"_\"):\n",
    "                count += 1\n",
    "            count = count_underscore_keys(v, count)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            count = count_underscore_keys(item, count)\n",
    "    return count\n",
    "\n",
    "print(f\"Draft: {count_underscore_keys(draft)} annotation fields\")\n",
    "print(f\"Clean: {count_underscore_keys(clean)} annotation fields\")\n",
    "\n",
    "print(f\"\\nClean contract JSON (excerpt):\")\n",
    "print(json.dumps(clean, indent=2, ensure_ascii=False)[:600] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Multi-Document Analysis\n",
    "\n",
    "The multi-document workflow profiles several documents independently, then merges\n",
    "them into a `MultiDocumentProfile` that aligns structurally similar tables across\n",
    "documents. This is the path for multi-provider contracts like `au_shipping_stem.json`\n",
    "where 6 different PDF formats all need one canonical schema.\n",
    "\n",
    "`merge_profiles()` groups tables by structural similarity (column-count ratio +\n",
    "Jaccard token overlap) and aligns columns, collecting all header variants for\n",
    "each structural position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile 3 Australian shipping stem PDFs (subset for speed)\n",
    "shipping_pdfs = sorted(Path(\"inputs\").glob(\"*.pdf\"))\n",
    "shipping_pdfs = [p for p in shipping_pdfs if \"shipping\" in p.name.lower()\n",
    "                 or \"loading\" in p.name.lower()\n",
    "                 or \"bunge\" in p.name.lower()\n",
    "                 or \"cbh\" in p.name.lower().replace(\" \", \"\")]\n",
    "\n",
    "print(f\"Profiling {len(shipping_pdfs)} shipping PDFs:\")\n",
    "pdf_profiles = []\n",
    "for p in shipping_pdfs[:3]:  # first 3 for speed\n",
    "    print(f\"  {p.name}... \", end=\"\", flush=True)\n",
    "    prof = profile_document(str(p))\n",
    "    pdf_profiles.append(prof)\n",
    "    print(f\"{len(prof.tables)} tables\")\n",
    "\n",
    "# Merge into consolidated multi-document profile\n",
    "multi = merge_profiles(pdf_profiles)\n",
    "print(f\"\\n{format_analysis_report(multi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect aligned columns — header variants across providers\n",
    "for i, tg in enumerate(multi.table_groups):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Group {i}: {len(tg.tables)} tables, layout={tg.layout}\")\n",
    "    print(f\"  Common tokens: {sorted(tg.common_tokens)[:10]}\")\n",
    "    if tg.all_section_labels:\n",
    "        print(f\"  Section labels: {tg.all_section_labels[:5]}\")\n",
    "    print(f\"  Aligned columns:\")\n",
    "    for ac in tg.aligned_columns:\n",
    "        variants = sorted(set(ac.all_headers))\n",
    "        print(f\"    {ac.canonical_header} ({ac.inferred_type}): {variants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend a contract from the multi-provider profile\n",
    "shipping_draft = recommend_contract(multi, provider_name=\"au_shipping_stem\")\n",
    "\n",
    "print(\"Multi-provider draft contract:\")\n",
    "print(f\"  Categories: {list(shipping_draft.get('categories', {}).keys())}\")\n",
    "print(f\"  Outputs: {list(shipping_draft.get('outputs', {}).keys())}\")\n",
    "\n",
    "# Compare against the hand-authored shipping contract\n",
    "shipping_report = compare_contract(multi, \"contracts/au_shipping_stem.json\")\n",
    "print(f\"\\n{shipping_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. SHACL Validation\n",
    "\n",
    "`generate_shapes()` auto-generates SHACL NodeShapes from a contract's output\n",
    "schemas — `sh:datatype` for typed columns, `sh:in` for string columns with aliases.\n",
    "`validate_records()` checks tabular records against these shapes.\n",
    "\n",
    "This lets you catch invalid values (wrong types, unknown categories) before they\n",
    "propagate downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHACL shapes from the shipping contract\n",
    "shapes = generate_shapes(\"contracts/au_shipping_stem.json\")\n",
    "\n",
    "print(f\"Generated shapes for: {list(shapes.keys())}\")\n",
    "for name, path in shapes.items():\n",
    "    content = Path(path).read_text()\n",
    "    print(f\"\\n--- {name} ({path}) ---\")\n",
    "    print(content[:500])\n",
    "    if len(content) > 500:\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate sample records — good and bad\n",
    "try:\n",
    "    shapes_path = shapes[\"vessels\"]\n",
    "\n",
    "    with open(\"contracts/au_shipping_stem.json\") as f:\n",
    "        au_contract = json.load(f)\n",
    "    schema_cols = au_contract[\"outputs\"][\"vessels\"][\"schema\"][\"columns\"]\n",
    "\n",
    "    # Valid records\n",
    "    good = [\n",
    "        {\"load_port\": \"GERALDTON\", \"vessel_name\": \"MV STAR\", \"commodity\": \"Wheat\",\n",
    "         \"tons\": 50000, \"eta\": \"2025-10-15\", \"status\": \"Loading\"},\n",
    "    ]\n",
    "    ok, report, _ = validate_records(good, shapes_path, schema_columns=schema_cols)\n",
    "    print(f\"Valid records: conforms={ok}\")\n",
    "\n",
    "    # Invalid records — wrong type\n",
    "    bad = [\n",
    "        {\"load_port\": \"GERALDTON\", \"vessel_name\": 12345, \"tons\": \"not a number\"},\n",
    "    ]\n",
    "    ok2, report2, _ = validate_records(bad, shapes_path, schema_columns=schema_cols)\n",
    "    print(f\"Invalid records: conforms={ok2}\")\n",
    "    if not ok2:\n",
    "        # Show first few lines of the report\n",
    "        for line in report2.strip().splitlines()[:15]:\n",
    "            print(f\"  {line}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SHACL validation requires pyshacl.\")\n",
    "    print(\"Install with: pip install contract-semantics[shacl]\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n5xv8wmu0yi",
   "source": "---\n## 9. End-to-End: Russian Agriculture\n\nComplete workflow from raw document to validated extraction. Compares two contracts:\n- **Draft** — auto-generated by the analyzer (what you get with zero domain knowledge)\n- **Semantic** — hand-authored + ontology-resolved aliases (AGROVOC crops, GeoNames regions)\n\nThe difference demonstrates how semantic enforcement improves extraction quality:\nmore deterministic mappings, enrichment columns, and consistent naming.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1e9wf391k8",
   "source": "# Step 1: Choose and profile a document\n\nfrom contract_semantics.analyze import profile_document, format_analysis_report\n\ndocx_files = sorted(Path(\"inputs/docx/input\").glob(\"*.docx\"))\nprint(\"Available DOCX reports:\")\nfor i, p in enumerate(docx_files):\n    print(f\"  [{i}] {p.name}\")\n\nchosen_docx = docx_files[0]\nprint(f\"\\nProfiling: {chosen_docx.name}\")\ne2e_profile = profile_document(str(chosen_docx))\nprint(format_analysis_report(e2e_profile))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ev4jz2el77d",
   "source": "# Step 2: Generate two contracts for comparison\n#   (a) Draft — auto-generated from the document profile (zero domain knowledge)\n#   (b) Semantic — hand-authored ru_ag_ministry.json enriched with ontology aliases\n\nfrom contract_semantics.recommend import recommend_contract, strip_recommendations\n\n# (a) Draft contract\ndraft = recommend_contract(e2e_profile, provider_name=\"ru_ag_draft\")\ndraft_clean = strip_recommendations(draft)\n\n# (b) Semantic contract — materialize with ontology aliases\nsemantic = materialize_contract(\n    \"contracts/ru_ag_ministry.json\",\n    agrovoc=agrovoc,\n    geonames=geonames,\n    merge_strategy=\"union\",\n)\n\n# Save both to temp files for the pipeline\nimport tempfile\ntmp_dir = Path(tempfile.mkdtemp())\ndraft_path = tmp_dir / \"draft_contract.json\"\nsemantic_path = tmp_dir / \"semantic_contract.json\"\ndraft_path.write_text(json.dumps(draft_clean, indent=2, ensure_ascii=False))\nsemantic_path.write_text(json.dumps(semantic, indent=2, ensure_ascii=False))\n\n# Compare structure\ndraft_aliases = sum(\n    len(c.get(\"aliases\", []))\n    for out in draft_clean.get(\"outputs\", {}).values()\n    for c in out.get(\"schema\", {}).get(\"columns\", [])\n)\nsemantic_aliases = sum(\n    len(c.get(\"aliases\", []))\n    for out in semantic.get(\"outputs\", {}).values()\n    for c in out.get(\"schema\", {}).get(\"columns\", [])\n)\ndraft_enrichment = sum(\n    1 for out in draft_clean.get(\"outputs\", {}).values()\n    for c in out.get(\"schema\", {}).get(\"columns\", [])\n    if c.get(\"source\")\n)\nsemantic_enrichment = sum(\n    1 for out in semantic.get(\"outputs\", {}).values()\n    for c in out.get(\"schema\", {}).get(\"columns\", [])\n    if c.get(\"source\")\n)\n\nprint(f\"{'='*60}\")\nprint(f\"  Contract comparison\")\nprint(f\"{'='*60}\")\nprint(f\"\\n  {'':20s} {'Draft':>10s}  {'Semantic':>10s}\")\nprint(f\"  {'Categories':20s} {len(draft_clean.get('categories', {})):>10d}  {len(semantic.get('categories', {})):>10d}\")\nprint(f\"  {'Outputs':20s} {len(draft_clean.get('outputs', {})):>10d}  {len(semantic.get('outputs', {})):>10d}\")\nprint(f\"  {'Total aliases':20s} {draft_aliases:>10d}  {semantic_aliases:>10d}\")\nprint(f\"  {'Enrichment cols':20s} {draft_enrichment:>10d}  {semantic_enrichment:>10d}\")\n\nprint(f\"\\nDraft saved to:    {draft_path}\")\nprint(f\"Semantic saved to: {semantic_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zyw4lkfxd2m",
   "source": "# Step 3: Run extraction with the semantic contract (preview)\n\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom docpact.contracts import load_contract\nfrom docpact.pipeline import process_document_async\n\nsemantic_cc = load_contract(str(semantic_path))\nprint(f\"Semantic contract: {semantic_cc.provider}\")\nprint(f\"  Model: {semantic_cc.model}\")\nprint(f\"  Outputs: {list(semantic_cc.outputs.keys())}\")\n\nsemantic_result = await process_document_async(str(chosen_docx), semantic_cc)\n\nprint(f\"\\nExtraction complete (report_date: {semantic_result.report_date})\")\nfor name, df in semantic_result.dataframes.items():\n    if df is not None and len(df) > 0:\n        print(f\"\\n{'='*60}\")\n        print(f\"  {name}: {len(df)} rows x {len(df.columns)} cols\")\n        print(f\"  Columns: {list(df.columns)}\")\n        print(f\"{'='*60}\")\n        display(df.head(8))\n    else:\n        print(f\"\\n  {name}: EMPTY\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j3gnt0vdl3t",
   "source": "# Step 4: Run extraction with the draft contract and compare\n\ntry:\n    draft_cc = load_contract(str(draft_path))\n    print(f\"Draft contract: {draft_cc.provider}\")\n    print(f\"  Outputs: {list(draft_cc.outputs.keys())}\")\n\n    draft_result = await process_document_async(str(chosen_docx), draft_cc)\n    print(f\"Draft extraction complete (report_date: {draft_result.report_date})\")\nexcept Exception as e:\n    draft_result = None\n    print(f\"Draft extraction error: {e}\")\n    print(\"This is expected — the auto-generated draft may lack structure\")\n    print(\"needed for full extraction (e.g. year templates, enrichment rules).\")\n\n# Comparison\nprint(f\"\\n{'='*60}\")\nprint(f\"  Extraction comparison: Draft vs Semantic\")\nprint(f\"{'='*60}\")\n\nall_outputs = sorted(set(\n    list(semantic_result.dataframes.keys()) +\n    (list(draft_result.dataframes.keys()) if draft_result else [])\n))\n\nprint(f\"\\n  {'Output':20s} {'Draft':>15s}  {'Semantic':>15s}\")\nprint(f\"  {'-'*20} {'-'*15}  {'-'*15}\")\n\nfor name in all_outputs:\n    df_s = semantic_result.dataframes.get(name)\n    df_d = draft_result.dataframes.get(name) if draft_result else None\n\n    s_shape = f\"{len(df_s)}r x {len(df_s.columns)}c\" if df_s is not None and len(df_s) > 0 else \"empty\"\n    d_shape = f\"{len(df_d)}r x {len(df_d.columns)}c\" if df_d is not None and len(df_d) > 0 else \"empty\"\n\n    print(f\"  {name:20s} {d_shape:>15s}  {s_shape:>15s}\")\n\n    # Columns only in semantic output (enrichment columns)\n    if df_s is not None and df_d is not None and len(df_s) > 0 and len(df_d) > 0:\n        extra = sorted(set(df_s.columns) - set(df_d.columns))\n        if extra:\n            print(f\"  {'':20s} Semantic adds: {extra}\")\n\nrd_s = semantic_result.report_date or \"N/A\"\nrd_d = (draft_result.report_date if draft_result else None) or \"N/A\"\nprint(f\"\\n  {'report_date':20s} {rd_d:>15s}  {rd_s:>15s}\")\n\n# Show a sample semantic DataFrame with all enrichment columns visible\nprint(f\"\\n{'='*60}\")\nprint(f\"  Semantic output sample (with enrichment columns)\")\nprint(f\"{'='*60}\")\nfirst_name = next(iter(semantic_result.dataframes), None)\nif first_name:\n    df_show = semantic_result.dataframes[first_name]\n    if df_show is not None and len(df_show) > 0:\n        display(df_show.head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jfiyf63wvw",
   "source": "# Interactive dashboard: Semantic Web Enrichment Benefits\n\n_ensure_pkg(\"plotly\", \"plotly>=5.0\")\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n# ═══════════════════════════════════════════════════════════\n# Prepare data\n# ═══════════════════════════════════════════════════════════\n\n# 1. Per-column alias breakdown: manual vs ontology-resolved\nwith open(\"contracts/ru_ag_ministry.json\") as f:\n    orig_contract = json.load(f)\n\nalias_rows = []\nfor out_name, out_spec in semantic.get(\"outputs\", {}).items():\n    for col in out_spec.get(\"schema\", {}).get(\"columns\", []):\n        all_aliases = col.get(\"aliases\", [])\n        if not all_aliases:\n            continue\n        col_name = col[\"name\"]\n        orig_aliases = []\n        if out_name in orig_contract.get(\"outputs\", {}):\n            for oc in orig_contract[\"outputs\"][out_name][\"schema\"][\"columns\"]:\n                if oc[\"name\"] == col_name:\n                    orig_aliases = oc.get(\"aliases\", [])\n                    break\n        manual_n = len(orig_aliases)\n        ontology_n = max(0, len(all_aliases) - manual_n)\n        alias_rows.append(dict(\n            column=f\"{col_name}\", output=out_name,\n            manual=manual_n, ontology=ontology_n, total=len(all_aliases),\n        ))\n\n# 2. Extraction comparison\nextract_rows = []\nfor name in sorted(semantic_result.dataframes.keys()):\n    df_s = semantic_result.dataframes.get(name)\n    df_d = draft_result.dataframes.get(name) if draft_result else None\n    s_rows = len(df_s) if df_s is not None else 0\n    d_rows = len(df_d) if df_d is not None else 0\n    s_cols = len(df_s.columns) if df_s is not None and len(df_s) > 0 else 0\n    d_cols = len(df_d.columns) if df_d is not None and len(df_d) > 0 else 0\n    extract_rows.append(dict(output=name, draft_rows=d_rows, semantic_rows=s_rows,\n                             draft_cols=d_cols, semantic_cols=s_cols))\n\n# 3. Crop breakdown (only available via title enrichment)\ndf_harvest = semantic_result.dataframes.get(\"harvest\")\ncrop_summary = None\nif df_harvest is not None and \"Crop\" in df_harvest.columns and \"Value\" in df_harvest.columns:\n    crop_summary = (\n        df_harvest.dropna(subset=[\"Crop\", \"Value\"])\n        .groupby(\"Crop\")[\"Value\"].sum()\n        .sort_values(ascending=True).tail(10)\n    )\n\n# 4. Geographic grounding — join harvest data with GeoNames coordinates\ngeo_rows = []\nif df_harvest is not None and \"Region\" in df_harvest.columns:\n    for gid, feat in geonames_features.items():\n        region_name = feat[\"name\"]\n        # Match by first word (e.g. \"Moscow\" matches \"Moscow Oblast\" or \"Moscow Region\")\n        keyword = region_name.split()[0]\n        matches = df_harvest[df_harvest[\"Region\"].str.contains(keyword, case=False, na=False)]\n        if len(matches) > 0:\n            total = matches[\"Value\"].dropna().sum()\n            geo_rows.append(dict(\n                region=region_name, lat=feat[\"lat\"], lng=feat[\"lng\"],\n                value=total, records=len(matches),\n                admin1=feat[\"admin1_code\"], pop=feat[\"population\"],\n            ))\n\n# ═══════════════════════════════════════════════════════════\n# Figure 1: Main Dashboard (2x2)\n# ═══════════════════════════════════════════════════════════\n\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=[\n        \"<b>Contract Structure</b>  Draft vs Semantic\",\n        \"<b>Extraction Results</b>  Rows per Output\",\n        \"<b>Alias Expansion</b>  Manual + Ontology-Resolved\",\n        \"<b>Crop Analysis</b>  Enabled by Title Enrichment\",\n    ],\n    vertical_spacing=0.16, horizontal_spacing=0.14,\n)\n\nDRAFT_COLOR = \"#EF553B\"\nSEMANTIC_COLOR = \"#636EFA\"\nONTOLOGY_COLOR = \"#00CC96\"\nCROP_COLOR = \"#AB63FA\"\n\n# ── Panel 1: Contract structure ──\nstruct_labels = [\"Categories\", \"Outputs\", \"Aliases\", \"Enrichment<br>Columns\"]\nstruct_draft = [\n    len(draft_clean.get(\"categories\", {})),\n    len(draft_clean.get(\"outputs\", {})),\n    draft_aliases, draft_enrichment,\n]\nstruct_semantic = [\n    len(semantic.get(\"categories\", {})),\n    len(semantic.get(\"outputs\", {})),\n    semantic_aliases, semantic_enrichment,\n]\nfig.add_trace(go.Bar(\n    x=struct_labels, y=struct_draft, name=\"Draft\",\n    marker_color=DRAFT_COLOR, legendgroup=\"a\", legendgrouptitle_text=\"Contract\",\n), row=1, col=1)\nfig.add_trace(go.Bar(\n    x=struct_labels, y=struct_semantic, name=\"Semantic\",\n    marker_color=SEMANTIC_COLOR, legendgroup=\"a\",\n), row=1, col=1)\n\n# ── Panel 2: Extraction results ──\nout_names = [r[\"output\"] for r in extract_rows]\nfig.add_trace(go.Bar(\n    x=out_names, y=[r[\"draft_rows\"] for r in extract_rows], name=\"Draft\",\n    marker_color=DRAFT_COLOR, legendgroup=\"b\", legendgrouptitle_text=\"Extraction\",\n    showlegend=True,\n    text=[f'{r[\"draft_cols\"]}c' for r in extract_rows], textposition=\"outside\",\n), row=1, col=2)\nfig.add_trace(go.Bar(\n    x=out_names, y=[r[\"semantic_rows\"] for r in extract_rows], name=\"Semantic\",\n    marker_color=SEMANTIC_COLOR, legendgroup=\"b\",\n    text=[f'{r[\"semantic_cols\"]}c' for r in extract_rows], textposition=\"outside\",\n), row=1, col=2)\n\n# ── Panel 3: Alias expansion ──\na_cols = [r[\"column\"] for r in alias_rows]\nfig.add_trace(go.Bar(\n    x=a_cols, y=[r[\"manual\"] for r in alias_rows], name=\"Manual\",\n    marker_color=SEMANTIC_COLOR, legendgroup=\"c\", legendgrouptitle_text=\"Aliases\",\n), row=2, col=1)\nfig.add_trace(go.Bar(\n    x=a_cols, y=[r[\"ontology\"] for r in alias_rows], name=\"Ontology\",\n    marker_color=ONTOLOGY_COLOR, legendgroup=\"c\",\n), row=2, col=1)\n\n# ── Panel 4: Crop breakdown ──\nif crop_summary is not None and len(crop_summary) > 0:\n    fig.add_trace(go.Bar(\n        y=crop_summary.index, x=crop_summary.values,\n        orientation=\"h\", marker_color=CROP_COLOR,\n        name=\"Harvest Total\", legendgroup=\"d\", legendgrouptitle_text=\"Crops\",\n        hovertemplate=\"%{y}: %{x:,.0f}<extra></extra>\",\n    ), row=2, col=2)\n    fig.add_annotation(\n        text=\"<i>Crop dimension extracted from table titles<br>— only possible with semantic enrichment</i>\",\n        xref=\"x4 domain\", yref=\"y4 domain\", x=0.95, y=0.02,\n        showarrow=False, font=dict(size=9, color=\"gray\"), xanchor=\"right\",\n    )\nelse:\n    fig.add_annotation(\n        text=\"No crop data — run Step 3 first\",\n        xref=\"x4 domain\", yref=\"y4 domain\", x=0.5, y=0.5, showarrow=False,\n    )\n\nfig.update_layout(\n    height=800, barmode=\"group\",\n    title=dict(text=\"<b>Semantic Web Enrichment — Impact Dashboard</b>\", x=0.5),\n    template=\"plotly_white\",\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.12, xanchor=\"center\", x=0.5),\n    font=dict(size=11),\n    margin=dict(b=100),\n)\nfig.show()\n\n# ═══════════════════════════════════════════════════════════\n# Figure 2: Geographic Grounding (GeoNames)\n# ═══════════════════════════════════════════════════════════\n\nif geo_rows:\n    import pandas as pd\n    df_geo = pd.DataFrame(geo_rows)\n\n    fig_geo = px.scatter_geo(\n        df_geo, lat=\"lat\", lon=\"lng\", size=\"value\",\n        hover_name=\"region\",\n        hover_data={\"records\": True, \"pop\": \":,\", \"value\": \":,.0f\", \"lat\": False, \"lng\": False},\n        color=\"value\", color_continuous_scale=\"Viridis\",\n        size_max=30, projection=\"natural earth\",\n        title=\"<b>Geographic Grounding</b> — Harvest values mapped via GeoNames coordinates\",\n    )\n    fig_geo.update_geos(\n        scope=\"europe\", center=dict(lat=55, lon=45),\n        projection_scale=3.5,\n        showland=True, landcolor=\"rgb(243,243,243)\",\n        showocean=True, oceancolor=\"rgb(204,224,245)\",\n        showcountries=True, countrycolor=\"rgb(204,204,204)\",\n    )\n    fig_geo.update_layout(\n        height=500, template=\"plotly_white\",\n        margin=dict(l=20, r=20, t=60, b=20),\n        font=dict(size=11),\n    )\n    fig_geo.add_annotation(\n        text=\"<i>3 regions from fixture — with full GeoNames data, all 78 Russian oblasts are mappable</i>\",\n        xref=\"paper\", yref=\"paper\", x=0.5, y=-0.02,\n        showarrow=False, font=dict(size=10, color=\"gray\"), xanchor=\"center\",\n    )\n    fig_geo.show()\nelse:\n    print(\"No geographic data available — run extraction (Step 3) first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oo3786qilwm",
   "source": "---\n## Summary\n\n### Contract Authoring (contract-semantics)\n\n| Capability | Module | Key function |\n|---|---|---|\n| Document profiling | `analyze.py` | `profile_document()`, `merge_profiles()` |\n| Contract recommendation | `recommend.py` | `recommend_contract()`, `compare_contract()` |\n| AGROVOC resolution | `agrovoc.py` | `AgrovocAdapter.resolve_concept()` |\n| GeoNames resolution | `geonames.py` | `GeoNamesAdapter.resolve_geoname()` |\n| Column resolution | `resolve.py` | `resolve_column()` |\n| Alias diff | `diff.py` | `diff_aliases()`, `diff_all()` |\n| Materialization | `materialize.py` | `materialize_contract()` |\n| SHACL validation | `validate.py` | `generate_shapes()`, `validate_records()` |\n| Semantic context builder | `context.py` | `build_semantic_context()` |\n\n### Pipeline Integration (docpact)\n\n| Capability | Module | Key function |\n|---|---|---|\n| Runtime alias enrichment | `pipeline.py` | `interpret_output_async(..., semantic_context=)` |\n| Pre-flight header check | `semantics.py` | `preflight_check()` |\n| Post-extraction validation | `semantics.py` | `validate_output()` |\n| Semantic context (data) | `semantics.py` | `SemanticContext.from_json()` / `.to_json()` |\n\n**CLI equivalents** for production use:\n```bash\ncontract-semantics analyze report.docx                       # profile + recommend\ncontract-semantics analyze *.pdf --compare contracts/X.json  # multi-doc + gap report\ncontract-semantics diff contracts/ru_ag_ministry.json        # alias coverage\ncontract-semantics materialize contracts/X.json -o out.json  # enrich + strip\ncontract-semantics build-context contracts/X.json -o ctx.json # build SemanticContext\ncontract-semantics generate-shapes contracts/X.json          # SHACL shapes\ncontract-semantics validate output.csv shapes/X.ttl          # SHACL check\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cb8ezuiwm3s",
   "source": "# Step 1: Build a SemanticContext from the annotated contract.\n# This resolves all concept_uris through the AGROVOC and GeoNames adapters\n# and packages the results into a plain dataclass that docpact can consume.\n\nctx = build_semantic_context(\n    \"contracts/ru_ag_ministry.json\",\n    agrovoc=agrovoc,\n    geonames=geonames,\n    merge_strategy=\"union\",\n)\n\nprint(f\"SemanticContext built at: {ctx.resolved_at}\")\nprint(f\"  Adapter versions: {ctx.adapter_versions}\")\n\ntotal_aliases = sum(len(a) for cols in ctx.resolved_aliases.values() for a in cols.values())\ntotal_valid = sum(len(v) for cols in ctx.valid_values.values() for v in cols.values())\nprint(f\"  Resolved aliases: {total_aliases} across all columns\")\nprint(f\"  Valid values: {total_valid} across all columns\")\n\n# Inspect resolved aliases per output/column\nfor out_name in sorted(ctx.resolved_aliases):\n    print(f\"\\n  {out_name}:\")\n    for col_name, aliases in sorted(ctx.resolved_aliases[out_name].items()):\n        preview = aliases[:5]\n        suffix = \"...\" if len(aliases) > 5 else \"\"\n        print(f\"    {col_name}: {len(aliases)} aliases — {preview}{suffix}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "mz4v8sij5ak",
   "source": "# Step 2: Run pipeline WITH and WITHOUT SemanticContext.\n# The annotated contract (with concept_uris) is used directly — no materialization needed.\n\nfrom docpact.contracts import load_contract\nfrom docpact.pipeline import process_document_async\n\ncc = load_contract(\"contracts/ru_ag_ministry.json\")\nprint(f\"Contract: {cc.provider}\")\nprint(f\"  has_semantic_annotations: {cc.has_semantic_annotations}\")\nfor out_name, out_spec in cc.outputs.items():\n    sem_cols = list(out_spec.semantic_columns.keys())\n    print(f\"  {out_name}: semantic_columns = {sem_cols}\")\n\n# With semantic context — enables preflight + validation + alias enrichment\nresult_with = await process_document_async(str(chosen_docx), cc, semantic_context=ctx)\n\n# Without semantic context — baseline behavior (identical to before)\nresult_without = await process_document_async(str(chosen_docx), cc)\n\nprint(f\"\\nExtraction complete: {chosen_docx.name}\")\nprint(f\"  report_date: {result_with.report_date}\")\nprint(f\"\\n  With SemanticContext:\")\nprint(f\"    preflight_reports:  {list(result_with.preflight_reports.keys())}\")\nprint(f\"    validation_reports: {list(result_with.validation_reports.keys())}\")\nprint(f\"\\n  Without SemanticContext:\")\nprint(f\"    preflight_reports:  {result_without.preflight_reports}\")\nprint(f\"    validation_reports: {result_without.validation_reports}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "91e8v1n4zlr",
   "source": "# Step 3: Inspect pre-flight reports — header coverage before extraction.\n# Pre-flight compares document pipe-table headers against all known aliases\n# (manual + ontology-resolved). Informational only — never blocks extraction.\n\nfor out_name, report in result_with.preflight_reports.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"  Pre-flight: {out_name}\")\n    print(f\"{'='*60}\")\n    print(f\"  Header coverage: {report.header_coverage:.0%}\")\n\n    if report.unmatched_headers:\n        print(f\"  Unmatched doc headers: {report.unmatched_headers}\")\n    else:\n        print(f\"  All document headers matched!\")\n\n    if report.missing_aliases:\n        print(f\"  Contract aliases with no doc header: {report.missing_aliases}\")\n\n    if report.findings:\n        print(f\"\\n  Findings ({len(report.findings)}):\")\n        for f in report.findings[:8]:\n            print(f\"    [{f.severity}] {f.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hpkof4ynslt",
   "source": "# Step 4: Inspect validation reports — value compliance after extraction.\n# For columns with validate=True, checks every extracted value against the\n# combined set of concept URI labels + ontology-resolved aliases.\n\nfor out_name, report in result_with.validation_reports.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"  Validation: {out_name}\")\n    print(f\"{'='*60}\")\n    print(f\"  Total rows: {report.total_rows}\")\n    print(f\"  Valid: {report.valid_count}  Invalid: {report.invalid_count}\")\n\n    if report.column_summaries:\n        print(f\"\\n  Column summaries:\")\n        for col_name, summary in report.column_summaries.items():\n            print(f\"    {col_name}: {summary['valid']} valid, {summary['invalid']} invalid\")\n            if summary.get(\"unknown_values\"):\n                unknowns = summary[\"unknown_values\"][:5]\n                suffix = \"...\" if len(summary[\"unknown_values\"]) > 5 else \"\"\n                print(f\"      unknown values: {unknowns}{suffix}\")\n\n    if report.findings:\n        print(f\"\\n  Findings ({len(report.findings)}):\")\n        for f in report.findings[:10]:\n            print(f\"    row {f.row_index}, {f.column_name}: \\\"{f.value}\\\" — {f.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "s6x24ux2ol",
   "source": "# Step 5: SemanticContext serialization — cache to JSON for reuse.\n# In production: build once (slow, hits ontology adapters), cache, reuse many times.\n\nfrom docpact.semantics import SemanticContext\n\nctx_path = Path(tempfile.mkdtemp()) / \"semantic_context.json\"\nctx.to_json(ctx_path)\n\n# Reload from cache\nctx_reloaded = SemanticContext.from_json(ctx_path)\n\nprint(f\"Cached to: {ctx_path}\")\nprint(f\"  File size: {ctx_path.stat().st_size:,} bytes\")\nprint(f\"  Round-trip OK: {ctx_reloaded.resolved_aliases == ctx.resolved_aliases}\")\nprint(f\"  resolved_at preserved: {ctx_reloaded.resolved_at == ctx.resolved_at}\")\nprint(f\"  adapter_versions preserved: {ctx_reloaded.adapter_versions == ctx.adapter_versions}\")\n\n# Show JSON structure\nprint(f\"\\nJSON excerpt:\")\nctx_json = json.loads(ctx_path.read_text())\nprint(json.dumps({k: type(v).__name__ for k, v in ctx_json.items()}, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Capability | Module | Key function |\n",
    "|---|---|---|\n",
    "| Document profiling | `analyze.py` | `profile_document()`, `merge_profiles()` |\n",
    "| Contract recommendation | `recommend.py` | `recommend_contract()`, `compare_contract()` |\n",
    "| AGROVOC resolution | `agrovoc.py` | `AgrovocAdapter.resolve_concept()` |\n",
    "| GeoNames resolution | `geonames.py` | `GeoNamesAdapter.resolve_geoname()` |\n",
    "| Column resolution | `resolve.py` | `resolve_column()` |\n",
    "| Alias diff | `diff.py` | `diff_aliases()`, `diff_all()` |\n",
    "| Materialization | `materialize.py` | `materialize_contract()` |\n",
    "| SHACL validation | `validate.py` | `generate_shapes()`, `validate_records()` |\n",
    "\n",
    "**CLI equivalents** for production use:\n",
    "```bash\n",
    "contract-semantics analyze report.docx                       # profile + recommend\n",
    "contract-semantics analyze *.pdf --compare contracts/X.json  # multi-doc + gap report\n",
    "contract-semantics diff contracts/ru_ag_ministry.json        # alias coverage\n",
    "contract-semantics materialize contracts/X.json -o out.json  # enrich + strip\n",
    "contract-semantics generate-shapes contracts/X.json          # SHACL shapes\n",
    "contract-semantics validate output.csv shapes/X.ttl          # SHACL check\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}