{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract-Driven Data Pipelines\n",
    "\n",
    "Extract structured data from heterogeneous documents using declarative YAML contracts.\n",
    "\n",
    "One schema. Many providers. Full async."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "A **data contract** (YAML) declares everything the pipeline needs:\n",
    "- Which LLM model to use\n",
    "- How to classify tables (keyword matching)\n",
    "- What output schema to produce (columns, types, aliases, formats)\n",
    "- How to enrich records (constants, metadata from filenames/titles)\n",
    "\n",
    "The pipeline code is 100% generic — swap the contract, not the code.\n",
    "\n",
    "**Three use cases** demonstrate the same pipeline across different domains:\n",
    "\n",
    "| # | Use Case | Format | Documents | Challenge |\n",
    "|---|----------|--------|-----------|----------|\n",
    "| 1 | Russian agricultural reports | DOCX | 2 weekly reports | Multi-category extraction, dynamic pivot years |\n",
    "| 2 | Australian shipping stems | PDF | 6 providers | One canonical model, 6 different layouts, full concurrency |\n",
    "| 3 | ACEA car registrations | PDF | 1 press release | Pivoted table → flat records, vision-enabled |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport asyncio\nimport re\nimport time\nimport yaml\nfrom pathlib import Path\n\nimport nest_asyncio\nimport pandas as pd\n\nfrom pdf_ocr import (\n    CanonicalSchema, ColumnDef,\n    classify_tables, classify_docx_tables,\n    compress_spatial_text, compress_spatial_text_structured,\n    compress_docx_tables, extract_pivot_values,\n    interpret_tables_async,\n    to_pandas, to_records, to_parquet,\n)\n# Direct async imports — avoid sync wrappers that spawn extra threads\nfrom pdf_ocr.interpret import _interpret_pages_batched_async, _split_pages\n\nnest_asyncio.apply()\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG. Supports PDF and DOCX.\"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Pipeline Architecture\n\n```\ncontract (YAML) → prepare() → fetch() → transform_async() → save()\n```\n\nFour levels of concurrency via `asyncio.gather()` — all using native BAML async (no sync wrappers, no extra threads):\n\n| Level | Scope | Pattern |\n|---|---|---|\n| **Document-level** | Process N documents simultaneously | `asyncio.gather(*[transform_async(doc) for doc in docs])` |\n| **Category-level** | Multiple outputs per document (e.g. harvest + planting) | `asyncio.gather(*[_transform_output_async(...) for out in outputs])` |\n| **Table/Page-level** | DOCX: N tables concurrent; PDF: N pages concurrent | `interpret_tables_async()` / `_interpret_pages_batched_async()` |\n| **Batch-level** | Step 2 mapping in chunks of 20 rows | Built into `_interpret_pages_batched_async()` |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Generic Pipeline Functions ────────────────────────────────────────────────\n# These functions are contract-agnostic. Swap the YAML contract and re-run.\n\n\ndef prepare(contract_path):\n    \"\"\"Load YAML contract → parse categories, schemas, enrichment rules.\"\"\"\n    with open(contract_path) as f:\n        contract = yaml.safe_load(f)\n\n    ctx = {\"contract\": contract, \"model\": contract.get(\"model\", \"openai/gpt-4o\")}\n\n    # Categories for table classification\n    ctx[\"categories\"] = {\n        name: cat[\"keywords\"]\n        for name, cat in contract.get(\"categories\", {}).items()\n    }\n\n    # Build CanonicalSchema + enrichment rules per output\n    ctx[\"schemas\"] = {}\n    ctx[\"enrichment\"] = {}\n    ctx[\"output_specs\"] = {}\n    for out_name, spec in contract.get(\"outputs\", {}).items():\n        llm_cols = []\n        enrich = {}\n        for col in spec[\"schema\"][\"columns\"]:\n            if \"source\" in col:\n                enrich[col[\"name\"]] = col\n            else:\n                llm_cols.append(ColumnDef(\n                    name=col[\"name\"],\n                    type=col.get(\"type\", \"string\"),\n                    description=col.get(\"description\", \"\"),\n                    aliases=col.get(\"aliases\", []),\n                    format=col.get(\"format\"),\n                ))\n        ctx[\"schemas\"][out_name] = CanonicalSchema(\n            description=spec[\"schema\"].get(\"description\", \"\"),\n            columns=llm_cols,\n        )\n        ctx[\"enrichment\"][out_name] = enrich\n        ctx[\"output_specs\"][out_name] = spec\n\n    print(f\"  Contract: {contract['provider']}\")\n    print(f\"  Model: {ctx['model']}\")\n    print(f\"  Outputs: {list(ctx['schemas'].keys())}\")\n    return ctx\n\n\ndef fetch(ctx, doc_path):\n    \"\"\"Auto-detect format, classify tables, compress to pipe-table markdown.\"\"\"\n    doc_path = str(doc_path)\n    doc_ctx = {\"doc_path\": doc_path, \"compressed_by_category\": {}}\n\n    # Extract report_date from filename if pattern is defined\n    pattern = ctx[\"contract\"].get(\"report_date_pattern\")\n    if pattern:\n        m = re.search(pattern, Path(doc_path).name)\n        doc_ctx[\"report_date\"] = m.group(1) if m else \"\"\n    else:\n        doc_ctx[\"report_date\"] = \"\"\n\n    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n\n    if is_docx:\n        classes = classify_docx_tables(doc_path, ctx[\"categories\"])\n        for out_name, spec in ctx[\"output_specs\"].items():\n            cat = spec[\"category\"]\n            indices = [c[\"index\"] for c in classes if c[\"category\"] == cat]\n            if indices:\n                doc_ctx[\"compressed_by_category\"][out_name] = compress_docx_tables(\n                    doc_path, table_indices=indices\n                )\n    else:\n        # PDF: compress → structured tables → classify\n        compressed_text = compress_spatial_text(doc_path, refine_headers=True)\n        structured = compress_spatial_text_structured(doc_path)\n        tuples = [t.to_compressed() for t in structured]\n        classes = classify_tables(tuples, ctx[\"categories\"]) if tuples else []\n\n        for out_name, spec in ctx[\"output_specs\"].items():\n            cat = spec[\"category\"]\n            matched = [c for c in classes if c[\"category\"] == cat]\n            if matched:\n                # For PDF, store the full compressed text\n                doc_ctx[\"compressed_by_category\"][out_name] = compressed_text\n                doc_ctx[\"pdf_path\"] = doc_path\n\n    cats_found = list(doc_ctx[\"compressed_by_category\"].keys())\n    print(f\"  {Path(doc_path).name[:50]}: categories={cats_found}\")\n    return doc_ctx\n\n\ndef _apply_enrichment(df, enrichment, doc_ctx, meta=None):\n    \"\"\"Apply enrichment columns (title, report_date, constant) to a DataFrame.\"\"\"\n    for col_name, spec in enrichment.items():\n        src = spec[\"source\"]\n        if src == \"title\" and meta:\n            df[col_name] = meta.get(\"title\") or \"Unknown\"\n        elif src == \"report_date\":\n            val = doc_ctx.get(\"report_date\", \"\")\n            if \"suffix\" in spec:\n                val += spec[\"suffix\"]\n            df[col_name] = val\n        elif src == \"constant\":\n            df[col_name] = spec[\"value\"]\n    return df\n\n\ndef _apply_formatting(df, col_specs):\n    \"\"\"Apply column-level format and filter transformations.\"\"\"\n    for col_spec in col_specs:\n        cn = col_spec[\"name\"]\n        # Format\n        fmt = col_spec.get(\"format\")\n        if fmt and cn in df.columns:\n            if fmt == \"lowercase\":\n                df[cn] = df[cn].astype(str).str.lower()\n            elif fmt == \"uppercase\":\n                df[cn] = df[cn].astype(str).str.upper()\n            elif fmt == \"titlecase\":\n                df[cn] = df[cn].astype(str).str.title()\n        # Filter\n        filt = col_spec.get(\"filter\")\n        if filt and filt != \"all\" and cn in df.columns:\n            if filt == \"latest\":\n                df = df[df[cn] == df[cn].max()]\n            elif filt == \"earliest\":\n                df = df[df[cn] == df[cn].min()]\n    return df\n\n\nasync def _transform_output_async(ctx, doc_ctx, out_name, data):\n    \"\"\"Async: interpret one output category. Returns (out_name, DataFrame).\"\"\"\n    schema = ctx[\"schemas\"][out_name]\n    spec = ctx[\"output_specs\"][out_name]\n    doc_path = doc_ctx[\"doc_path\"]\n    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n\n    # Resolve dynamic pivot aliases\n    for col in schema.columns:\n        col_spec = next(\n            (c for c in spec[\"schema\"][\"columns\"] if c[\"name\"] == col.name), None\n        )\n        if col_spec and col_spec.get(\"dynamic_aliases\") == \"pivot\":\n            if is_docx and isinstance(data, list):\n                pivot_vals = extract_pivot_values(data[0][0])\n                col.aliases = pivot_vals[-2:]\n\n    if is_docx and isinstance(data, list):\n        # DOCX: multiple independent tables → native BAML async\n        texts = [md for md, _ in data]\n        mapped_list = await interpret_tables_async(texts, schema, model=ctx[\"model\"])\n        frames = []\n        for (md, meta), mapped in zip(data, mapped_list):\n            df = to_pandas(mapped, schema)\n            df = _apply_enrichment(df, ctx[\"enrichment\"][out_name], doc_ctx, meta)\n            frames.append(df)\n        df_out = pd.concat(frames, ignore_index=True)\n    else:\n        # PDF: split pages, call BAML async directly (no sync wrapper, no extra threads)\n        pages = _split_pages(data, \"\\f\")\n        result = await _interpret_pages_batched_async(\n            pages, schema, model=ctx[\"model\"],\n        )\n        df_out = to_pandas(result, schema)\n        df_out = _apply_enrichment(df_out, ctx[\"enrichment\"][out_name], doc_ctx)\n\n    df_out = _apply_formatting(df_out, spec[\"schema\"][\"columns\"])\n\n    # Reorder columns to match contract\n    col_order = [c[\"name\"] for c in spec[\"schema\"][\"columns\"] if c[\"name\"] in df_out.columns]\n    return out_name, df_out[col_order]\n\n\nasync def transform_async(ctx, doc_ctx):\n    \"\"\"Async: interpret ALL output categories concurrently.\"\"\"\n    # Launch all categories in parallel (e.g. harvest + planting at the same time)\n    tasks = [\n        _transform_output_async(ctx, doc_ctx, out_name, data)\n        for out_name, data in doc_ctx[\"compressed_by_category\"].items()\n    ]\n    results = await asyncio.gather(*tasks)\n    doc_ctx[\"dataframes\"] = dict(results)\n    return doc_ctx\n\n\ndef save(results, output_dir=\"outputs\"):\n    \"\"\"Merge DataFrames across documents, write each output to Parquet.\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    merged = {}\n\n    for doc_ctx in results:\n        for out_name, df in doc_ctx.get(\"dataframes\", {}).items():\n            if out_name not in merged:\n                merged[out_name] = []\n            merged[out_name].append(df)\n\n    paths = {}\n    for out_name, frames in merged.items():\n        df = pd.concat(frames, ignore_index=True)\n        path = output_dir / f\"{out_name}.parquet\"\n        df.to_parquet(path, index=False)\n        paths[out_name] = path\n        print(f\"  {out_name}: {path} ({len(df)} rows)\")\n\n    return merged, paths\n\n\nasync def run_pipeline_async(contract_path, doc_paths, output_dir=\"outputs\"):\n    \"\"\"Orchestrate full pipeline with document-level concurrency.\"\"\"\n    t0 = time.perf_counter()\n\n    print(\"PREPARE\")\n    ctx = prepare(contract_path)\n\n    print(\"\\nFETCH\")\n    doc_ctxs = [fetch(ctx, p) for p in doc_paths]\n\n    print(\"\\nTRANSFORM (async)\")\n    results = await asyncio.gather(\n        *[transform_async(ctx, dc) for dc in doc_ctxs]\n    )\n\n    print(\"\\nSAVE\")\n    merged, paths = save(results, output_dir)\n\n    elapsed = time.perf_counter() - t0\n    print(f\"\\nDone in {elapsed:.1f}s\")\n    return results, merged, paths, elapsed\n\n\nprint(\"Pipeline functions defined: prepare -> fetch -> transform_async -> save\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 1: Russian Agricultural DOCX Reports\n",
    "\n",
    "Multi-category extraction from Russian Ministry of Agriculture weekly grain reports.\n",
    "Each DOCX contains harvest, planting, and export tables. The contract classifies\n",
    "tables by keywords and extracts crop names from table titles, report dates from\n",
    "filenames, and year values from dynamic pivot headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Russian agricultural contract\n",
    "ru_ctx = prepare(\"contracts/ru_ag_ministry.yaml\")\n",
    "\n",
    "print(\"\\nSchema summary:\")\n",
    "for name, schema in ru_ctx[\"schemas\"].items():\n",
    "    cols = [c.name for c in schema.columns]\n",
    "    enrich = list(ru_ctx[\"enrichment\"][name].keys())\n",
    "    print(f\"  {name}: LLM cols={cols}, enriched={enrich}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the June DOCX\n",
    "june_path = \"inputs/docx/input/2025-06-24_11-58-45.Russian weekly grain EOW June 20-21 2025-1.docx\"\n",
    "\n",
    "results_june, merged_june, _, elapsed_june = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.yaml\", [june_path], output_dir=\"outputs/ru_june\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side: DOCX page | pipe-table | DataFrame (first available category)\ntry:\n    img_b64, _ = render_document_page(june_path, page=0, dpi=120)\nexcept Exception:\n    img_b64 = None  # LibreOffice not available\n\n# Get the first available category from this document\njune_doc_ctx = results_june[0]\nfirst_cat = next(iter(june_doc_ctx[\"compressed_by_category\"]), None)\n\nif first_cat:\n    cat_data = june_doc_ctx[\"compressed_by_category\"][first_cat]\n    sample_md = cat_data[0][0]\n    df_display = june_doc_ctx[\"dataframes\"][first_cat]\n    print(f\"Showing category: {first_cat} ({len(df_display)} rows)\")\n    side_by_side_display(image_b64=img_b64, compressed_text=sample_md, dataframe=df_display)\nelse:\n    print(\"No tables classified for this document.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display output DataFrames\n",
    "for name, frames in merged_june.items():\n",
    "    df = frames[0] if isinstance(frames, list) else frames\n",
    "    print(f\"=== {name.upper()} ({len(df)} rows) ===\")\n",
    "    display(df.head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the July DOCX — same contract, different document\n",
    "july_path = \"inputs/docx/input/2025-07-17_10-16-25.Russian weekly grain EOW July 11-12 2025-1.docx\"\n",
    "\n",
    "results_july, merged_july, _, elapsed_july = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.yaml\", [july_path], output_dir=\"outputs/ru_july\"\n",
    ")\n",
    "\n",
    "print(f\"\\nJune: {elapsed_june:.1f}s, July: {elapsed_july:.1f}s\")\n",
    "for name in merged_july:\n",
    "    df = merged_july[name][0] if isinstance(merged_july[name], list) else merged_july[name]\n",
    "    print(f\"  {name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 2: Australian Shipping Stems — XXL\n",
    "\n",
    "**One canonical model, six providers, full concurrency.**\n",
    "\n",
    "6 shipping stem PDFs from 6 different providers — each expresses the same semantic data\n",
    "(vessel name, port, commodity, tonnage, ETA) with completely different layouts, column\n",
    "names, and formatting. A single canonical schema with rich aliases normalizes them all\n",
    "into one unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shipping stem contract\n",
    "ship_ctx = prepare(\"contracts/au_shipping_stem.yaml\")\n",
    "\n",
    "print(\"\\nSchema columns with aliases:\")\n",
    "for col in ship_ctx[\"schemas\"][\"vessels\"].columns:\n",
    "    print(f\"  {col.name:15s} {col.type:6s} aliases={col.aliases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all 6 PDFs\n",
    "shipping_pdfs = {\n",
    "    \"Newcastle\":  \"inputs/2857439.pdf\",\n",
    "    \"Bunge\":      \"inputs/Bunge_loadingstatement_2025-09-25.pdf\",\n",
    "    \"CBH\":        \"inputs/CBH Shipping Stem 26092025.pdf\",\n",
    "    \"GrainCorp\":  \"inputs/shipping-stem-2025-11-13.pdf\",\n",
    "    \"Riordan\":    \"inputs/shipping_stem-accc-30092025-1.pdf\",\n",
    "    \"Queensland\": \"inputs/document (1).pdf\",\n",
    "}\n",
    "\n",
    "import fitz\n",
    "print(f\"{'Provider':<14s} {'Filename':<50s} {'Pages':>5s}\")\n",
    "print(\"-\" * 72)\n",
    "for provider, path in shipping_pdfs.items():\n",
    "    doc = fitz.open(path)\n",
    "    pages = len(doc)\n",
    "    doc.close()\n",
    "    print(f\"{provider:<14s} {Path(path).name:<50s} {pages:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ALL 6 PDFs concurrently\n",
    "results_ship, merged_ship, paths_ship, elapsed_ship = await run_pipeline_async(\n",
    "    \"contracts/au_shipping_stem.yaml\",\n",
    "    list(shipping_pdfs.values()),\n",
    "    output_dir=\"outputs/shipping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-document summary\n",
    "print(f\"{'Provider':<14s} {'Records':>8s}\")\n",
    "print(\"-\" * 24)\n",
    "total = 0\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    n = sum(len(df) for df in doc_ctx.get(\"dataframes\", {}).values())\n",
    "    total += n\n",
    "    print(f\"{provider:<14s} {n:>8d}\")\n",
    "print(\"-\" * 24)\n",
    "print(f\"{'TOTAL':<14s} {total:>8d}\")\n",
    "print(f\"\\nWall-clock time: {elapsed_ship:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side gallery — one representative page per provider\n",
    "for provider, path in list(shipping_pdfs.items())[:3]:\n",
    "    img_b64, _ = render_document_page(path, page=0, dpi=100)\n",
    "    compressed = compress_spatial_text(path, refine_headers=False)\n",
    "    # Truncate compressed text for display\n",
    "    lines = compressed.splitlines()[:25]\n",
    "    truncated = \"\\n\".join(lines) + \"\\n...\"\n",
    "    print(f\"\\n--- {provider} ---\")\n",
    "    side_by_side_display(image_b64=img_b64, compressed_text=truncated, max_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unified DataFrame — all providers in one schema\ndf_vessel_frames = merged_ship[\"vessels\"]\ndf_all = pd.concat(df_vessel_frames, ignore_index=True) if isinstance(df_vessel_frames, list) else df_vessel_frames\nprint(f\"Unified DataFrame: {df_all.shape}\")\ndisplay(df_all.head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by provider (inferred from source document)\n",
    "# Add source_provider from doc_ctx ordering\n",
    "provider_frames = []\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    for df in doc_ctx.get(\"dataframes\", {}).values():\n",
    "        dfp = df.copy()\n",
    "        dfp[\"source_provider\"] = provider\n",
    "        provider_frames.append(dfp)\n",
    "\n",
    "if provider_frames:\n",
    "    df_with_provider = pd.concat(provider_frames, ignore_index=True)\n",
    "    print(\"Records by provider:\")\n",
    "    print(df_with_provider.groupby(\"source_provider\").size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 3: ACEA Car Registrations\n",
    "\n",
    "**Normalization: pivoted table to flat records.**\n",
    "\n",
    "The ACEA press release PDF contains a dense pivoted table: 28 countries x 7 power types\n",
    "x 3 metrics. The pipeline unpivots this into flat records — a 23-column wide table\n",
    "becomes a 4-column long DataFrame. Vision-enabled interpretation handles the dense\n",
    "hierarchical headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ACEA contract\n",
    "acea_ctx = prepare(\"contracts/acea_car_registrations.yaml\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for col in acea_ctx[\"schemas\"][\"registrations_by_market\"].columns:\n",
    "    print(f\"  {col.name:25s} {col.type:6s} {col.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ACEA press release PDF (vision-enabled)\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "\n",
    "results_acea, merged_acea, _, elapsed_acea = await run_pipeline_async(\n",
    "    \"contracts/acea_car_registrations.yaml\",\n",
    "    [acea_pdf],\n",
    "    output_dir=\"outputs/acea\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF | compressed pivot table | unpivoted DataFrame\n",
    "img_b64, _ = render_document_page(acea_pdf, page=0, dpi=120)\n",
    "acea_compressed = compress_spatial_text(acea_pdf, refine_headers=False)\n",
    "acea_df = list(merged_acea.values())[0]\n",
    "if isinstance(acea_df, list):\n",
    "    acea_df = acea_df[0]\n",
    "\n",
    "# Show first page of compressed text\n",
    "first_page = acea_compressed.split(\"\\f\")[0]\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=first_page, dataframe=acea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unpivoted DataFrame\n",
    "print(f\"Shape: {acea_df.shape} (from wide pivoted table to long flat records)\")\n",
    "display(acea_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip verification: pivot back to wide format\n",
    "if \"country\" in acea_df.columns and \"car_motorization\" in acea_df.columns:\n",
    "    try:\n",
    "        pivot = acea_df.pivot_table(\n",
    "            index=\"country\",\n",
    "            columns=\"car_motorization\",\n",
    "            values=\"new_car_registration\",\n",
    "            aggfunc=\"sum\",\n",
    "        )\n",
    "        print(f\"Pivoted back: {pivot.shape} (countries x power types)\")\n",
    "        display(pivot.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Pivot failed: {e}\")\n",
    "else:\n",
    "    print(\"Columns not available for pivot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Use Case | Format | Documents | Elapsed | Key Feature |\n",
    "|----------|--------|-----------|---------|-------------|\n",
    "| Russian Agriculture | DOCX | 2 reports | see above | Multi-category, pivot years, enrichment |\n",
    "| Australian Shipping | PDF | 6 providers | see above | One schema, 6 layouts, full concurrency |\n",
    "| ACEA Registrations | PDF | 1 press release | see above | Pivoted → flat normalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_docs = 2 + 6 + 1  # June + July + 6 shipping + 1 ACEA\n",
    "total_records = 0\n",
    "for m in [merged_june, merged_july, merged_ship, merged_acea]:\n",
    "    for v in m.values():\n",
    "        df = v[0] if isinstance(v, list) else v\n",
    "        total_records += len(df)\n",
    "\n",
    "total_time = elapsed_june + elapsed_july + elapsed_ship + elapsed_acea\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Total records extracted:   {total_records:,}\")\n",
    "print(f\"Total wall-clock time:     {total_time:.1f}s\")\n",
    "print(f\"Throughput:                {total_records / total_time:.0f} records/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}