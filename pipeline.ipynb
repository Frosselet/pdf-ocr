{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Contract-Driven Data Pipelines\n\nExtract structured data from heterogeneous documents using declarative JSON contracts.\n\nOne schema. Many providers. Full async."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Design\n\nA **data contract** (JSON) declares everything the pipeline needs:\n- Which LLM model to use\n- How to classify tables (keyword matching)\n- What output schema to produce (columns, types, aliases, formats)\n- How to enrich records (constants, metadata from filenames/titles)\n\nThe pipeline code is 100% generic — swap the contract, not the code.\n\nThe interpretation step uses a **deterministic-first** architecture: when schema\naliases fully cover every ` / `-separated header part, records are built via pure\nstring matching with zero LLM calls. The LLM pipeline activates only as a fallback\nfor pages with unmatched columns.\n\n**Three use cases** demonstrate the same pipeline across different domains:\n\n| # | Use Case | Format | Documents | Challenge |\n|---|----------|--------|-----------|----------|\n| 1 | Russian agricultural reports | DOCX | 7 weekly reports | Multi-category extraction, dynamic pivot years, deterministic mapping |\n| 2 | Australian shipping stems | PDF | 6 providers | One canonical model, 6 different layouts, full concurrency |\n| 3 | ACEA car registrations | PDF | 1 press release | Pivoted table → flat records, deterministic unpivoting |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport asyncio\nimport time\nfrom pathlib import Path\n\nimport nest_asyncio\nimport pandas as pd\n\nfrom pdf_ocr import (\n    CanonicalSchema, ColumnDef,\n    compress_spatial_text,\n    to_pandas, to_records, to_parquet,\n)\n\nnest_asyncio.apply()\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG. Supports PDF and DOCX.\"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline Architecture\n\n```\ncontract (JSON) → load_contract() → process_document_async() → save()\n```\n\n**Deterministic bypass**: Before any LLM call, each page is tested for deterministic\nmapping. If every ` / `-separated header part matches a schema alias, records are built\nvia string matching — no LLM call, no latency, no cost. This fires automatically for\nRussian DOCX (compound headers) and ACEA PDF (hierarchical headers) when the contract\nhas complete aliases.\n\nFive levels of concurrency — all on a single event loop, no sync wrappers:\n\n| Level | Scope | Pattern |\n|---|---|---|\n| **Document-level** | Process N documents simultaneously | `asyncio.gather(*[process_document_async(doc, cc) for doc in docs])` |\n| **Compression** | PDF compression in thread pool (CPU-bound) | `asyncio.to_thread(compress_spatial_text, ...)` |\n| **Category-level** | Multiple outputs per document (e.g. harvest + planting) | `asyncio.gather(*[interpret_output_async(...)])` |\n| **Table/Page-level** | DOCX: N tables concurrent; PDF: N pages concurrent | `interpret_tables_async()` / `_interpret_pages_batched_async()` |\n| **Batch-level** | Step 2 mapping in chunks of 20 rows | Built into `_interpret_pages_batched_async()` |\n\nKey optimization: compression and interpretation are **pipelined per document** — as soon as\none PDF finishes compression, its LLM interpretation starts immediately while other PDFs\nare still compressing. No sequential fetch barrier.\n\nThe three pipeline helpers live in `pdf_ocr.pipeline`:\n- `compress_and_classify_async()` — compress + classify (Layer 1)\n- `interpret_output_async()` — interpret + enrich + format one category (Layer 2)\n- `process_document_async()` — full per-document pipeline (Layer 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Pipeline imports ──────────────────────────────────────────────────────────\n# All orchestration lives in pdf_ocr.pipeline — nothing to define here.\n\nfrom pdf_ocr.contracts import load_contract\nfrom pdf_ocr.pipeline import process_document_async, run_pipeline_async, save, DocumentResult\n\nprint(\"Pipeline imports ready: load_contract, run_pipeline_async, save, process_document_async\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 1: Russian Agricultural DOCX Reports\n\nMulti-category extraction from Russian Ministry of Agriculture weekly grain reports.\nEach DOCX contains harvest, planting, and export tables. The contract classifies\ntables by keywords and extracts crop names from table titles, report dates from\nfilenames, and year values from dynamic pivot headers.\n\n**7 weekly reports** processed concurrently in a single `run_pipeline_async()` call —\neach document independently compressed, classified, and interpreted.\n\n**Deterministic mapping**: The DOCX extractor produces compound headers with ` / `\nseparators (e.g., `spring crops / MOA Target 2025`). When the contract aliases cover\nevery header part, the interpretation step resolves the full table — including\nunpivoting across crop groups — with zero LLM calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the Russian agricultural contract\nru_cc = load_contract(\"contracts/ru_ag_ministry.json\")\nprint(f\"  Contract: {ru_cc.provider}, Model: {ru_cc.model}, Outputs: {list(ru_cc.outputs.keys())}\")\n\nprint(\"\\nSchema summary:\")\nfor name, spec in ru_cc.outputs.items():\n    cols = [c.name for c in spec.schema.columns]\n    enrich = list(spec.enrichment.keys())\n    print(f\"  {name}: LLM cols={cols}, enriched={enrich}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run pipeline on ALL 7 DOCX reports concurrently\nimport re\n\nru_docs = sorted(Path(\"inputs/docx/input\").glob(\"*.docx\"))\nprint(f\"Found {len(ru_docs)} DOCX reports:\")\nfor p in ru_docs:\n    print(f\"  {p.name}\")\n\nresults_ru, merged_ru, paths_ru, elapsed_ru = await run_pipeline_async(\n    \"contracts/ru_ag_ministry.json\",\n    [str(p) for p in ru_docs],\n    output_dir=\"outputs/ru\",\n)\n\n# Write per-document Parquets into subdirectories\noutput_names = list(ru_cc.outputs.keys())\nfor i, result in enumerate(results_ru):\n    slug = re.sub(r\"[^\\w\\-]\", \"_\", result.report_date.lower()).strip(\"_\") if result.report_date else f\"doc_{i:02d}\"\n    doc_dir = Path(\"outputs/ru\") / slug\n    doc_dir.mkdir(parents=True, exist_ok=True)\n    for name in output_names:\n        df = result.dataframes.get(name)\n        if df is not None and len(df) > 0:\n            df.to_parquet(doc_dir / f\"{name}.parquet\", index=False)\n    written = list(doc_dir.glob(\"*.parquet\"))\n    print(f\"  {slug}/: {[p.name for p in written] or '(empty)'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-document summary\nsummary_rows = []\nfor result in results_ru:\n    name = Path(result.doc_path).name[:60]\n    row = {\"document\": name, \"report_date\": result.report_date}\n    for out_name, df in result.dataframes.items():\n        row[f\"{out_name}_rows\"] = len(df)\n    summary_rows.append(row)\n\ndf_summary = pd.DataFrame(summary_rows)\nprint(f\"Processed {len(results_ru)} documents in {elapsed_ru:.1f}s\")\ndisplay(df_summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-document output — display both tables for each DOCX\nfor result in results_ru:\n    doc_name = Path(result.doc_path).name[:65]\n    print(f\"\\n{'='*80}\")\n    print(f\"  {doc_name}\")\n    print(f\"  report_date: {result.report_date!r}\")\n    print(f\"{'='*80}\")\n    for name in output_names:\n        df = result.dataframes.get(name)\n        if df is not None and len(df) > 0:\n            print(f\"\\n  --- {name} ({len(df)} rows) ---\")\n            display(df.head(8))\n        else:\n            print(f\"\\n  --- {name}: EMPTY ---\")"
  },
  {
   "cell_type": "code",
   "source": "# List all Parquet files under outputs/ru/ and display each one in full\nru_parquets = sorted(Path(\"outputs/ru\").rglob(\"*.parquet\"))\nprint(f\"Found {len(ru_parquets)} Parquet files:\\n\")\n\nfor p in ru_parquets:\n    df = pd.read_parquet(p)\n    label = str(p.relative_to(\"outputs/ru\"))\n    print(f\"{'='*80}\")\n    print(f\"  {label}  —  {len(df)} rows x {len(df.columns)} cols\")\n    print(f\"{'='*80}\")\n    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n        display(df)\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side: DOCX page | pipe-table | DataFrame (first document, first category)\nsample_result = results_ru[0]\nsample_path = sample_result.doc_path\n\ntry:\n    img_b64, _ = render_document_page(sample_path, page=0, dpi=120)\nexcept Exception:\n    img_b64 = None  # LibreOffice not available\n\nfirst_cat = next(iter(sample_result.dataframes), None)\nif first_cat:\n    cat_data = sample_result.compressed_by_category.get(first_cat)\n    sample_md = cat_data[0][0] if isinstance(cat_data, list) else cat_data.split(\"\\f\")[0]\n    df_display = sample_result.dataframes[first_cat]\n    print(f\"Sample: {Path(sample_path).name[:50]} — {first_cat} ({len(df_display)} rows)\")\n    side_by_side_display(image_b64=img_b64, compressed_text=sample_md, dataframe=df_display)\nelse:\n    print(\"No tables classified for this document.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 2: Australian Shipping Stems — XXL\n",
    "\n",
    "**One canonical model, six providers, full concurrency.**\n",
    "\n",
    "6 shipping stem PDFs from 6 different providers — each expresses the same semantic data\n",
    "(vessel name, port, commodity, tonnage, ETA) with completely different layouts, column\n",
    "names, and formatting. A single canonical schema with rich aliases normalizes them all\n",
    "into one unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the shipping stem contract\nship_cc = load_contract(\"contracts/au_shipping_stem.json\")\nprint(f\"  Contract: {ship_cc.provider}, Model: {ship_cc.model}, Outputs: {list(ship_cc.outputs.keys())}\")\n\nprint(\"\\nSchema columns with aliases:\")\nfor col in ship_cc.outputs[\"vessels\"].schema.columns:\n    print(f\"  {col.name:15s} {col.type:6s} aliases={col.aliases}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all 6 PDFs\n",
    "shipping_pdfs = {\n",
    "    \"Newcastle\":  \"inputs/2857439.pdf\",\n",
    "    \"Bunge\":      \"inputs/Bunge_loadingstatement_2025-09-25.pdf\",\n",
    "    \"CBH\":        \"inputs/CBH Shipping Stem 26092025.pdf\",\n",
    "    \"GrainCorp\":  \"inputs/shipping-stem-2025-11-13.pdf\",\n",
    "    \"Riordan\":    \"inputs/shipping_stem-accc-30092025-1.pdf\",\n",
    "    \"Queensland\": \"inputs/document (1).pdf\",\n",
    "}\n",
    "\n",
    "import fitz\n",
    "print(f\"{'Provider':<14s} {'Filename':<50s} {'Pages':>5s}\")\n",
    "print(\"-\" * 72)\n",
    "for provider, path in shipping_pdfs.items():\n",
    "    doc = fitz.open(path)\n",
    "    pages = len(doc)\n",
    "    doc.close()\n",
    "    print(f\"{provider:<14s} {Path(path).name:<50s} {pages:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ALL 6 PDFs concurrently\n",
    "results_ship, merged_ship, paths_ship, elapsed_ship = await run_pipeline_async(\n",
    "    \"contracts/au_shipping_stem.json\",\n",
    "    list(shipping_pdfs.values()),\n",
    "    output_dir=\"outputs/shipping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-document summary\nprint(f\"{'Provider':<14s} {'Records':>8s}\")\nprint(\"-\" * 24)\ntotal = 0\nfor (provider, _), result in zip(shipping_pdfs.items(), results_ship):\n    n = sum(len(df) for df in result.dataframes.values())\n    total += n\n    print(f\"{provider:<14s} {n:>8d}\")\nprint(\"-\" * 24)\nprint(f\"{'TOTAL':<14s} {total:>8d}\")\nprint(f\"\\nWall-clock time: {elapsed_ship:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side gallery — one representative page per provider\n",
    "for provider, path in list(shipping_pdfs.items())[:3]:\n",
    "    img_b64, _ = render_document_page(path, page=0, dpi=100)\n",
    "    compressed = compress_spatial_text(path, refine_headers=False)\n",
    "    # Truncate compressed text for display\n",
    "    lines = compressed.splitlines()[:25]\n",
    "    truncated = \"\\n\".join(lines) + \"\\n...\"\n",
    "    print(f\"\\n--- {provider} ---\")\n",
    "    side_by_side_display(image_b64=img_b64, compressed_text=truncated, max_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unified DataFrame — all providers in one schema\ndf_vessel_frames = merged_ship[\"vessels\"]\ndf_all = pd.concat(df_vessel_frames, ignore_index=True) if isinstance(df_vessel_frames, list) else df_vessel_frames\nprint(f\"Unified DataFrame: {df_all.shape}\")\ndisplay(df_all.head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution by provider (inferred from source document)\n# Add source_provider from result ordering\nprovider_frames = []\nfor (provider, _), result in zip(shipping_pdfs.items(), results_ship):\n    for df in result.dataframes.values():\n        dfp = df.copy()\n        dfp[\"source_provider\"] = provider\n        provider_frames.append(dfp)\n\nif provider_frames:\n    df_with_provider = pd.concat(provider_frames, ignore_index=True)\n    print(\"Records by provider:\")\n    print(df_with_provider.groupby(\"source_provider\").size().to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 3: ACEA Car Registrations\n\n**Normalization: pivoted table to flat records.**\n\nThe ACEA press release PDF contains a dense pivoted table: 28 countries x 7 power types\nx 3 metrics. The pipeline unpivots this into flat records — a 23-column wide table\nbecomes a 4-column long DataFrame.\n\n**Deterministic mapping**: After header separator standardization (stacked headers\njoined with ` / `), headers like `BATTERY ELECTRIC / Dec-25` have parts that match\nschema aliases for motorization type and period. When aliases cover all header parts,\nthe deterministic mapper handles the full unpivot — each data row produces one record\nper power type — with zero LLM calls. Vision mode is only needed when headers are\ngarbled and aliases cannot match."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the ACEA contract\nacea_cc = load_contract(\"contracts/acea_car_registrations.json\")\nprint(f\"  Contract: {acea_cc.provider}, Model: {acea_cc.model}, Outputs: {list(acea_cc.outputs.keys())}\")\n\nprint(\"\\nSchema:\")\nfor col in acea_cc.outputs[\"registrations_by_market\"].schema.columns:\n    print(f\"  {col.name:25s} {col.type:6s} {col.description}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ACEA press release PDF (vision-enabled)\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "\n",
    "results_acea, merged_acea, _, elapsed_acea = await run_pipeline_async(\n",
    "    \"contracts/acea_car_registrations.json\",\n",
    "    [acea_pdf],\n",
    "    output_dir=\"outputs/acea\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF | compressed pivot table | unpivoted DataFrame\n",
    "img_b64, _ = render_document_page(acea_pdf, page=0, dpi=120)\n",
    "acea_compressed = compress_spatial_text(acea_pdf, refine_headers=False)\n",
    "acea_df = list(merged_acea.values())[0]\n",
    "if isinstance(acea_df, list):\n",
    "    acea_df = acea_df[0]\n",
    "\n",
    "# Show first page of compressed text\n",
    "first_page = acea_compressed.split(\"\\f\")[0]\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=first_page, dataframe=acea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unpivoted DataFrame\n",
    "print(f\"Shape: {acea_df.shape} (from wide pivoted table to long flat records)\")\n",
    "display(acea_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip verification: pivot back to wide format\n",
    "if \"country\" in acea_df.columns and \"car_motorization\" in acea_df.columns:\n",
    "    try:\n",
    "        pivot = acea_df.pivot_table(\n",
    "            index=\"country\",\n",
    "            columns=\"car_motorization\",\n",
    "            values=\"new_car_registration\",\n",
    "            aggfunc=\"sum\",\n",
    "        )\n",
    "        print(f\"Pivoted back: {pivot.shape} (countries x power types)\")\n",
    "        display(pivot.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Pivot failed: {e}\")\n",
    "else:\n",
    "    print(\"Columns not available for pivot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| Use Case | Format | Documents | Output | Key Feature |\n|----------|--------|-----------|--------|-------------|\n| Russian Agriculture | DOCX | 7 reports | Parquet | Multi-category, pivot years, deterministic mapping |\n| Australian Shipping | PDF | 6 providers | CSV | One schema, 6 layouts, full concurrency |\n| ACEA Registrations | PDF | 1 press release | CSV | Deterministic unpivot, vision fallback |\n\n**Deterministic-first architecture**: Tables with complete alias coverage in the schema\nare interpreted via pure string matching — no LLM calls, no latency, no cost. The LLM\npipeline activates only as a fallback for pages with unmatched header parts.\n\n**Contract-driven output format**: The `filename` field in each contract output\ndetermines whether `save()` writes CSV, TSV, or Parquet — no code changes needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics\ntotal_docs = len(results_ru) + len(results_ship) + len(results_acea)\ntotal_records = 0\nfor m in [merged_ru, merged_ship, merged_acea]:\n    for v in m.values():\n        frames = v if isinstance(v, list) else [v]\n        total_records += sum(len(f) for f in frames)\n\ntotal_time = elapsed_ru + elapsed_ship + elapsed_acea\n\nprint(f\"Total documents processed: {total_docs}\")\nprint(f\"Total records extracted:   {total_records:,}\")\nprint(f\"Total wall-clock time:     {total_time:.1f}s\")\nprint(f\"Throughput:                {total_records / total_time:.0f} records/s\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}