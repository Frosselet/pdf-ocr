{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Contract-Driven Data Pipelines\n\nExtract structured data from heterogeneous documents using declarative JSON contracts.\n\nOne schema. Many providers. Full async."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Design\n\nA **data contract** (JSON) declares everything the pipeline needs:\n- Which LLM model to use\n- How to classify tables (keyword matching)\n- What output schema to produce (columns, types, aliases, formats)\n- How to enrich records (constants, metadata from filenames/titles)\n\nThe pipeline code is 100% generic — swap the contract, not the code.\n\nThe interpretation step uses a **deterministic-first** architecture: when schema\naliases fully cover every ` / `-separated header part, records are built via pure\nstring matching with zero LLM calls. The LLM pipeline activates only as a fallback\nfor pages with unmatched columns.\n\n**Three use cases** demonstrate the same pipeline across different domains:\n\n| # | Use Case | Format | Documents | Challenge |\n|---|----------|--------|-----------|----------|\n| 1 | Russian agricultural reports | DOCX | 2 weekly reports | Multi-category extraction, dynamic pivot years, deterministic mapping |\n| 2 | Australian shipping stems | PDF | 6 providers | One canonical model, 6 different layouts, full concurrency |\n| 3 | ACEA car registrations | PDF | 1 press release | Pivoted table → flat records, deterministic unpivoting |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport asyncio\nimport json\nimport re\nimport time\nfrom pathlib import Path\n\nimport nest_asyncio\nimport pandas as pd\n\nfrom pdf_ocr import (\n    CanonicalSchema, ColumnDef,\n    classify_tables, classify_docx_tables,\n    compress_spatial_text, compress_spatial_text_structured,\n    compress_docx_tables, extract_pivot_values,\n    interpret_tables_async,\n    to_pandas, to_records, to_parquet,\n)\n# Direct async imports — avoid sync wrappers that spawn extra threads\nfrom pdf_ocr.interpret import _interpret_pages_batched_async, _split_pages\n\nnest_asyncio.apply()\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG. Supports PDF and DOCX.\"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline Architecture\n\n```\ncontract (JSON) → prepare() → process_documents_async() → save()\n```\n\n**Deterministic bypass**: Before any LLM call, each page is tested for deterministic\nmapping. If every ` / `-separated header part matches a schema alias, records are built\nvia string matching — no LLM call, no latency, no cost. This fires automatically for\nRussian DOCX (compound headers) and ACEA PDF (hierarchical headers) when the contract\nhas complete aliases.\n\nFive levels of concurrency — all on a single event loop, no sync wrappers:\n\n| Level | Scope | Pattern |\n|---|---|---|\n| **Document-level** | Process N documents simultaneously | `asyncio.gather(*[_process_doc_async(doc) for doc in docs])` |\n| **Compression** | PDF compression in thread pool (CPU-bound) | `asyncio.to_thread(compress_spatial_text, ...)` |\n| **Category-level** | Multiple outputs per document (e.g. harvest + planting) | `asyncio.gather(*[_transform_output_async(...)])` |\n| **Table/Page-level** | DOCX: N tables concurrent; PDF: N pages concurrent | `interpret_tables_async()` / `_interpret_pages_batched_async()` |\n| **Batch-level** | Step 2 mapping in chunks of 20 rows | Built into `_interpret_pages_batched_async()` |\n\nKey optimization: compression and interpretation are **pipelined per document** — as soon as\none PDF finishes compression, its LLM interpretation starts immediately while other PDFs\nare still compressing. No sequential fetch barrier."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Generic Pipeline Functions ────────────────────────────────────────────────\n# These functions are contract-agnostic. Swap the JSON contract and re-run.\n\nfrom pdf_ocr.contracts import (\n    ContractContext,\n    OutputSpec,\n    load_contract,\n    resolve_year_templates,\n    enrich_dataframe,\n    format_dataframe,\n    _YEAR_TEMPLATE_RE,\n)\nfrom pdf_ocr.interpret import UnpivotStrategy\nfrom pdf_ocr.report_date import ReportDateConfig, resolve_report_date\n\n\ndef prepare(contract_path):\n    \"\"\"Load JSON contract → parse categories, schemas, enrichment rules.\n\n    Thin adapter: calls load_contract() and converts to the dict format\n    expected by the rest of this notebook's orchestration functions.\n    \"\"\"\n    cc = load_contract(contract_path)\n\n    ctx = {\"contract\": cc.raw, \"model\": cc.model}\n    ctx[\"categories\"] = cc.categories\n    ctx[\"report_date_config\"] = cc.report_date_config\n    ctx[\"unpivot\"] = cc.unpivot\n\n    ctx[\"schemas\"] = {name: spec.schema for name, spec in cc.outputs.items()}\n    ctx[\"enrichment\"] = {name: spec.enrichment for name, spec in cc.outputs.items()}\n    ctx[\"output_specs\"] = {\n        name: cc.raw[\"outputs\"][name] for name in cc.outputs\n    }\n\n    print(f\"  Contract: {cc.provider}\")\n    print(f\"  Model: {cc.model}\")\n    print(f\"  Outputs: {list(cc.outputs.keys())}\")\n    return ctx\n\n\n# ── Per-output interpretation (runs concurrently across categories) ──────────\n\nasync def _transform_output_async(ctx, doc_ctx, out_name, data):\n    \"\"\"Async: interpret one output category. Returns (out_name, DataFrame).\"\"\"\n    schema = ctx[\"schemas\"][out_name]\n    spec = ctx[\"output_specs\"][out_name]\n    doc_path = doc_ctx[\"doc_path\"]\n    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n\n    # Resolve {YYYY} alias templates from document pivot years\n    has_templates = any(\n        _YEAR_TEMPLATE_RE.search(a)\n        for col in schema.columns for a in col.aliases\n    )\n    if has_templates:\n        if is_docx and isinstance(data, list):\n            all_years: set[str] = set()\n            for md, _ in data:\n                all_years.update(extract_pivot_values(md))\n            pivot_years = sorted(all_years)\n        else:\n            pivot_years = []  # PDF: not yet implemented\n        for col in schema.columns:\n            col.aliases = resolve_year_templates(col.aliases, pivot_years)\n\n    if is_docx and isinstance(data, list):\n        # DOCX: use batched pipeline (same as PDF) — each table is a \"page\"\n        # This ensures Step 2 (MapToCanonicalSchema) is batched in chunks of 20\n        # rows, preventing output truncation on large tables.\n        texts = [md for md, _ in data]\n        page_results = await _interpret_pages_batched_async(\n            texts, schema, model=ctx[\"model\"],\n            unpivot=ctx.get(\"unpivot\", True),\n        )\n        frames = []\n        for i, (md, meta) in enumerate(data):\n            mapped = page_results.get(i + 1)  # 1-indexed page numbers\n            if mapped is None:\n                continue\n            df = to_pandas(mapped, schema)\n            df = enrich_dataframe(\n                df, ctx[\"enrichment\"][out_name],\n                title=meta.get(\"title\"), report_date=doc_ctx.get(\"report_date\", \"\"),\n            )\n            frames.append(df)\n        df_out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n    else:\n        # PDF: split pages, call BAML async directly\n        pages = _split_pages(data, \"\\f\")\n        result = await _interpret_pages_batched_async(\n            pages, schema, model=ctx[\"model\"],\n            unpivot=ctx.get(\"unpivot\", True),\n        )\n        df_out = to_pandas(result, schema)\n        df_out = enrich_dataframe(\n            df_out, ctx[\"enrichment\"][out_name],\n            report_date=doc_ctx.get(\"report_date\", \"\"),\n        )\n\n    df_out = format_dataframe(df_out, spec[\"schema\"][\"columns\"])\n    col_order = [c[\"name\"] for c in spec[\"schema\"][\"columns\"] if c[\"name\"] in df_out.columns]\n    return out_name, df_out[col_order]\n\n\n# ── Per-document pipeline (compress → classify → interpret, fully async) ─────\n\nasync def _process_doc_async(ctx, doc_path):\n    \"\"\"Async: compress + classify + interpret a single document.\n\n    PDF compression runs in a thread pool so it overlaps with other documents.\n    Interpretation starts immediately once compression finishes — no barrier.\n    \"\"\"\n    doc_path = str(doc_path)\n    doc_ctx = {\"doc_path\": doc_path, \"compressed_by_category\": {}}\n\n    # Resolve report_date via declarative config (LLM-based or deterministic)\n    rd_config = ctx.get(\"report_date_config\")\n    if rd_config:\n        doc_ctx[\"report_date\"] = await resolve_report_date(rd_config, doc_path=doc_path)\n    else:\n        doc_ctx[\"report_date\"] = \"\"\n\n    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n    single_category = len(ctx[\"categories\"]) == 1\n\n    if is_docx:\n        # DOCX: classify + compress (pure Python, fast — no need for thread)\n        classes = classify_docx_tables(doc_path, ctx[\"categories\"])\n        for out_name, spec in ctx[\"output_specs\"].items():\n            cat = spec[\"category\"]\n            indices = [c[\"index\"] for c in classes if c[\"category\"] == cat]\n            if indices:\n                doc_ctx[\"compressed_by_category\"][out_name] = compress_docx_tables(\n                    doc_path, table_indices=indices\n                )\n    else:\n        # PDF: compress in thread pool (CPU-bound + LLM refinement call)\n        compressed_text = await asyncio.to_thread(\n            compress_spatial_text, doc_path, refine_headers=True\n        )\n\n        if single_category:\n            # Single category — skip structured extraction + classification entirely\n            for out_name in ctx[\"output_specs\"]:\n                doc_ctx[\"compressed_by_category\"][out_name] = compressed_text\n        else:\n            # Multi-category — need structured extraction for classification\n            structured = await asyncio.to_thread(\n                compress_spatial_text_structured, doc_path\n            )\n            tuples = [t.to_compressed() for t in structured]\n            classes = classify_tables(tuples, ctx[\"categories\"]) if tuples else []\n            for out_name, spec in ctx[\"output_specs\"].items():\n                cat = spec[\"category\"]\n                if any(c[\"category\"] == cat for c in classes):\n                    doc_ctx[\"compressed_by_category\"][out_name] = compressed_text\n\n        doc_ctx[\"pdf_path\"] = doc_path\n\n    # Interpret all output categories concurrently\n    tasks = [\n        _transform_output_async(ctx, doc_ctx, out_name, data)\n        for out_name, data in doc_ctx[\"compressed_by_category\"].items()\n    ]\n    if tasks:\n        results = await asyncio.gather(*tasks)\n        doc_ctx[\"dataframes\"] = dict(results)\n    else:\n        doc_ctx[\"dataframes\"] = {}\n\n    cats = list(doc_ctx[\"dataframes\"].keys())\n    rows = sum(len(df) for df in doc_ctx[\"dataframes\"].values())\n    print(f\"  {Path(doc_path).name[:50]}: {cats} → {rows} records\")\n    return doc_ctx\n\n\n# ── Orchestration ────────────────────────────────────────────────────────────\n\ndef save(results, output_dir=\"outputs\"):\n    \"\"\"Merge DataFrames across documents, write each output to Parquet.\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    merged = {}\n\n    for doc_ctx in results:\n        for out_name, df in doc_ctx.get(\"dataframes\", {}).items():\n            if out_name not in merged:\n                merged[out_name] = []\n            merged[out_name].append(df)\n\n    paths = {}\n    for out_name, frames in merged.items():\n        df = pd.concat(frames, ignore_index=True)\n        path = output_dir / f\"{out_name}.parquet\"\n        df.to_parquet(path, index=False)\n        paths[out_name] = path\n        print(f\"  {out_name}: {path} ({len(df)} rows)\")\n\n    return merged, paths\n\n\nasync def run_pipeline_async(contract_path, doc_paths, output_dir=\"outputs\"):\n    \"\"\"Orchestrate full pipeline with document-level concurrency.\n\n    All documents are processed in parallel: compression, classification,\n    and interpretation overlap across documents via asyncio.gather().\n    \"\"\"\n    t0 = time.perf_counter()\n\n    print(\"PREPARE\")\n    ctx = prepare(contract_path)\n\n    print(\"\\nPROCESS (async — compress + classify + interpret per document)\")\n    results = await asyncio.gather(\n        *[_process_doc_async(ctx, p) for p in doc_paths]\n    )\n\n    print(\"\\nSAVE\")\n    merged, paths = save(results, output_dir)\n\n    elapsed = time.perf_counter() - t0\n    print(f\"\\nDone in {elapsed:.1f}s\")\n    return results, merged, paths, elapsed\n\n\nprint(\"Pipeline functions defined: prepare -> run_pipeline_async -> save\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 1: Russian Agricultural DOCX Reports\n\nMulti-category extraction from Russian Ministry of Agriculture weekly grain reports.\nEach DOCX contains harvest, planting, and export tables. The contract classifies\ntables by keywords and extracts crop names from table titles, report dates from\nfilenames, and year values from dynamic pivot headers.\n\n**Deterministic mapping**: The DOCX extractor produces compound headers with ` / `\nseparators (e.g., `spring crops / MOA Target 2025`). When the contract aliases cover\nevery header part, the interpretation step resolves the full table — including\nunpivoting across crop groups — with zero LLM calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Russian agricultural contract\n",
    "ru_ctx = prepare(\"contracts/ru_ag_ministry.json\")\n",
    "\n",
    "print(\"\\nSchema summary:\")\n",
    "for name, schema in ru_ctx[\"schemas\"].items():\n",
    "    cols = [c.name for c in schema.columns]\n",
    "    enrich = list(ru_ctx[\"enrichment\"][name].keys())\n",
    "    print(f\"  {name}: LLM cols={cols}, enriched={enrich}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the June DOCX\n",
    "june_path = \"inputs/docx/input/2025-06-24_11-58-45.Russian weekly grain EOW June 20-21 2025-1.docx\"\n",
    "\n",
    "results_june, merged_june, _, elapsed_june = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.json\", [june_path], output_dir=\"outputs/ru_june\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side: DOCX page | pipe-table | DataFrame (first available category)\ntry:\n    img_b64, _ = render_document_page(june_path, page=0, dpi=120)\nexcept Exception:\n    img_b64 = None  # LibreOffice not available\n\n# Get the first available category from this document\njune_doc_ctx = results_june[0]\nfirst_cat = next(iter(june_doc_ctx[\"dataframes\"]), None)\n\nif first_cat:\n    cat_data = june_doc_ctx[\"compressed_by_category\"].get(first_cat)\n    # DOCX: list of (md, meta) tuples; PDF: compressed text string\n    if isinstance(cat_data, list):\n        sample_md = cat_data[0][0]\n    elif isinstance(cat_data, str):\n        sample_md = cat_data.split(\"\\f\")[0]  # first page\n    else:\n        sample_md = \"(no compressed text)\"\n    df_display = june_doc_ctx[\"dataframes\"][first_cat]\n    print(f\"Showing category: {first_cat} ({len(df_display)} rows)\")\n    side_by_side_display(image_b64=img_b64, compressed_text=sample_md, dataframe=df_display)\nelse:\n    print(\"No tables classified for this document.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display output DataFrames\n",
    "for name, frames in merged_june.items():\n",
    "    df = frames[0] if isinstance(frames, list) else frames\n",
    "    print(f\"=== {name.upper()} ({len(df)} rows) ===\")\n",
    "    display(df.head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the July DOCX — same contract, different document\n",
    "july_path = \"inputs/docx/input/2025-07-17_10-16-25.Russian weekly grain EOW July 11-12 2025-1.docx\"\n",
    "\n",
    "results_july, merged_july, _, elapsed_july = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.json\", [july_path], output_dir=\"outputs/ru_july\"\n",
    ")\n",
    "\n",
    "print(f\"\\nJune: {elapsed_june:.1f}s, July: {elapsed_july:.1f}s\")\n",
    "for name in merged_july:\n",
    "    df = merged_july[name][0] if isinstance(merged_july[name], list) else merged_july[name]\n",
    "    print(f\"  {name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 2: Australian Shipping Stems — XXL\n",
    "\n",
    "**One canonical model, six providers, full concurrency.**\n",
    "\n",
    "6 shipping stem PDFs from 6 different providers — each expresses the same semantic data\n",
    "(vessel name, port, commodity, tonnage, ETA) with completely different layouts, column\n",
    "names, and formatting. A single canonical schema with rich aliases normalizes them all\n",
    "into one unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shipping stem contract\n",
    "ship_ctx = prepare(\"contracts/au_shipping_stem.json\")\n",
    "\n",
    "print(\"\\nSchema columns with aliases:\")\n",
    "for col in ship_ctx[\"schemas\"][\"vessels\"].columns:\n",
    "    print(f\"  {col.name:15s} {col.type:6s} aliases={col.aliases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all 6 PDFs\n",
    "shipping_pdfs = {\n",
    "    \"Newcastle\":  \"inputs/2857439.pdf\",\n",
    "    \"Bunge\":      \"inputs/Bunge_loadingstatement_2025-09-25.pdf\",\n",
    "    \"CBH\":        \"inputs/CBH Shipping Stem 26092025.pdf\",\n",
    "    \"GrainCorp\":  \"inputs/shipping-stem-2025-11-13.pdf\",\n",
    "    \"Riordan\":    \"inputs/shipping_stem-accc-30092025-1.pdf\",\n",
    "    \"Queensland\": \"inputs/document (1).pdf\",\n",
    "}\n",
    "\n",
    "import fitz\n",
    "print(f\"{'Provider':<14s} {'Filename':<50s} {'Pages':>5s}\")\n",
    "print(\"-\" * 72)\n",
    "for provider, path in shipping_pdfs.items():\n",
    "    doc = fitz.open(path)\n",
    "    pages = len(doc)\n",
    "    doc.close()\n",
    "    print(f\"{provider:<14s} {Path(path).name:<50s} {pages:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ALL 6 PDFs concurrently\n",
    "results_ship, merged_ship, paths_ship, elapsed_ship = await run_pipeline_async(\n",
    "    \"contracts/au_shipping_stem.json\",\n",
    "    list(shipping_pdfs.values()),\n",
    "    output_dir=\"outputs/shipping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-document summary\n",
    "print(f\"{'Provider':<14s} {'Records':>8s}\")\n",
    "print(\"-\" * 24)\n",
    "total = 0\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    n = sum(len(df) for df in doc_ctx.get(\"dataframes\", {}).values())\n",
    "    total += n\n",
    "    print(f\"{provider:<14s} {n:>8d}\")\n",
    "print(\"-\" * 24)\n",
    "print(f\"{'TOTAL':<14s} {total:>8d}\")\n",
    "print(f\"\\nWall-clock time: {elapsed_ship:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side gallery — one representative page per provider\n",
    "for provider, path in list(shipping_pdfs.items())[:3]:\n",
    "    img_b64, _ = render_document_page(path, page=0, dpi=100)\n",
    "    compressed = compress_spatial_text(path, refine_headers=False)\n",
    "    # Truncate compressed text for display\n",
    "    lines = compressed.splitlines()[:25]\n",
    "    truncated = \"\\n\".join(lines) + \"\\n...\"\n",
    "    print(f\"\\n--- {provider} ---\")\n",
    "    side_by_side_display(image_b64=img_b64, compressed_text=truncated, max_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unified DataFrame — all providers in one schema\ndf_vessel_frames = merged_ship[\"vessels\"]\ndf_all = pd.concat(df_vessel_frames, ignore_index=True) if isinstance(df_vessel_frames, list) else df_vessel_frames\nprint(f\"Unified DataFrame: {df_all.shape}\")\ndisplay(df_all.head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by provider (inferred from source document)\n",
    "# Add source_provider from doc_ctx ordering\n",
    "provider_frames = []\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    for df in doc_ctx.get(\"dataframes\", {}).values():\n",
    "        dfp = df.copy()\n",
    "        dfp[\"source_provider\"] = provider\n",
    "        provider_frames.append(dfp)\n",
    "\n",
    "if provider_frames:\n",
    "    df_with_provider = pd.concat(provider_frames, ignore_index=True)\n",
    "    print(\"Records by provider:\")\n",
    "    print(df_with_provider.groupby(\"source_provider\").size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 3: ACEA Car Registrations\n\n**Normalization: pivoted table to flat records.**\n\nThe ACEA press release PDF contains a dense pivoted table: 28 countries x 7 power types\nx 3 metrics. The pipeline unpivots this into flat records — a 23-column wide table\nbecomes a 4-column long DataFrame.\n\n**Deterministic mapping**: After header separator standardization (stacked headers\njoined with ` / `), headers like `BATTERY ELECTRIC / Dec-25` have parts that match\nschema aliases for motorization type and period. When aliases cover all header parts,\nthe deterministic mapper handles the full unpivot — each data row produces one record\nper power type — with zero LLM calls. Vision mode is only needed when headers are\ngarbled and aliases cannot match."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ACEA contract\n",
    "acea_ctx = prepare(\"contracts/acea_car_registrations.json\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for col in acea_ctx[\"schemas\"][\"registrations_by_market\"].columns:\n",
    "    print(f\"  {col.name:25s} {col.type:6s} {col.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ACEA press release PDF (vision-enabled)\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "\n",
    "results_acea, merged_acea, _, elapsed_acea = await run_pipeline_async(\n",
    "    \"contracts/acea_car_registrations.json\",\n",
    "    [acea_pdf],\n",
    "    output_dir=\"outputs/acea\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF | compressed pivot table | unpivoted DataFrame\n",
    "img_b64, _ = render_document_page(acea_pdf, page=0, dpi=120)\n",
    "acea_compressed = compress_spatial_text(acea_pdf, refine_headers=False)\n",
    "acea_df = list(merged_acea.values())[0]\n",
    "if isinstance(acea_df, list):\n",
    "    acea_df = acea_df[0]\n",
    "\n",
    "# Show first page of compressed text\n",
    "first_page = acea_compressed.split(\"\\f\")[0]\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=first_page, dataframe=acea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unpivoted DataFrame\n",
    "print(f\"Shape: {acea_df.shape} (from wide pivoted table to long flat records)\")\n",
    "display(acea_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip verification: pivot back to wide format\n",
    "if \"country\" in acea_df.columns and \"car_motorization\" in acea_df.columns:\n",
    "    try:\n",
    "        pivot = acea_df.pivot_table(\n",
    "            index=\"country\",\n",
    "            columns=\"car_motorization\",\n",
    "            values=\"new_car_registration\",\n",
    "            aggfunc=\"sum\",\n",
    "        )\n",
    "        print(f\"Pivoted back: {pivot.shape} (countries x power types)\")\n",
    "        display(pivot.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Pivot failed: {e}\")\n",
    "else:\n",
    "    print(\"Columns not available for pivot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| Use Case | Format | Documents | Elapsed | Key Feature |\n|----------|--------|-----------|---------|-------------|\n| Russian Agriculture | DOCX | 2 reports | see above | Multi-category, pivot years, deterministic mapping |\n| Australian Shipping | PDF | 6 providers | see above | One schema, 6 layouts, full concurrency |\n| ACEA Registrations | PDF | 1 press release | see above | Deterministic unpivot, vision fallback |\n\n**Deterministic-first architecture**: Tables with complete alias coverage in the schema\nare interpreted via pure string matching — no LLM calls, no latency, no cost. The LLM\npipeline activates only as a fallback for pages with unmatched header parts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_docs = 2 + 6 + 1  # June + July + 6 shipping + 1 ACEA\n",
    "total_records = 0\n",
    "for m in [merged_june, merged_july, merged_ship, merged_acea]:\n",
    "    for v in m.values():\n",
    "        df = v[0] if isinstance(v, list) else v\n",
    "        total_records += len(df)\n",
    "\n",
    "total_time = elapsed_june + elapsed_july + elapsed_ship + elapsed_acea\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Total records extracted:   {total_records:,}\")\n",
    "print(f\"Total wall-clock time:     {total_time:.1f}s\")\n",
    "print(f\"Throughput:                {total_records / total_time:.0f} records/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}