{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Contract-Driven Data Pipelines\n\nExtract structured data from heterogeneous documents using declarative JSON contracts.\n\nOne schema. Many providers. Full async."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Design\n\nA **data contract** (JSON) declares everything the pipeline needs:\n- Which LLM model to use\n- How to classify tables (keyword matching)\n- What output schema to produce (columns, types, aliases, formats)\n- How to enrich records (constants, metadata from filenames/titles)\n\nThe pipeline code is 100% generic — swap the contract, not the code.\n\nThe interpretation step uses a **deterministic-first** architecture: when schema\naliases fully cover every ` / `-separated header part, records are built via pure\nstring matching with zero LLM calls. The LLM pipeline activates only as a fallback\nfor pages with unmatched columns.\n\n**Three use cases** demonstrate the same pipeline across different domains:\n\n| # | Use Case | Format | Documents | Challenge |\n|---|----------|--------|-----------|----------|\n| 1 | Russian agricultural reports | DOCX | 2 weekly reports | Multi-category extraction, dynamic pivot years, deterministic mapping |\n| 2 | Australian shipping stems | PDF | 6 providers | One canonical model, 6 different layouts, full concurrency |\n| 3 | ACEA car registrations | PDF | 1 press release | Pivoted table → flat records, deterministic unpivoting |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport asyncio\nimport time\nfrom pathlib import Path\n\nimport nest_asyncio\nimport pandas as pd\n\nfrom pdf_ocr import (\n    CanonicalSchema, ColumnDef,\n    compress_spatial_text,\n    to_pandas, to_records, to_parquet,\n)\n\nnest_asyncio.apply()\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG. Supports PDF and DOCX.\"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline Architecture\n\n```\ncontract (JSON) → load_contract() → process_document_async() → save()\n```\n\n**Deterministic bypass**: Before any LLM call, each page is tested for deterministic\nmapping. If every ` / `-separated header part matches a schema alias, records are built\nvia string matching — no LLM call, no latency, no cost. This fires automatically for\nRussian DOCX (compound headers) and ACEA PDF (hierarchical headers) when the contract\nhas complete aliases.\n\nFive levels of concurrency — all on a single event loop, no sync wrappers:\n\n| Level | Scope | Pattern |\n|---|---|---|\n| **Document-level** | Process N documents simultaneously | `asyncio.gather(*[process_document_async(doc, cc) for doc in docs])` |\n| **Compression** | PDF compression in thread pool (CPU-bound) | `asyncio.to_thread(compress_spatial_text, ...)` |\n| **Category-level** | Multiple outputs per document (e.g. harvest + planting) | `asyncio.gather(*[interpret_output_async(...)])` |\n| **Table/Page-level** | DOCX: N tables concurrent; PDF: N pages concurrent | `interpret_tables_async()` / `_interpret_pages_batched_async()` |\n| **Batch-level** | Step 2 mapping in chunks of 20 rows | Built into `_interpret_pages_batched_async()` |\n\nKey optimization: compression and interpretation are **pipelined per document** — as soon as\none PDF finishes compression, its LLM interpretation starts immediately while other PDFs\nare still compressing. No sequential fetch barrier.\n\nThe three pipeline helpers live in `pdf_ocr.pipeline`:\n- `compress_and_classify_async()` — compress + classify (Layer 1)\n- `interpret_output_async()` — interpret + enrich + format one category (Layer 2)\n- `process_document_async()` — full per-document pipeline (Layer 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Generic Pipeline Functions ────────────────────────────────────────────────\n# These functions are contract-agnostic. Swap the JSON contract and re-run.\n\nfrom pdf_ocr.contracts import load_contract, ContractContext\nfrom pdf_ocr.pipeline import process_document_async, DocumentResult\n\n\ndef save(results: list[DocumentResult], output_dir=\"outputs\"):\n    \"\"\"Merge DataFrames across documents, write each output to Parquet.\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    merged = {}\n\n    for result in results:\n        for out_name, df in result.dataframes.items():\n            if out_name not in merged:\n                merged[out_name] = []\n            merged[out_name].append(df)\n\n    paths = {}\n    for out_name, frames in merged.items():\n        df = pd.concat(frames, ignore_index=True)\n        path = output_dir / f\"{out_name}.parquet\"\n        df.to_parquet(path, index=False)\n        paths[out_name] = path\n        print(f\"  {out_name}: {path} ({len(df)} rows)\")\n\n    return merged, paths\n\n\nasync def run_pipeline_async(contract_path, doc_paths, output_dir=\"outputs\"):\n    \"\"\"Orchestrate full pipeline with document-level concurrency.\"\"\"\n    t0 = time.perf_counter()\n\n    print(\"PREPARE\")\n    cc = load_contract(contract_path)\n    print(f\"  Contract: {cc.provider}, Model: {cc.model}, Outputs: {list(cc.outputs.keys())}\")\n\n    print(\"\\nPROCESS (async — compress + classify + interpret per document)\")\n    results = await asyncio.gather(\n        *[process_document_async(str(p), cc) for p in doc_paths]\n    )\n    for r in results:\n        cats = list(r.dataframes.keys())\n        rows = sum(len(df) for df in r.dataframes.values())\n        print(f\"  {Path(r.doc_path).name[:50]}: {cats} → {rows} records\")\n\n    print(\"\\nSAVE\")\n    merged, paths = save(results, output_dir)\n\n    elapsed = time.perf_counter() - t0\n    print(f\"\\nDone in {elapsed:.1f}s\")\n    return results, merged, paths, elapsed\n\n\nprint(\"Pipeline functions defined: load_contract -> run_pipeline_async -> save\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 1: Russian Agricultural DOCX Reports\n\nMulti-category extraction from Russian Ministry of Agriculture weekly grain reports.\nEach DOCX contains harvest, planting, and export tables. The contract classifies\ntables by keywords and extracts crop names from table titles, report dates from\nfilenames, and year values from dynamic pivot headers.\n\n**Deterministic mapping**: The DOCX extractor produces compound headers with ` / `\nseparators (e.g., `spring crops / MOA Target 2025`). When the contract aliases cover\nevery header part, the interpretation step resolves the full table — including\nunpivoting across crop groups — with zero LLM calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the Russian agricultural contract\nru_cc = load_contract(\"contracts/ru_ag_ministry.json\")\nprint(f\"  Contract: {ru_cc.provider}, Model: {ru_cc.model}, Outputs: {list(ru_cc.outputs.keys())}\")\n\nprint(\"\\nSchema summary:\")\nfor name, spec in ru_cc.outputs.items():\n    cols = [c.name for c in spec.schema.columns]\n    enrich = list(spec.enrichment.keys())\n    print(f\"  {name}: LLM cols={cols}, enriched={enrich}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the June DOCX\n",
    "june_path = \"inputs/docx/input/2025-06-24_11-58-45.Russian weekly grain EOW June 20-21 2025-1.docx\"\n",
    "\n",
    "results_june, merged_june, _, elapsed_june = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.json\", [june_path], output_dir=\"outputs/ru_june\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side: DOCX page | pipe-table | DataFrame (first available category)\ntry:\n    img_b64, _ = render_document_page(june_path, page=0, dpi=120)\nexcept Exception:\n    img_b64 = None  # LibreOffice not available\n\n# Get the first available category from this document\njune_result = results_june[0]\nfirst_cat = next(iter(june_result.dataframes), None)\n\nif first_cat:\n    cat_data = june_result.compressed_by_category.get(first_cat)\n    # DOCX: list of (md, meta) tuples; PDF: compressed text string\n    if isinstance(cat_data, list):\n        sample_md = cat_data[0][0]\n    elif isinstance(cat_data, str):\n        sample_md = cat_data.split(\"\\f\")[0]  # first page\n    else:\n        sample_md = \"(no compressed text)\"\n    df_display = june_result.dataframes[first_cat]\n    print(f\"Showing category: {first_cat} ({len(df_display)} rows)\")\n    side_by_side_display(image_b64=img_b64, compressed_text=sample_md, dataframe=df_display)\nelse:\n    print(\"No tables classified for this document.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display output DataFrames\n",
    "for name, frames in merged_june.items():\n",
    "    df = frames[0] if isinstance(frames, list) else frames\n",
    "    print(f\"=== {name.upper()} ({len(df)} rows) ===\")\n",
    "    display(df.head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the July DOCX — same contract, different document\n",
    "july_path = \"inputs/docx/input/2025-07-17_10-16-25.Russian weekly grain EOW July 11-12 2025-1.docx\"\n",
    "\n",
    "results_july, merged_july, _, elapsed_july = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.json\", [july_path], output_dir=\"outputs/ru_july\"\n",
    ")\n",
    "\n",
    "print(f\"\\nJune: {elapsed_june:.1f}s, July: {elapsed_july:.1f}s\")\n",
    "for name in merged_july:\n",
    "    df = merged_july[name][0] if isinstance(merged_july[name], list) else merged_july[name]\n",
    "    print(f\"  {name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 2: Australian Shipping Stems — XXL\n",
    "\n",
    "**One canonical model, six providers, full concurrency.**\n",
    "\n",
    "6 shipping stem PDFs from 6 different providers — each expresses the same semantic data\n",
    "(vessel name, port, commodity, tonnage, ETA) with completely different layouts, column\n",
    "names, and formatting. A single canonical schema with rich aliases normalizes them all\n",
    "into one unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the shipping stem contract\nship_cc = load_contract(\"contracts/au_shipping_stem.json\")\nprint(f\"  Contract: {ship_cc.provider}, Model: {ship_cc.model}, Outputs: {list(ship_cc.outputs.keys())}\")\n\nprint(\"\\nSchema columns with aliases:\")\nfor col in ship_cc.outputs[\"vessels\"].schema.columns:\n    print(f\"  {col.name:15s} {col.type:6s} aliases={col.aliases}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all 6 PDFs\n",
    "shipping_pdfs = {\n",
    "    \"Newcastle\":  \"inputs/2857439.pdf\",\n",
    "    \"Bunge\":      \"inputs/Bunge_loadingstatement_2025-09-25.pdf\",\n",
    "    \"CBH\":        \"inputs/CBH Shipping Stem 26092025.pdf\",\n",
    "    \"GrainCorp\":  \"inputs/shipping-stem-2025-11-13.pdf\",\n",
    "    \"Riordan\":    \"inputs/shipping_stem-accc-30092025-1.pdf\",\n",
    "    \"Queensland\": \"inputs/document (1).pdf\",\n",
    "}\n",
    "\n",
    "import fitz\n",
    "print(f\"{'Provider':<14s} {'Filename':<50s} {'Pages':>5s}\")\n",
    "print(\"-\" * 72)\n",
    "for provider, path in shipping_pdfs.items():\n",
    "    doc = fitz.open(path)\n",
    "    pages = len(doc)\n",
    "    doc.close()\n",
    "    print(f\"{provider:<14s} {Path(path).name:<50s} {pages:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ALL 6 PDFs concurrently\n",
    "results_ship, merged_ship, paths_ship, elapsed_ship = await run_pipeline_async(\n",
    "    \"contracts/au_shipping_stem.json\",\n",
    "    list(shipping_pdfs.values()),\n",
    "    output_dir=\"outputs/shipping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-document summary\nprint(f\"{'Provider':<14s} {'Records':>8s}\")\nprint(\"-\" * 24)\ntotal = 0\nfor (provider, _), result in zip(shipping_pdfs.items(), results_ship):\n    n = sum(len(df) for df in result.dataframes.values())\n    total += n\n    print(f\"{provider:<14s} {n:>8d}\")\nprint(\"-\" * 24)\nprint(f\"{'TOTAL':<14s} {total:>8d}\")\nprint(f\"\\nWall-clock time: {elapsed_ship:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side gallery — one representative page per provider\n",
    "for provider, path in list(shipping_pdfs.items())[:3]:\n",
    "    img_b64, _ = render_document_page(path, page=0, dpi=100)\n",
    "    compressed = compress_spatial_text(path, refine_headers=False)\n",
    "    # Truncate compressed text for display\n",
    "    lines = compressed.splitlines()[:25]\n",
    "    truncated = \"\\n\".join(lines) + \"\\n...\"\n",
    "    print(f\"\\n--- {provider} ---\")\n",
    "    side_by_side_display(image_b64=img_b64, compressed_text=truncated, max_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unified DataFrame — all providers in one schema\ndf_vessel_frames = merged_ship[\"vessels\"]\ndf_all = pd.concat(df_vessel_frames, ignore_index=True) if isinstance(df_vessel_frames, list) else df_vessel_frames\nprint(f\"Unified DataFrame: {df_all.shape}\")\ndisplay(df_all.head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution by provider (inferred from source document)\n# Add source_provider from result ordering\nprovider_frames = []\nfor (provider, _), result in zip(shipping_pdfs.items(), results_ship):\n    for df in result.dataframes.values():\n        dfp = df.copy()\n        dfp[\"source_provider\"] = provider\n        provider_frames.append(dfp)\n\nif provider_frames:\n    df_with_provider = pd.concat(provider_frames, ignore_index=True)\n    print(\"Records by provider:\")\n    print(df_with_provider.groupby(\"source_provider\").size().to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Use Case 3: ACEA Car Registrations\n\n**Normalization: pivoted table to flat records.**\n\nThe ACEA press release PDF contains a dense pivoted table: 28 countries x 7 power types\nx 3 metrics. The pipeline unpivots this into flat records — a 23-column wide table\nbecomes a 4-column long DataFrame.\n\n**Deterministic mapping**: After header separator standardization (stacked headers\njoined with ` / `), headers like `BATTERY ELECTRIC / Dec-25` have parts that match\nschema aliases for motorization type and period. When aliases cover all header parts,\nthe deterministic mapper handles the full unpivot — each data row produces one record\nper power type — with zero LLM calls. Vision mode is only needed when headers are\ngarbled and aliases cannot match."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the ACEA contract\nacea_cc = load_contract(\"contracts/acea_car_registrations.json\")\nprint(f\"  Contract: {acea_cc.provider}, Model: {acea_cc.model}, Outputs: {list(acea_cc.outputs.keys())}\")\n\nprint(\"\\nSchema:\")\nfor col in acea_cc.outputs[\"registrations_by_market\"].schema.columns:\n    print(f\"  {col.name:25s} {col.type:6s} {col.description}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ACEA press release PDF (vision-enabled)\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "\n",
    "results_acea, merged_acea, _, elapsed_acea = await run_pipeline_async(\n",
    "    \"contracts/acea_car_registrations.json\",\n",
    "    [acea_pdf],\n",
    "    output_dir=\"outputs/acea\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF | compressed pivot table | unpivoted DataFrame\n",
    "img_b64, _ = render_document_page(acea_pdf, page=0, dpi=120)\n",
    "acea_compressed = compress_spatial_text(acea_pdf, refine_headers=False)\n",
    "acea_df = list(merged_acea.values())[0]\n",
    "if isinstance(acea_df, list):\n",
    "    acea_df = acea_df[0]\n",
    "\n",
    "# Show first page of compressed text\n",
    "first_page = acea_compressed.split(\"\\f\")[0]\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=first_page, dataframe=acea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unpivoted DataFrame\n",
    "print(f\"Shape: {acea_df.shape} (from wide pivoted table to long flat records)\")\n",
    "display(acea_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip verification: pivot back to wide format\n",
    "if \"country\" in acea_df.columns and \"car_motorization\" in acea_df.columns:\n",
    "    try:\n",
    "        pivot = acea_df.pivot_table(\n",
    "            index=\"country\",\n",
    "            columns=\"car_motorization\",\n",
    "            values=\"new_car_registration\",\n",
    "            aggfunc=\"sum\",\n",
    "        )\n",
    "        print(f\"Pivoted back: {pivot.shape} (countries x power types)\")\n",
    "        display(pivot.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Pivot failed: {e}\")\n",
    "else:\n",
    "    print(\"Columns not available for pivot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n| Use Case | Format | Documents | Elapsed | Key Feature |\n|----------|--------|-----------|---------|-------------|\n| Russian Agriculture | DOCX | 2 reports | see above | Multi-category, pivot years, deterministic mapping |\n| Australian Shipping | PDF | 6 providers | see above | One schema, 6 layouts, full concurrency |\n| ACEA Registrations | PDF | 1 press release | see above | Deterministic unpivot, vision fallback |\n\n**Deterministic-first architecture**: Tables with complete alias coverage in the schema\nare interpreted via pure string matching — no LLM calls, no latency, no cost. The LLM\npipeline activates only as a fallback for pages with unmatched header parts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_docs = 2 + 6 + 1  # June + July + 6 shipping + 1 ACEA\n",
    "total_records = 0\n",
    "for m in [merged_june, merged_july, merged_ship, merged_acea]:\n",
    "    for v in m.values():\n",
    "        df = v[0] if isinstance(v, list) else v\n",
    "        total_records += len(df)\n",
    "\n",
    "total_time = elapsed_june + elapsed_july + elapsed_ship + elapsed_acea\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Total records extracted:   {total_records:,}\")\n",
    "print(f\"Total wall-clock time:     {total_time:.1f}s\")\n",
    "print(f\"Throughput:                {total_records / total_time:.0f} records/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}