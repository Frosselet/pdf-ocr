{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract-Driven Data Pipelines\n",
    "\n",
    "Extract structured data from heterogeneous documents using declarative YAML contracts.\n",
    "\n",
    "One schema. Many providers. Full async."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "A **data contract** (YAML) declares everything the pipeline needs:\n",
    "- Which LLM model to use\n",
    "- How to classify tables (keyword matching)\n",
    "- What output schema to produce (columns, types, aliases, formats)\n",
    "- How to enrich records (constants, metadata from filenames/titles)\n",
    "\n",
    "The pipeline code is 100% generic — swap the contract, not the code.\n",
    "\n",
    "**Three use cases** demonstrate the same pipeline across different domains:\n",
    "\n",
    "| # | Use Case | Format | Documents | Challenge |\n",
    "|---|----------|--------|-----------|----------|\n",
    "| 1 | Russian agricultural reports | DOCX | 2 weekly reports | Multi-category extraction, dynamic pivot years |\n",
    "| 2 | Australian shipping stems | PDF | 6 providers | One canonical model, 6 different layouts, full concurrency |\n",
    "| 3 | ACEA car registrations | PDF | 1 press release | Pivoted table → flat records, vision-enabled |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import asyncio\n",
    "import re\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "from pdf_ocr import (\n",
    "    CanonicalSchema, ColumnDef,\n",
    "    classify_tables, classify_docx_tables,\n",
    "    compress_spatial_text, compress_spatial_text_structured,\n",
    "    compress_docx_tables, extract_pivot_values,\n",
    "    interpret_table, interpret_tables, interpret_tables_async,\n",
    "    to_pandas, to_records, to_parquet,\n",
    ")\n",
    "from pdf_ocr.interpret import interpret_table_async\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helpers\n",
    "import base64, html as html_mod, shutil, subprocess, tempfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def render_document_page(path, page=0, dpi=150):\n",
    "    \"\"\"Render a document page as a base64 PNG. Supports PDF and DOCX.\"\"\"\n",
    "    import fitz\n",
    "\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".docx\", \".doc\")):\n",
    "        soffice = shutil.which(\"soffice\") or \"/Applications/LibreOffice.app/Contents/MacOS/soffice\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            subprocess.run(\n",
    "                [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", tmpdir, path],\n",
    "                capture_output=True, check=True,\n",
    "            )\n",
    "            pdf_path = next(Path(tmpdir).glob(\"*.pdf\"))\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "    else:\n",
    "        doc = fitz.open(path)\n",
    "\n",
    "    page_count = len(doc)\n",
    "    pix = doc[page].get_pixmap(dpi=dpi)\n",
    "    b64 = base64.b64encode(pix.tobytes(\"png\")).decode()\n",
    "    doc.close()\n",
    "    return b64, page_count\n",
    "\n",
    "\n",
    "def side_by_side_display(*, image_b64=None, compressed_text=None, dataframe=None, max_height=500):\n",
    "    \"\"\"Display up to 3 panels side-by-side: document image | compressed text | DataFrame.\"\"\"\n",
    "    panels = []\n",
    "    style = f\"overflow-y:auto; max-height:{max_height}px; border:1px solid #ddd; padding:6px; flex:1\"\n",
    "    if image_b64:\n",
    "        panels.append(f'<div style=\"{style}\"><img src=\"data:image/png;base64,{image_b64}\" style=\"width:100%\"></div>')\n",
    "    if compressed_text:\n",
    "        escaped = html_mod.escape(compressed_text)\n",
    "        panels.append(f'<div style=\"{style}\"><pre style=\"font-size:11px; margin:0; white-space:pre\">{escaped}</pre></div>')\n",
    "    if dataframe is not None:\n",
    "        df_html = dataframe.to_html(index=False, max_rows=30)\n",
    "        panels.append(f'<div style=\"{style}; font-size:11px\">{df_html}</div>')\n",
    "    display(HTML(f'<div style=\"display:flex; gap:8px; align-items:flex-start\">{\" \".join(panels)}</div>'))\n",
    "\n",
    "\n",
    "print(\"Display helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "contract (YAML) → prepare() → fetch() → transform_async() → save()\n",
    "```\n",
    "\n",
    "Three levels of parallelism via `asyncio.gather()`:\n",
    "\n",
    "| Level | Scope | Pattern |\n",
    "|---|---|---|\n",
    "| **Document-level** | Process N documents simultaneously | `asyncio.gather(*[transform_async(doc) for doc in docs])` |\n",
    "| **Page-level** | Multi-page PDFs split on `\\f` | Built into `interpret_table()` |\n",
    "| **Batch-level** | Step 2 mapping in chunks of 20 rows | Built into internal batching |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generic Pipeline Functions ────────────────────────────────────────────────\n",
    "# These functions are contract-agnostic. Swap the YAML contract and re-run.\n",
    "\n",
    "\n",
    "def prepare(contract_path):\n",
    "    \"\"\"Load YAML contract → parse categories, schemas, enrichment rules.\"\"\"\n",
    "    with open(contract_path) as f:\n",
    "        contract = yaml.safe_load(f)\n",
    "\n",
    "    ctx = {\"contract\": contract, \"model\": contract.get(\"model\", \"openai/gpt-4o\")}\n",
    "\n",
    "    # Categories for table classification\n",
    "    ctx[\"categories\"] = {\n",
    "        name: cat[\"keywords\"]\n",
    "        for name, cat in contract.get(\"categories\", {}).items()\n",
    "    }\n",
    "\n",
    "    # Build CanonicalSchema + enrichment rules per output\n",
    "    ctx[\"schemas\"] = {}\n",
    "    ctx[\"enrichment\"] = {}\n",
    "    ctx[\"output_specs\"] = {}\n",
    "    for out_name, spec in contract.get(\"outputs\", {}).items():\n",
    "        llm_cols = []\n",
    "        enrich = {}\n",
    "        for col in spec[\"schema\"][\"columns\"]:\n",
    "            if \"source\" in col:\n",
    "                enrich[col[\"name\"]] = col\n",
    "            else:\n",
    "                llm_cols.append(ColumnDef(\n",
    "                    name=col[\"name\"],\n",
    "                    type=col.get(\"type\", \"string\"),\n",
    "                    description=col.get(\"description\", \"\"),\n",
    "                    aliases=col.get(\"aliases\", []),\n",
    "                    format=col.get(\"format\"),\n",
    "                ))\n",
    "        ctx[\"schemas\"][out_name] = CanonicalSchema(\n",
    "            description=spec[\"schema\"].get(\"description\", \"\"),\n",
    "            columns=llm_cols,\n",
    "        )\n",
    "        ctx[\"enrichment\"][out_name] = enrich\n",
    "        ctx[\"output_specs\"][out_name] = spec\n",
    "\n",
    "    print(f\"  Contract: {contract['provider']}\")\n",
    "    print(f\"  Model: {ctx['model']}\")\n",
    "    print(f\"  Outputs: {list(ctx['schemas'].keys())}\")\n",
    "    return ctx\n",
    "\n",
    "\n",
    "def fetch(ctx, doc_path):\n",
    "    \"\"\"Auto-detect format, classify tables, compress to pipe-table markdown.\"\"\"\n",
    "    doc_path = str(doc_path)\n",
    "    doc_ctx = {\"doc_path\": doc_path, \"compressed_by_category\": {}}\n",
    "\n",
    "    # Extract report_date from filename if pattern is defined\n",
    "    pattern = ctx[\"contract\"].get(\"report_date_pattern\")\n",
    "    if pattern:\n",
    "        m = re.search(pattern, Path(doc_path).name)\n",
    "        doc_ctx[\"report_date\"] = m.group(1) if m else \"\"\n",
    "    else:\n",
    "        doc_ctx[\"report_date\"] = \"\"\n",
    "\n",
    "    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n",
    "\n",
    "    if is_docx:\n",
    "        classes = classify_docx_tables(doc_path, ctx[\"categories\"])\n",
    "        for out_name, spec in ctx[\"output_specs\"].items():\n",
    "            cat = spec[\"category\"]\n",
    "            indices = [c[\"index\"] for c in classes if c[\"category\"] == cat]\n",
    "            if indices:\n",
    "                doc_ctx[\"compressed_by_category\"][out_name] = compress_docx_tables(\n",
    "                    doc_path, table_indices=indices\n",
    "                )\n",
    "    else:\n",
    "        # PDF: compress → structured tables → classify\n",
    "        compressed_text = compress_spatial_text(doc_path, refine_headers=True)\n",
    "        structured = compress_spatial_text_structured(doc_path)\n",
    "        tuples = [t.to_compressed() for t in structured]\n",
    "        classes = classify_tables(tuples, ctx[\"categories\"]) if tuples else []\n",
    "\n",
    "        for out_name, spec in ctx[\"output_specs\"].items():\n",
    "            cat = spec[\"category\"]\n",
    "            matched = [c for c in classes if c[\"category\"] == cat]\n",
    "            if matched:\n",
    "                # For PDF, store the full compressed text (interpret_table handles multi-page)\n",
    "                doc_ctx[\"compressed_by_category\"][out_name] = compressed_text\n",
    "                doc_ctx[\"pdf_path\"] = doc_path\n",
    "\n",
    "    cats_found = list(doc_ctx[\"compressed_by_category\"].keys())\n",
    "    print(f\"  {Path(doc_path).name[:50]}: categories={cats_found}\")\n",
    "    return doc_ctx\n",
    "\n",
    "\n",
    "async def transform_async(ctx, doc_ctx):\n",
    "    \"\"\"Async: interpret tables and apply enrichment. Returns doc_ctx with dataframes.\"\"\"\n",
    "    doc_ctx[\"dataframes\"] = {}\n",
    "    doc_path = doc_ctx[\"doc_path\"]\n",
    "    is_docx = doc_path.lower().endswith((\".docx\", \".doc\"))\n",
    "\n",
    "    for out_name, data in doc_ctx[\"compressed_by_category\"].items():\n",
    "        schema = ctx[\"schemas\"][out_name]\n",
    "        spec = ctx[\"output_specs\"][out_name]\n",
    "\n",
    "        # Resolve dynamic pivot aliases\n",
    "        for col in schema.columns:\n",
    "            col_spec = next(\n",
    "                (c for c in spec[\"schema\"][\"columns\"] if c[\"name\"] == col.name), None\n",
    "            )\n",
    "            if col_spec and col_spec.get(\"dynamic_aliases\") == \"pivot\":\n",
    "                if is_docx and isinstance(data, list):\n",
    "                    pivot_vals = extract_pivot_values(data[0][0])\n",
    "                    col.aliases = pivot_vals[-2:]\n",
    "\n",
    "        if is_docx and isinstance(data, list):\n",
    "            # DOCX: multiple independent tables → interpret_tables_async\n",
    "            texts = [md for md, _ in data]\n",
    "            mapped_list = await interpret_tables_async(\n",
    "                texts, schema, model=ctx[\"model\"]\n",
    "            )\n",
    "            frames = []\n",
    "            for (md, meta), mapped in zip(data, mapped_list):\n",
    "                df = to_pandas(mapped, schema)\n",
    "                # Apply enrichment\n",
    "                for col_name, enrich_spec in ctx[\"enrichment\"][out_name].items():\n",
    "                    src = enrich_spec[\"source\"]\n",
    "                    if src == \"title\":\n",
    "                        df[col_name] = meta.get(\"title\") or \"Unknown\"\n",
    "                    elif src == \"report_date\":\n",
    "                        val = doc_ctx.get(\"report_date\", \"\")\n",
    "                        if \"suffix\" in enrich_spec:\n",
    "                            val += enrich_spec[\"suffix\"]\n",
    "                        df[col_name] = val\n",
    "                    elif src == \"constant\":\n",
    "                        df[col_name] = enrich_spec[\"value\"]\n",
    "                frames.append(df)\n",
    "            df_out = pd.concat(frames, ignore_index=True)\n",
    "        else:\n",
    "            # PDF: full compressed text → interpret_table (handles multi-page internally)\n",
    "            # Run in thread to avoid blocking the event loop\n",
    "            pdf_path = doc_ctx.get(\"pdf_path\")\n",
    "            result = await asyncio.to_thread(\n",
    "                interpret_table, data, schema,\n",
    "                model=ctx[\"model\"], pdf_path=pdf_path,\n",
    "            )\n",
    "            df_out = to_pandas(result, schema)\n",
    "            # Apply enrichment for PDF\n",
    "            for col_name, enrich_spec in ctx[\"enrichment\"][out_name].items():\n",
    "                src = enrich_spec[\"source\"]\n",
    "                if src == \"constant\":\n",
    "                    df_out[col_name] = enrich_spec[\"value\"]\n",
    "                elif src == \"report_date\":\n",
    "                    val = doc_ctx.get(\"report_date\", \"\")\n",
    "                    if \"suffix\" in enrich_spec:\n",
    "                        val += enrich_spec[\"suffix\"]\n",
    "                    df_out[col_name] = val\n",
    "\n",
    "        # Apply column-level format transformations\n",
    "        for col_spec in spec[\"schema\"][\"columns\"]:\n",
    "            fmt = col_spec.get(\"format\")\n",
    "            cn = col_spec[\"name\"]\n",
    "            if not fmt or cn not in df_out.columns:\n",
    "                continue\n",
    "            if fmt == \"lowercase\":\n",
    "                df_out[cn] = df_out[cn].astype(str).str.lower()\n",
    "            elif fmt == \"uppercase\":\n",
    "                df_out[cn] = df_out[cn].astype(str).str.upper()\n",
    "            elif fmt == \"titlecase\":\n",
    "                df_out[cn] = df_out[cn].astype(str).str.title()\n",
    "\n",
    "        # Apply column filters\n",
    "        for col_spec in spec[\"schema\"][\"columns\"]:\n",
    "            filt = col_spec.get(\"filter\")\n",
    "            cn = col_spec[\"name\"]\n",
    "            if not filt or filt == \"all\" or cn not in df_out.columns:\n",
    "                continue\n",
    "            if filt == \"latest\":\n",
    "                df_out = df_out[df_out[cn] == df_out[cn].max()]\n",
    "            elif filt == \"earliest\":\n",
    "                df_out = df_out[df_out[cn] == df_out[cn].min()]\n",
    "\n",
    "        # Reorder columns to match contract\n",
    "        col_order = [c[\"name\"] for c in spec[\"schema\"][\"columns\"] if c[\"name\"] in df_out.columns]\n",
    "        df_out = df_out[col_order]\n",
    "\n",
    "        doc_ctx[\"dataframes\"][out_name] = df_out\n",
    "\n",
    "    return doc_ctx\n",
    "\n",
    "\n",
    "def save(results, output_dir=\"outputs\"):\n",
    "    \"\"\"Merge DataFrames across documents, write each output to Parquet.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged = {}\n",
    "\n",
    "    for doc_ctx in results:\n",
    "        for out_name, df in doc_ctx.get(\"dataframes\", {}).items():\n",
    "            if out_name not in merged:\n",
    "                merged[out_name] = []\n",
    "            merged[out_name].append(df)\n",
    "\n",
    "    paths = {}\n",
    "    for out_name, frames in merged.items():\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "        path = output_dir / f\"{out_name}.parquet\"\n",
    "        df.to_parquet(path, index=False)\n",
    "        paths[out_name] = path\n",
    "        print(f\"  {out_name}: {path} ({len(df)} rows)\")\n",
    "\n",
    "    return merged, paths\n",
    "\n",
    "\n",
    "async def run_pipeline_async(contract_path, doc_paths, output_dir=\"outputs\"):\n",
    "    \"\"\"Orchestrate full pipeline with document-level concurrency.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    print(\"PREPARE\")\n",
    "    ctx = prepare(contract_path)\n",
    "\n",
    "    print(\"\\nFETCH\")\n",
    "    doc_ctxs = [fetch(ctx, p) for p in doc_paths]\n",
    "\n",
    "    print(\"\\nTRANSFORM (async)\")\n",
    "    results = await asyncio.gather(\n",
    "        *[transform_async(ctx, dc) for dc in doc_ctxs]\n",
    "    )\n",
    "\n",
    "    print(\"\\nSAVE\")\n",
    "    merged, paths = save(results, output_dir)\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    print(f\"\\nDone in {elapsed:.1f}s\")\n",
    "    return results, merged, paths, elapsed\n",
    "\n",
    "\n",
    "print(\"Pipeline functions defined: prepare -> fetch -> transform_async -> save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 1: Russian Agricultural DOCX Reports\n",
    "\n",
    "Multi-category extraction from Russian Ministry of Agriculture weekly grain reports.\n",
    "Each DOCX contains harvest, planting, and export tables. The contract classifies\n",
    "tables by keywords and extracts crop names from table titles, report dates from\n",
    "filenames, and year values from dynamic pivot headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Russian agricultural contract\n",
    "ru_ctx = prepare(\"contracts/ru_ag_ministry.yaml\")\n",
    "\n",
    "print(\"\\nSchema summary:\")\n",
    "for name, schema in ru_ctx[\"schemas\"].items():\n",
    "    cols = [c.name for c in schema.columns]\n",
    "    enrich = list(ru_ctx[\"enrichment\"][name].keys())\n",
    "    print(f\"  {name}: LLM cols={cols}, enriched={enrich}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the June DOCX\n",
    "june_path = \"inputs/docx/input/2025-06-24_11-58-45.Russian weekly grain EOW June 20-21 2025-1.docx\"\n",
    "\n",
    "results_june, merged_june, _, elapsed_june = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.yaml\", [june_path], output_dir=\"outputs/ru_june\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: DOCX page | pipe-table | DataFrame (harvest)\n",
    "try:\n",
    "    img_b64, _ = render_document_page(june_path, page=0, dpi=120)\n",
    "except Exception:\n",
    "    img_b64 = None  # LibreOffice not available\n",
    "\n",
    "# Get a sample compressed table for display\n",
    "june_doc_ctx = results_june[0]\n",
    "harvest_data = june_doc_ctx[\"compressed_by_category\"].get(\"harvest\", [])\n",
    "sample_md = harvest_data[0][0] if harvest_data else \"(no harvest tables)\"\n",
    "df_harvest = merged_june.get(\"harvest\", [pd.DataFrame()])\n",
    "df_h = df_harvest[0] if isinstance(df_harvest, list) else df_harvest\n",
    "\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=sample_md, dataframe=df_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display output DataFrames\n",
    "for name, frames in merged_june.items():\n",
    "    df = frames[0] if isinstance(frames, list) else frames\n",
    "    print(f\"=== {name.upper()} ({len(df)} rows) ===\")\n",
    "    display(df.head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on the July DOCX — same contract, different document\n",
    "july_path = \"inputs/docx/input/2025-07-17_10-16-25.Russian weekly grain EOW July 11-12 2025-1.docx\"\n",
    "\n",
    "results_july, merged_july, _, elapsed_july = await run_pipeline_async(\n",
    "    \"contracts/ru_ag_ministry.yaml\", [july_path], output_dir=\"outputs/ru_july\"\n",
    ")\n",
    "\n",
    "print(f\"\\nJune: {elapsed_june:.1f}s, July: {elapsed_july:.1f}s\")\n",
    "for name in merged_july:\n",
    "    df = merged_july[name][0] if isinstance(merged_july[name], list) else merged_july[name]\n",
    "    print(f\"  {name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 2: Australian Shipping Stems — XXL\n",
    "\n",
    "**One canonical model, six providers, full concurrency.**\n",
    "\n",
    "6 shipping stem PDFs from 6 different providers — each expresses the same semantic data\n",
    "(vessel name, port, commodity, tonnage, ETA) with completely different layouts, column\n",
    "names, and formatting. A single canonical schema with rich aliases normalizes them all\n",
    "into one unified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shipping stem contract\n",
    "ship_ctx = prepare(\"contracts/au_shipping_stem.yaml\")\n",
    "\n",
    "print(\"\\nSchema columns with aliases:\")\n",
    "for col in ship_ctx[\"schemas\"][\"vessels\"].columns:\n",
    "    print(f\"  {col.name:15s} {col.type:6s} aliases={col.aliases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all 6 PDFs\n",
    "shipping_pdfs = {\n",
    "    \"Newcastle\":  \"inputs/2857439.pdf\",\n",
    "    \"Bunge\":      \"inputs/Bunge_loadingstatement_2025-09-25.pdf\",\n",
    "    \"CBH\":        \"inputs/CBH Shipping Stem 26092025.pdf\",\n",
    "    \"GrainCorp\":  \"inputs/shipping-stem-2025-11-13.pdf\",\n",
    "    \"Riordan\":    \"inputs/shipping_stem-accc-30092025-1.pdf\",\n",
    "    \"Queensland\": \"inputs/document (1).pdf\",\n",
    "}\n",
    "\n",
    "import fitz\n",
    "print(f\"{'Provider':<14s} {'Filename':<50s} {'Pages':>5s}\")\n",
    "print(\"-\" * 72)\n",
    "for provider, path in shipping_pdfs.items():\n",
    "    doc = fitz.open(path)\n",
    "    pages = len(doc)\n",
    "    doc.close()\n",
    "    print(f\"{provider:<14s} {Path(path).name:<50s} {pages:>5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ALL 6 PDFs concurrently\n",
    "results_ship, merged_ship, paths_ship, elapsed_ship = await run_pipeline_async(\n",
    "    \"contracts/au_shipping_stem.yaml\",\n",
    "    list(shipping_pdfs.values()),\n",
    "    output_dir=\"outputs/shipping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-document summary\n",
    "print(f\"{'Provider':<14s} {'Records':>8s}\")\n",
    "print(\"-\" * 24)\n",
    "total = 0\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    n = sum(len(df) for df in doc_ctx.get(\"dataframes\", {}).values())\n",
    "    total += n\n",
    "    print(f\"{provider:<14s} {n:>8d}\")\n",
    "print(\"-\" * 24)\n",
    "print(f\"{'TOTAL':<14s} {total:>8d}\")\n",
    "print(f\"\\nWall-clock time: {elapsed_ship:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side gallery — one representative page per provider\n",
    "for provider, path in list(shipping_pdfs.items())[:3]:\n",
    "    img_b64, _ = render_document_page(path, page=0, dpi=100)\n",
    "    compressed = compress_spatial_text(path, refine_headers=False)\n",
    "    # Truncate compressed text for display\n",
    "    lines = compressed.splitlines()[:25]\n",
    "    truncated = \"\\n\".join(lines) + \"\\n...\"\n",
    "    print(f\"\\n--- {provider} ---\")\n",
    "    side_by_side_display(image_b64=img_b64, compressed_text=truncated, max_height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified DataFrame — all providers in one schema\n",
    "df_vessels = merged_ship[\"vessels\"]\n",
    "df_all = df_vessels[0] if isinstance(df_vessels, list) else df_vessels\n",
    "print(f\"Unified DataFrame: {df_all.shape}\")\n",
    "display(df_all.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by provider (inferred from source document)\n",
    "# Add source_provider from doc_ctx ordering\n",
    "provider_frames = []\n",
    "for (provider, _), doc_ctx in zip(shipping_pdfs.items(), results_ship):\n",
    "    for df in doc_ctx.get(\"dataframes\", {}).values():\n",
    "        dfp = df.copy()\n",
    "        dfp[\"source_provider\"] = provider\n",
    "        provider_frames.append(dfp)\n",
    "\n",
    "if provider_frames:\n",
    "    df_with_provider = pd.concat(provider_frames, ignore_index=True)\n",
    "    print(\"Records by provider:\")\n",
    "    print(df_with_provider.groupby(\"source_provider\").size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case 3: ACEA Car Registrations\n",
    "\n",
    "**Normalization: pivoted table to flat records.**\n",
    "\n",
    "The ACEA press release PDF contains a dense pivoted table: 28 countries x 7 power types\n",
    "x 3 metrics. The pipeline unpivots this into flat records — a 23-column wide table\n",
    "becomes a 4-column long DataFrame. Vision-enabled interpretation handles the dense\n",
    "hierarchical headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ACEA contract\n",
    "acea_ctx = prepare(\"contracts/acea_car_registrations.yaml\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for col in acea_ctx[\"schemas\"][\"registrations_by_market\"].columns:\n",
    "    print(f\"  {col.name:25s} {col.type:6s} {col.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on ACEA press release PDF (vision-enabled)\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "\n",
    "results_acea, merged_acea, _, elapsed_acea = await run_pipeline_async(\n",
    "    \"contracts/acea_car_registrations.yaml\",\n",
    "    [acea_pdf],\n",
    "    output_dir=\"outputs/acea\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side: PDF | compressed pivot table | unpivoted DataFrame\n",
    "img_b64, _ = render_document_page(acea_pdf, page=0, dpi=120)\n",
    "acea_compressed = compress_spatial_text(acea_pdf, refine_headers=False)\n",
    "acea_df = list(merged_acea.values())[0]\n",
    "if isinstance(acea_df, list):\n",
    "    acea_df = acea_df[0]\n",
    "\n",
    "# Show first page of compressed text\n",
    "first_page = acea_compressed.split(\"\\f\")[0]\n",
    "side_by_side_display(image_b64=img_b64, compressed_text=first_page, dataframe=acea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unpivoted DataFrame\n",
    "print(f\"Shape: {acea_df.shape} (from wide pivoted table to long flat records)\")\n",
    "display(acea_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-trip verification: pivot back to wide format\n",
    "if \"country\" in acea_df.columns and \"car_motorization\" in acea_df.columns:\n",
    "    try:\n",
    "        pivot = acea_df.pivot_table(\n",
    "            index=\"country\",\n",
    "            columns=\"car_motorization\",\n",
    "            values=\"new_car_registration\",\n",
    "            aggfunc=\"sum\",\n",
    "        )\n",
    "        print(f\"Pivoted back: {pivot.shape} (countries x power types)\")\n",
    "        display(pivot.head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"Pivot failed: {e}\")\n",
    "else:\n",
    "    print(\"Columns not available for pivot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Use Case | Format | Documents | Elapsed | Key Feature |\n",
    "|----------|--------|-----------|---------|-------------|\n",
    "| Russian Agriculture | DOCX | 2 reports | see above | Multi-category, pivot years, enrichment |\n",
    "| Australian Shipping | PDF | 6 providers | see above | One schema, 6 layouts, full concurrency |\n",
    "| ACEA Registrations | PDF | 1 press release | see above | Pivoted → flat normalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_docs = 2 + 6 + 1  # June + July + 6 shipping + 1 ACEA\n",
    "total_records = 0\n",
    "for m in [merged_june, merged_july, merged_ship, merged_acea]:\n",
    "    for v in m.values():\n",
    "        df = v[0] if isinstance(v, list) else v\n",
    "        total_records += len(df)\n",
    "\n",
    "total_time = elapsed_june + elapsed_july + elapsed_ship + elapsed_acea\n",
    "\n",
    "print(f\"Total documents processed: {total_docs}\")\n",
    "print(f\"Total records extracted:   {total_records:,}\")\n",
    "print(f\"Total wall-clock time:     {total_time:.1f}s\")\n",
    "print(f\"Throughput:                {total_records / total_time:.0f} records/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
