{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c3aeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Shipping Stem Report\n",
      "\n",
      "Date Generated: 15/09/2025\n",
      "\n",
      "\n",
      "\n",
      "Port Ship Name Ref # Exporter Commodity Quantity(tonnes) NominationReceivedDate of NominationAcceptedDate of ETA ETB ETS Load Status\n",
      "\n",
      "\n",
      "\n",
      "Ship Name Ref #\n",
      "\n",
      "\n",
      "\n",
      "Date of\n",
      "Nomination\n",
      "\n",
      "\n",
      "\n",
      "Accepted\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 10/07/2025 06/08/2025\n",
      "\n",
      "Newcastle ADAGIO NT25084 ARROW COMMODITIES Wheat 26,914\n",
      "11:45 AM 2:25 PM 8:06 AM\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025\n",
      "ADAGIO NT25084 ARROW COMMODITIES Wheat 26,914\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 10/07/2025 06/08/2025 06/08/2025 09/08/2025\n",
      "\n",
      "ARROW COMMODITIES Wheat 26,914 Completed\n",
      "\n",
      "11:45 AM 2:25 PM 8:06 AM 8:06 AM 11:15 PM\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 06/08/2025 06/08/2025 09/08/2025\n",
      "\n",
      "2:25 PM 8:06 AM 8:06 AM 11:15 PM\n",
      "\n",
      "\n",
      "\n",
      "2:25 PM\n",
      "\n",
      "\n",
      "\n",
      "8:06 AM\n",
      "\n",
      "\n",
      "\n",
      "09/08/2025\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 10/07/2025 17/08/2025\n",
      "\n",
      "Newcastle QC ISABELLA NT25082 QUBE GRAINS Wheat 20,333\n",
      "7:59 AM 5:40 PM 4:06 PM\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025\n",
      "QC ISABELLA NT25082 QUBE GRAINS Wheat 20,333\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 10/07/2025 17/08/2025 17/08/2025 19/08/2025\n",
      "\n",
      "QUBE GRAINS Wheat 20,333 Completed\n",
      "\n",
      "7:59 AM 5:40 PM 4:06 PM 4:06 PM 11:55 AM\n",
      "\n",
      "\n",
      "\n",
      "10/07/2025 17/08/2025 17/08/2025 19/08/2025\n",
      "\n",
      "5:40 PM 4:06 PM 4:06 PM 11:55 AM\n",
      "\n",
      "\n",
      "\n",
      "5:40 PM\n",
      "\n",
      "\n",
      "\n",
      "4:06 PM\n",
      "\n",
      "\n",
      "\n",
      "19/08/2025\n",
      "\n",
      "\n",
      "\n",
      "01/08/2025 04/08/2025 20/08/2024\n",
      "\n",
      "Newcastle BRIGHTEN TRADER NT25085 ARROW COMMODITIES Wheat 33,000\n",
      "7:06 PM 10:39 AM 8:00 AM\n",
      "\n",
      "\n",
      "\n",
      "01/08/2025\n",
      "BRIGHTEN TRADER NT25085 ARROW COMMODITIES Wheat 33,000\n",
      "\n",
      "\n",
      "\n",
      "01/08/2025 04/08/2025 20/08/2024 20/08/2024 23/08/2025\n",
      "\n",
      "ARROW COMMODITIES Wheat 33,000 Completed\n",
      "\n",
      "7:06 PM 10:39 AM 8:00 AM 8:00 AM 5:30 PM\n",
      "\n",
      "\n",
      "\n",
      "04/08/2025 20/08/2024 20/08/2024 23/08/2025\n",
      "\n",
      "10:39 AM 8:00 AM 8:00 AM 5:30 PM\n",
      "\n",
      "\n",
      "\n",
      "10:39 AM\n",
      "\n",
      "\n",
      "\n",
      "8:00 AM\n",
      "\n",
      "\n",
      "\n",
      "23/08/2025\n",
      "\n",
      "\n",
      "\n",
      "25/07/2025 25/07/2025 28/08/2025\n",
      "\n",
      "Newcastle ARUNA NAZIK NT25083 CHS BROADBENT Wheat 55,000\n",
      "12:59 PM 3:09 PM 6:35 AM\n",
      "\n",
      "\n",
      "\n",
      "25/07/2025\n",
      "ARUNA NAZIK NT25083 CHS BROADBENT Wheat 55,000\n",
      "\n",
      "\n",
      "\n",
      "25/07/2025 25/07/2025 28/08/2025 28/08/2025 31/08/2025\n",
      "\n",
      "CHS BROADBENT Wheat 55,000 Completed\n",
      "\n",
      "12:59 PM 3:09 PM 6:35 AM 6:35 AM 11:12 AM\n",
      "\n",
      "\n",
      "\n",
      "25/07/2025 28/08/2025 28/08/2025 31/08/2025\n",
      "\n",
      "3:09 PM 6:35 AM 6:35 AM 11:12 AM\n",
      "\n",
      "\n",
      "\n",
      "3:09 PM\n",
      "\n",
      "\n",
      "\n",
      "6:35 AM\n",
      "\n",
      "\n",
      "\n",
      "31/08/2025\n",
      "\n",
      "\n",
      "\n",
      "04/08/2025 04/08/2025 03/09/2025\n",
      "\n",
      "Newcastle GREAT FORTUNE NT25086 QUBE GRAINS Wheat 54,998\n",
      "7:26 AM 9:11 AM 1:06 PM\n",
      "\n",
      "\n",
      "\n",
      "04/08/2025\n",
      "GREAT FORTUNE NT25086 QUBE GRAINS Wheat 54,998\n",
      "\n",
      "\n",
      "\n",
      "04/08/2025 04/08/2025 03/09/2025 03/09/2025 11/09/2025\n",
      "\n",
      "QUBE GRAINS Wheat 54,998 Completed\n",
      "\n",
      "7:26 AM 9:11 AM 1:06 PM 1:06 PM 5:00 AM\n",
      "\n",
      "\n",
      "\n",
      "04/08/2025 03/09/2025 03/09/2025 11/09/2025\n",
      "\n",
      "9:11 AM 1:06 PM 1:06 PM 5:00 AM\n",
      "\n",
      "\n",
      "\n",
      "9:11 AM\n",
      "\n",
      "\n",
      "\n",
      "1:06 PM\n",
      "\n",
      "\n",
      "\n",
      "11/09/2025\n",
      "\n",
      "\n",
      "\n",
      "06/08/2025 07/08/2025 12/09/2025\n",
      "\n",
      "Newcastle ASL ROSE NT25092 CHS BROADBENT Wheat 24,000\n",
      "5:03 PM 12:43 PM 8:36 PM\n",
      "\n",
      "\n",
      "\n",
      "06/08/2025\n",
      "ASL ROSE NT25092 CHS BROADBENT Wheat 24,000\n",
      "\n",
      "\n",
      "\n",
      "06/08/2025 07/08/2025 12/09/2025 12/09/2025 14/09/2025\n",
      "\n",
      "CHS BROADBENT Wheat 24,000 Completed\n",
      "\n",
      "5:03 PM 12:43 PM 8:36 PM 8:36 PM 8:30 PM\n",
      "\n",
      "\n",
      "\n",
      "07/08/2025 12/09/2025 12/09/2025 14/09/2025\n",
      "\n",
      "12:43 PM 8:36 PM 8:36 PM 8:30 PM\n",
      "\n",
      "\n",
      "\n",
      "12:43 PM\n",
      "\n",
      "\n",
      "\n",
      "8:36 PM\n",
      "\n",
      "\n",
      "\n",
      "14/09/2025\n",
      "\n",
      "\n",
      "\n",
      "16/08/2025 17/08/2025 14/09/2025\n",
      "\n",
      "Newcastle MAGIC CELESTE NT25094 ARROW COMMODITIES Wheat 30,000\n",
      "6:09 PM 11:56 AM 6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "16/08/2025\n",
      "MAGIC CELESTE NT25094 ARROW COMMODITIES Wheat 30,000\n",
      "\n",
      "\n",
      "\n",
      "16/08/2025 17/08/2025 14/09/2025 14/09/2025 18/09/2025\n",
      "\n",
      "ARROW COMMODITIES Wheat 30,000 Nominated\n",
      "\n",
      "6:09 PM 11:56 AM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "17/08/2025 14/09/2025 14/09/2025 18/09/2025\n",
      "\n",
      "11:56 AM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "11:56 AM\n",
      "\n",
      "\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "18/09/2025\n",
      "\n",
      "\n",
      "\n",
      "01/09/2025 01/09/2025 22/09/2025\n",
      "\n",
      "Newcastle ANNE NT25091 QUBE GRAINS Wheat 50,000\n",
      "10:28 AM 10:48 AM 6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "01/09/2025\n",
      "ANNE NT25091 QUBE GRAINS Wheat 50,000\n",
      "\n",
      "\n",
      "\n",
      "01/09/2025 01/09/2025 22/09/2025 22/09/2025 26/09/2025\n",
      "\n",
      "QUBE GRAINS Wheat 50,000 Nominated\n",
      "\n",
      "10:28 AM 10:48 AM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "01/09/2025 22/09/2025 22/09/2025 26/09/2025\n",
      "\n",
      "10:48 AM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "10:48 AM\n",
      "\n",
      "\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "26/09/2025\n",
      "\n",
      "\n",
      "\n",
      "03/09/2025 03/09/2025 25/09/2025\n",
      "\n",
      "Newcastle OCEAN ACE NT25093 CHS BROADBENT Wheat 50,000\n",
      "10:04 AM 12:32 PM 6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "03/09/2025\n",
      "OCEAN ACE NT25093 CHS BROADBENT Wheat 50,000\n",
      "\n",
      "\n",
      "\n",
      "03/09/2025 03/09/2025 25/09/2025 25/09/2025 29/09/2025\n",
      "\n",
      "CHS BROADBENT Wheat 50,000 Nominated\n",
      "\n",
      "10:04 AM 12:32 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "03/09/2025 25/09/2025 25/09/2025 29/09/2025\n",
      "\n",
      "12:32 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "12:32 PM\n",
      "\n",
      "\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "29/09/2025\n",
      "\n",
      "\n",
      "\n",
      "28/08/2025 29/08/2025 01/10/2025\n",
      "\n",
      "Newcastle OCEAN EMERALD NT25103 QUBE GRAINS Wheat 20,000\n",
      "2:17 PM 10:30 PM 6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "28/08/2025\n",
      "OCEAN EMERALD NT25103 QUBE GRAINS Wheat 20,000\n",
      "\n",
      "\n",
      "\n",
      "28/08/2025 29/08/2025 01/10/2025 01/10/2025 05/10/2025\n",
      "\n",
      "QUBE GRAINS Wheat 20,000 Nominated\n",
      "\n",
      "2:17 PM 10:30 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "29/08/2025 01/10/2025 01/10/2025 05/10/2025\n",
      "\n",
      "10:30 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "10:30 PM\n",
      "\n",
      "\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "05/10/2025\n",
      "\n",
      "\n",
      "\n",
      "08/09/2025 08/09/2025 10/10/2025\n",
      "\n",
      "Newcastle PAOLO TOPIC NT25105 ARROW COMMODITIES Wheat 24,000\n",
      "9:49 AM 12:10 PM 6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "08/09/2025\n",
      "PAOLO TOPIC NT25105 ARROW COMMODITIES Wheat 24,000\n",
      "\n",
      "\n",
      "\n",
      "08/09/2025 08/09/2025 10/10/2025 10/10/2025 14/10/2025\n",
      "\n",
      "ARROW COMMODITIES Wheat 24,000 Nominated\n",
      "\n",
      "9:49 AM 12:10 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "08/09/2025 10/10/2025 10/10/2025 14/10/2025\n",
      "\n",
      "12:10 PM 6:00 AM 6:00 AM 6:00 PM\n",
      "\n",
      "\n",
      "\n",
      "12:10 PM\n",
      "\n",
      "\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "\n",
      "14/10/2025\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "path = \"/Volumes/WD Green/dev/git/pdf_ocr/pdf-ocr/inputs/2857439.pdf\"\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(path)\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1f34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping Stem Report\n",
      "\n",
      "Date Generated: 15/09/2025\n",
      "\n",
      "Ref #\n",
      "Ship Name\n",
      "\n",
      "Date of\n",
      "Nomination\n",
      "\n",
      "Received\n",
      "Port\n",
      "ETA\n",
      "Exporter\n",
      "Quantity\n",
      "(tonnes)\n",
      "Commodity\n",
      "Load Status\n",
      "ETB\n",
      "ETS\n",
      "Date of\n",
      "Nomination\n",
      "\n",
      "Accepted\n",
      "\n",
      "09/08/2025\n",
      "\n",
      "NT25084\n",
      "ADAGIO\n",
      "10/07/2025\n",
      "\n",
      "### 11:45 AM\n",
      "Newcastle\n",
      "06/08/2025\n",
      "\n",
      "8:06 AM\n",
      "ARROW COMMODITIES\n",
      "26,914\n",
      "Wheat\n",
      "Completed\n",
      "06/08/2025\n",
      "\n",
      "11:15 PM\n",
      "10/07/2025\n",
      "\n",
      "2:25 PM\n",
      "\n",
      "8:06 AM\n",
      "\n",
      "19/08/2025\n",
      "\n",
      "### NT25082\n",
      "QC ISABELLA\n",
      "10/07/2025\n",
      "\n",
      "### 7:59 AM\n",
      "Newcastle\n",
      "17/08/2025\n",
      "\n",
      "4:06 PM\n",
      "QUBE GRAINS\n",
      "20,333\n",
      "Wheat\n",
      "Completed\n",
      "17/08/2025\n",
      "\n",
      "11:55 AM\n",
      "10/07/2025\n",
      "\n",
      "5:40 PM\n",
      "\n",
      "4:06 PM\n",
      "\n",
      "23/08/2025\n",
      "\n",
      "### NT25085\n",
      "BRIGHTEN TRADER\n",
      "01/08/2025\n",
      "\n",
      "### 7:06 PM\n",
      "Newcastle\n",
      "20/08/2024\n",
      "\n",
      "8:00 AM\n",
      "ARROW COMMODITIES\n",
      "33,000\n",
      "Wheat\n",
      "Completed\n",
      "20/08/2024\n",
      "\n",
      "5:30 PM\n",
      "04/08/2025\n",
      "\n",
      "10:39 AM\n",
      "\n",
      "8:00 AM\n",
      "\n",
      "31/08/2025\n",
      "\n",
      "### NT25083\n",
      "ARUNA NAZIK\n",
      "25/07/2025\n",
      "\n",
      "### 12:59 PM\n",
      "Newcastle\n",
      "28/08/2025\n",
      "\n",
      "6:35 AM\n",
      "CHS BROADBENT\n",
      "55,000\n",
      "Wheat\n",
      "Completed\n",
      "28/08/2025\n",
      "\n",
      "11:12 AM\n",
      "25/07/2025\n",
      "\n",
      "3:09 PM\n",
      "\n",
      "6:35 AM\n",
      "\n",
      "11/09/2025\n",
      "\n",
      "### NT25086\n",
      "GREAT FORTUNE\n",
      "04/08/2025\n",
      "\n",
      "### 7:26 AM\n",
      "Newcastle\n",
      "03/09/2025\n",
      "\n",
      "1:06 PM\n",
      "QUBE GRAINS\n",
      "54,998\n",
      "Wheat\n",
      "Completed\n",
      "03/09/2025\n",
      "\n",
      "5:00 AM\n",
      "04/08/2025\n",
      "\n",
      "9:11 AM\n",
      "\n",
      "1:06 PM\n",
      "\n",
      "14/09/2025\n",
      "\n",
      "### NT25092\n",
      "ASL ROSE\n",
      "06/08/2025\n",
      "\n",
      "### 5:03 PM\n",
      "Newcastle\n",
      "12/09/2025\n",
      "\n",
      "8:36 PM\n",
      "CHS BROADBENT\n",
      "24,000\n",
      "Wheat\n",
      "Completed\n",
      "12/09/2025\n",
      "\n",
      "8:30 PM\n",
      "07/08/2025\n",
      "\n",
      "12:43 PM\n",
      "\n",
      "8:36 PM\n",
      "\n",
      "18/09/2025\n",
      "\n",
      "### NT25094\n",
      "MAGIC CELESTE\n",
      "16/08/2025\n",
      "\n",
      "### 6:09 PM\n",
      "Newcastle\n",
      "14/09/2025\n",
      "\n",
      "6:00 AM\n",
      "ARROW COMMODITIES\n",
      "30,000\n",
      "Wheat\n",
      "Nominated\n",
      "14/09/2025\n",
      "\n",
      "6:00 PM\n",
      "17/08/2025\n",
      "\n",
      "11:56 AM\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "26/09/2025\n",
      "\n",
      "NT25091\n",
      "ANNE\n",
      "01/09/2025\n",
      "\n",
      "### 10:28 AM\n",
      "Newcastle\n",
      "22/09/2025\n",
      "\n",
      "6:00 AM\n",
      "QUBE GRAINS\n",
      "50,000\n",
      "Wheat\n",
      "Nominated\n",
      "22/09/2025\n",
      "\n",
      "6:00 PM\n",
      "01/09/2025\n",
      "\n",
      "10:48 AM\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "29/09/2025\n",
      "\n",
      "### NT25093\n",
      "OCEAN ACE\n",
      "03/09/2025\n",
      "\n",
      "### 10:04 AM\n",
      "Newcastle\n",
      "25/09/2025\n",
      "\n",
      "6:00 AM\n",
      "CHS BROADBENT\n",
      "50,000\n",
      "Wheat\n",
      "Nominated\n",
      "25/09/2025\n",
      "\n",
      "6:00 PM\n",
      "03/09/2025\n",
      "\n",
      "12:32 PM\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "05/10/2025\n",
      "\n",
      "### NT25103\n",
      "OCEAN EMERALD\n",
      "28/08/2025\n",
      "\n",
      "### 2:17 PM\n",
      "Newcastle\n",
      "01/10/2025\n",
      "\n",
      "6:00 AM\n",
      "QUBE GRAINS\n",
      "20,000\n",
      "Wheat\n",
      "Nominated\n",
      "01/10/2025\n",
      "\n",
      "6:00 PM\n",
      "29/08/2025\n",
      "\n",
      "10:30 PM\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "14/10/2025\n",
      "\n",
      "### NT25105\n",
      "PAOLO TOPIC\n",
      "08/09/2025\n",
      "\n",
      "### 9:49 AM\n",
      "Newcastle\n",
      "10/10/2025\n",
      "\n",
      "6:00 AM\n",
      "ARROW COMMODITIES\n",
      "24,000\n",
      "Wheat\n",
      "Nominated\n",
      "10/10/2025\n",
      "\n",
      "6:00 PM\n",
      "08/09/2025\n",
      "\n",
      "12:10 PM\n",
      "\n",
      "6:00 AM\n",
      "\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def reconstruct_markdown(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_md = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Récupère les blocs de texte triés (haut-gauche vers bas-droite)\n",
    "        # 'sort=True' gère souvent déjà l'ordre de lecture naturel\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        \n",
    "        for b in blocks:\n",
    "            x0, y0, x1, y1, text, block_no, block_type = b\n",
    "            \n",
    "            if block_type == 0:  # C'est un bloc de texte\n",
    "                clean_text = text.strip()\n",
    "                if not clean_text: continue\n",
    "                \n",
    "                # Exemple de logique de style simple :\n",
    "                # Si le bloc est très à gauche, c'est peut-être un titre ou une puce\n",
    "                if x0 < 100 and len(clean_text) < 50:\n",
    "                    full_md.append(f\"### {clean_text}\\n\")\n",
    "                else:\n",
    "                    full_md.append(f\"{clean_text}\\n\")\n",
    "        \n",
    "        full_md.append(\"\\n---\\n\") # Séparateur de page\n",
    "    \n",
    "    return \"\\n\".join(full_md)\n",
    "\n",
    "md_output = reconstruct_markdown(path)\n",
    "print(md_output)"
   ]
  },
  {
   "cell_type": "code",
   "id": "moi32xu0ihr",
   "source": "from pdf_ocr import pdf_to_spatial_text\n\noutput = pdf_to_spatial_text(\"inputs/2857439.pdf\")\nprint(output)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4i3qj1yp7qc",
   "source": "import os\n\nfor fname in sorted(os.listdir(\"inputs\")):\n    if not fname.endswith(\".pdf\"):\n        continue\n    path = os.path.join(\"inputs\", fname)\n    text = pdf_to_spatial_text(path)\n    lines = text.split(\"\\n\")\n    max_width = max((len(l) for l in lines), default=0)\n    print(f\"{fname:50s}  lines={len(lines):4d}  max_width={max_width:4d}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ja98ihg0ty",
   "source": "## Compressed Spatial Text\n\n`compress_spatial_text()` produces a token-efficient representation by classifying page regions (tables, headings, text blocks, key-value pairs) and rendering them as markdown tables and flowing text instead of whitespace-heavy grids.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "idb5df8kpmf",
   "source": "from pdf_ocr import compress_spatial_text, pdf_to_spatial_text\n\npdf = \"inputs/2857439.pdf\"\n\nspatial = pdf_to_spatial_text(pdf)\ncompressed = compress_spatial_text(pdf)\n\nprint(compressed)\nprint(f\"\\n--- Compression: {len(spatial)} chars → {len(compressed)} chars ({(1 - len(compressed)/len(spatial))*100:.0f}% reduction)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fl46cmb0yjk",
   "source": "import os\n\nprint(f\"{'File':<50} {'Spatial':>8} {'Compressed':>10} {'Reduction':>10}\")\nprint(\"-\" * 82)\n\nfor fname in sorted(os.listdir(\"inputs\")):\n    if not fname.endswith(\".pdf\"):\n        continue\n    path = os.path.join(\"inputs\", fname)\n    s = pdf_to_spatial_text(path)\n    c = compress_spatial_text(path)\n    reduction = (1 - len(c) / len(s)) * 100 if len(s) > 0 else 0\n    print(f\"{fname:<50} {len(s):>8} {len(c):>10} {reduction:>9.0f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lovuvmoji2f",
   "source": "## Table Interpretation\n\n`interpret_table()` takes compressed text and a **canonical schema** describing the columns your application expects, then uses an LLM pipeline to extract structured records.\n\nThe schema maps inconsistent PDF column names (e.g. \"Ship Name\", \"Vessel\", \"Vessel Name\") to stable canonical names via aliases. Two modes are available:\n\n- **2-step** (`interpret_table`) — parse table structure first, then map to schema. Step 2 is **batched**: each page's parsed rows are split into chunks (default 20 rows) so the LLM produces complete output without truncation. All batches across all pages run concurrently.\n- **Single-shot** (`interpret_table_single_shot`) — one LLM call per page. Faster for simple flat tables, but cannot batch and may truncate on dense pages (50+ rows).\n\nBoth modes **auto-split** multi-page input (pages joined by `\\f`) and process all pages **concurrently** via `asyncio.gather()`, then merge results into a single `MappedTable`. Single-page input with few rows is processed synchronously with no event loop overhead.\n\nEach output record carries a `_page` extra field (1-indexed) indicating which source page it originated from.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pvubmndte5j",
   "source": "from pdf_ocr import (\n    compress_spatial_text,\n    interpret_table,\n    interpret_table_single_shot,\n    CanonicalSchema,\n    ColumnDef,\n    to_records,\n)\n\n# Define the canonical schema as a plain dict (e.g. loaded from a JSON file).\n# CanonicalSchema.from_dict() converts it into the typed dataclass.\n#\n# Note: \"port\" has no aliases — it will be inferred from context (section headers,\n# document title, or repeated contextual values) rather than matched to a column name.\nschema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"port\",            \"type\": \"string\", \"description\": \"Loading port name — may appear as a section header or document-level context rather than a table column\", \"aliases\": []},\n        {\"name\": \"vessel_name\",     \"type\": \"string\", \"description\": \"Name of the vessel\",             \"aliases\": [\"Ship Name\", \"Vessel\"]},\n        {\"name\": \"ref\",             \"type\": \"string\", \"description\": \"Reference number\",               \"aliases\": [\"Ref #\", \"Reference\"]},\n        {\"name\": \"exporter\",        \"type\": \"string\", \"description\": \"Exporting company\",              \"aliases\": [\"Exporter\"]},\n        {\"name\": \"commodity\",       \"type\": \"string\", \"description\": \"Type of commodity\",              \"aliases\": [\"Commodity\"]},\n        {\"name\": \"quantity_tonnes\", \"type\": \"int\",    \"description\": \"Quantity in metric tonnes\",      \"aliases\": [\"Quantity\", \"Quantity(tonnes)\", \"Total\"]},\n        {\"name\": \"eta\",             \"type\": \"string\", \"description\": \"Estimated time of arrival\",      \"aliases\": [\"ETA\", \"Date ETA of Ship\"]},\n        {\"name\": \"etb\",             \"type\": \"string\", \"description\": \"Estimated time of berthing\",     \"aliases\": [\"ETB\"]},\n        {\"name\": \"ets\",             \"type\": \"string\", \"description\": \"Estimated time of sailing\",      \"aliases\": [\"ETS\"]},\n        {\"name\": \"status\",          \"type\": \"string\", \"description\": \"Loading status\",                 \"aliases\": [\"Status\", \"Load Status\"]},\n    ],\n}\n\nschema = CanonicalSchema.from_dict(schema_dict)\n\nprint(f\"Schema: {schema.description}\")\nprint(f\"Columns ({len(schema.columns)}):\")\nfor col in schema.columns:\n    print(f\"  {col.name:20s}  {col.type:6s}  aliases={col.aliases}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0ptdjxh7e469",
   "source": "# Compress the shipping stem PDF, then run the 2-step interpret pipeline.\n# Step 1 (AnalyzeAndParseTable) identifies structure and parses rows.\n# Step 2 (MapToCanonicalSchema) maps columns to our canonical names.\n\ncompressed = compress_spatial_text(\"inputs/2857439.pdf\")\nresult = interpret_table(compressed, schema, model=\"openai/gpt-4o\")\n\nprint(f\"Records:           {len(result.records)}\")\nprint(f\"Unmapped columns:  {result.unmapped_columns}\")\nif result.mapping_notes:\n    print(f\"Mapping notes:     {result.mapping_notes}\")\n\nprint(\"\\n--- Extracted records ---\\n\")\nfor i, rec in enumerate(to_records(result), 1):\n    print(f\"[{i}] {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b5havbh3emb",
   "source": "# Interpretation metadata: model used, per-field mapping rationale, detected sections.\n# This shows HOW each canonical column was resolved — direct match, alias, or context inference.\n\nmeta = result.metadata\nprint(f\"Model: {meta.model}\")\nprint(f\"Sections detected: {meta.sections_detected}\")\nprint(f\"\\nField mappings ({len(meta.field_mappings)}):\\n\")\nfor fm in meta.field_mappings:\n    print(f\"  {fm.column_name:20s}  confidence={fm.confidence.value:6s}  source={fm.source}\")\n    print(f\"  {'':20s}  rationale: {fm.rationale}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "trggmwnj7mi",
   "source": "# Compare: single-shot mode does both steps in one LLM call.\n# Faster (1 round-trip instead of 2), but may be less accurate on complex tables.\n\nresult_ss = interpret_table_single_shot(compressed, schema, model=\"openai/gpt-4o\")\n\nprint(f\"Records (single-shot):  {len(result_ss.records)}\")\nprint(f\"Unmapped columns:       {result_ss.unmapped_columns}\")\n\nprint(\"\\n--- Single-shot records ---\\n\")\nfor i, rec in enumerate(to_records(result_ss), 1):\n    print(f\"[{i}] {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "naakfdlktr",
   "source": "## Multi-page auto-split with batching\n\n`compress_spatial_text()` joins pages with `\\f` (form-feed). When `interpret_table()` receives multi-page input, it splits on `\\f` and processes all pages **concurrently**.\n\nStep 2 (schema mapping) is **batched** — each page's parsed rows are split into chunks of `batch_size` rows (default 20) before calling the LLM. This prevents truncation on dense pages with many data rows. All batches across all pages run concurrently via `asyncio.gather()`, then results are merged into a single `MappedTable`.\n\nEach record in the output carries a `_page` field (1-indexed) so you can trace which source page it came from.\n\nBelow we run the full pipeline on `shipping-stem-2025-11-13.pdf` (3 pages, 180+ records) — no manual splitting needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e2gqgcwhc3k",
   "source": "# Multi-page PDF: 3 pages, 180+ records.\n# interpret_table() auto-splits on \\f, batches step 2 (~20 rows per LLM call),\n# and runs all batches concurrently.\n\ncompressed_mp = compress_spatial_text(\"inputs/shipping-stem-2025-11-13.pdf\")\npages = [p for p in compressed_mp.split(\"\\f\") if p.strip()]\nprint(f\"Pages detected: {len(pages)}\")\nprint(f\"Total compressed chars: {len(compressed_mp)}\")\n\nresult_mp = interpret_table(compressed_mp, schema, model=\"openai/gpt-4o\")\n\nrecords_mp = to_records(result_mp)\nprint(f\"\\nRecords extracted:  {len(records_mp)}\")\nprint(f\"Unmapped columns:   {result_mp.unmapped_columns}\")\nprint(f\"Sections detected:  {result_mp.metadata.sections_detected}\")\nif result_mp.mapping_notes:\n    print(f\"Mapping notes:      {result_mp.mapping_notes}\")\n\n# Show page distribution\nfrom collections import Counter\npage_counts = Counter(r.get(\"_page\") for r in records_mp)\nprint(f\"\\nRecords per page:   {dict(sorted(page_counts.items()))}\")\n\nprint(\"\\n--- First 10 records ---\\n\")\nfor i, rec in enumerate(records_mp[:10], 1):\n    print(f\"[{i}] {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}