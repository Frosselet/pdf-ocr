{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6d3e450f0a",
   "metadata": {},
   "source": [
    "# PDF-OCR Walkthrough\n",
    "\n",
    "This notebook demonstrates why standard PDF extraction tools fail on real-world tabular documents, and how pdf-ocr's spatial approach solves the problem — producing clean, structured text that LLMs can reliably interpret.\n",
    "\n",
    "We use `2857439.pdf` (a shipping stem with 11 vessel records) as the running example. It has dense multi-row records: each vessel spans 3 rows (dates, data, times) with stacked headers and no drawn table borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30fdcaa9c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"inputs/2857439.pdf\"\n",
    "BUNGE_PATH = \"inputs/Bunge_loadingstatement_2025-09-25.pdf\"\n",
    "CBH_PATH = \"inputs/CBH Shipping Stem 26092025.pdf\"\n",
    "QUEENSLAND_PATH = \"inputs/document (1).pdf\"\n",
    "RIORDAN_PATH= \"inputs/shipping_stem-accc-30092025-1.pdf\"\n",
    "GRAINCORP_PATH = \"inputs/shipping-stem-2025-11-13.pdf\"\n",
    "ACEA_PATH = \"inputs/Loading-Statement-for-Web-Portal-20250923.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1ed786ea4",
   "metadata": {},
   "source": [
    "## 1. Traditional PDF Extraction Tools\n",
    "\n",
    "Let's try the most common Python libraries for extracting text and tables from PDFs, and see where they break down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cc23d5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open(PDF_PATH)\n",
    "page = doc[0]\n",
    "\n",
    "# Raw text extraction — returns text in PDF stream order\n",
    "raw_text = page.get_text()\n",
    "doc.close()\n",
    "\n",
    "print(\"=== fitz page.get_text() ===\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b633fe6a432",
   "metadata": {},
   "source": [
    "`get_text()` returns text in **PDF stream order** — the order objects were written into the file. Column headers, dates, ship names, and numeric values are jumbled into a single stream. There is no way to tell which value belongs to which column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e1791f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(PDF_PATH)\n",
    "print(\"=== pymupdf4llm.to_markdown() ===\")\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3952cff0c4e",
   "metadata": {},
   "source": [
    "`pymupdf4llm` attempts to reconstruct markdown from the PDF. It detects there is a table, but the result is garbled — headers and data don't align into the correct columns. Multi-row records (dates/data/times per vessel) are impossible to reconstruct from this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7e5522e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pdfplumber\n",
    "\n",
    "    with pdfplumber.open(PDF_PATH) as pdf:\n",
    "        page = pdf.pages[0]\n",
    "        tables = page.extract_tables()\n",
    "        print(f\"=== pdfplumber: {len(tables)} table(s) detected ===\\n\")\n",
    "\n",
    "        for i, table in enumerate(tables):\n",
    "            print(f\"--- Table {i+1} ({len(table)} rows) ---\")\n",
    "            for row in table[:8]:\n",
    "                print(row)\n",
    "            if len(table) > 8:\n",
    "                print(f\"  ... ({len(table) - 8} more rows)\")\n",
    "\n",
    "        if not tables:\n",
    "            text = page.extract_text()\n",
    "            print(\"No tables detected. Raw text (first 500 chars):\")\n",
    "            print(text[:500])\n",
    "\n",
    "except ImportError:\n",
    "    print(\"pdfplumber is not installed (pip install pdfplumber)\")\n",
    "    print()\n",
    "    print(\"Typical pdfplumber behavior on this PDF:\")\n",
    "    print(\"- extract_tables() returns 0 tables (no visible borders/lines)\")\n",
    "    print(\"- extract_text() returns stream-order text, similar to fitz\")\n",
    "    print()\n",
    "    print(\"pdfplumber relies on visible table borders (lines/rectangles)\")\n",
    "    print(\"to detect table boundaries. Most shipping stems use whitespace\")\n",
    "    print(\"alignment without drawn borders, so pdfplumber finds nothing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b549372443",
   "metadata": {},
   "source": [
    "## 2. Why Traditional Tools Fail\n",
    "\n",
    "The fundamental issue is **stream order vs. visual layout**:\n",
    "\n",
    "- PDFs store text as positioned drawing commands — each text span has (x, y) coordinates and a string. The **file order is arbitrary** and rarely matches reading order.\n",
    "- `get_text()` returns spans in file order, **destroying spatial relationships**.\n",
    "- `pymupdf4llm` applies heuristics to reorder text, but breaks on multi-row records, narrow columns, and stacked headers.\n",
    "- `pdfplumber` relies on **drawn table borders** (lines/rectangles) to detect tables. Most real-world documents — shipping stems, loading statements, financial reports — use whitespace alignment with no borders at all.\n",
    "\n",
    "These tools try to *reorder* text into reading order. But tabular PDFs need **spatial positioning**: knowing that `\"ADAGIO\"` sits directly below `\"Ship Name\"` and to the right of `\"Newcastle\"`. No amount of reordering recovers that relationship — you need the original (x, y) coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59dffff6c4",
   "metadata": {},
   "source": [
    "### A deeper problem: merged spans\n",
    "\n",
    "Beyond stream order, some PDFs encode **multiple column values as a single text string**. The Bunge loading statement is a prime example — PyMuPDF returns the time and exporter as one span: `\"7:00:00 PM BUNGE\"`, and the quantity and commodity as another: `\"33020 WHEAT\"`.\n",
    "\n",
    "No amount of reordering can split these — the values are fused at the PDF encoding level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebba58efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(BUNGE_PATH)\n",
    "page = doc[0]\n",
    "data = page.get_text(\"dict\")\n",
    "\n",
    "print(\"=== Bunge: raw PDF spans (right side of table) ===\\n\")\n",
    "for block in data[\"blocks\"]:\n",
    "    if block[\"type\"] != 0:\n",
    "        continue\n",
    "    for line in block[\"lines\"]:\n",
    "        for span in line[\"spans\"]:\n",
    "            text = span[\"text\"].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            x = span[\"origin\"][0]\n",
    "            y = span[\"origin\"][1]\n",
    "            # Focus on the right side of the first few data rows\n",
    "            if x >= 440 and 85 < y < 105:\n",
    "                w = span[\"bbox\"][2] - span[\"bbox\"][0]\n",
    "                print(f'  x={x:7.1f}  width={w:5.1f}  \"{text}\"')\n",
    "\n",
    "print()\n",
    "print(\"Notice: \\\"7:00:00 PM BUNGE\\\" is ONE span (width=38.7pt).\")\n",
    "print(\"The time and exporter are fused — no tool can separate them\")\n",
    "print(\"without knowing where the column boundary should be.\")\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4fd88d1de",
   "metadata": {},
   "source": [
    "## 3. Spatial Grid Rendering\n",
    "\n",
    "Instead of reordering text, pdf-ocr projects every text span onto a **monospace character grid** that mirrors the physical page:\n",
    "\n",
    "```\n",
    "PDF span (x=120pt, y=200pt, \"ADAGIO\")  →  grid[row=15][col=20] = \"ADAGIO\"\n",
    "```\n",
    "\n",
    "The pipeline:\n",
    "\n",
    "1. **Extract spans** — `page.get_text(\"dict\")` returns each span with its text, origin (x, y), and bounding box\n",
    "2. **Compute cell width** — median character width across the page (adapts to font size)\n",
    "3. **Cluster y-coordinates** — group spans into rows (2pt tolerance for sub-pixel jitter)\n",
    "4. **Map to grid** — `col = round((x - x_min) / cell_w)`, `row = cluster_index`\n",
    "5. **Render** — write each span into a character buffer at its grid position\n",
    "\n",
    "The result is plain text where columns, tables, and scattered labels appear exactly where they sit visually in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e94e546918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "spatial = pdf_to_spatial_text(PDF_PATH)\n",
    "print(\"=== pdf_to_spatial_text() ===\")\n",
    "print(spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b81331b8f3",
   "metadata": {},
   "source": [
    "The spatial grid **preserves the visual layout perfectly**. Column headers sit above their data. Multi-row records (dates / data / times) stay visually grouped per vessel. You can read the table just as you would in the PDF.\n",
    "\n",
    "But there's a problem: this ~6,000 character output is **mostly whitespace padding** — an expensive waste of LLM tokens. The next step compresses this into a token-efficient structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293254230aa0",
   "metadata": {},
   "source": [
    "### Spatial text reveals column boundaries\n",
    "\n",
    "Even when spans are merged, the spatial grid positions every character correctly. Looking at the Bunge spatial text, you can see the header labels `\"EXPORTER\"` at column 219 and `\"COMMODITY\"` at column 245 — exactly above where the exporter and commodity values should appear in the data rows.\n",
    "\n",
    "The merged span `\"7:00:00 PM BUNGE\"` starts at column 208 and stretches to column 223. Character 11 (`\"B\"` of `\"BUNGE\"`) lands at column 219 — the same position as the `\"EXPORTER\"` header. The spatial grid tells us exactly where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6d293e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr.spatial_text import _extract_page_layout, _open_pdf\n",
    "\n",
    "doc = _open_pdf(BUNGE_PATH)\n",
    "layout = _extract_page_layout(doc[0])\n",
    "\n",
    "print(\"=== Bunge: header rows (define column boundaries) ===\\n\")\n",
    "for ri in [3, 4]:  # Key header rows\n",
    "    entries = sorted(layout.rows[ri])\n",
    "    for col, text in entries:\n",
    "        if col >= 190:\n",
    "            print(f\"  row {ri}  col={col:4d}  \\\"{text}\\\"\")\n",
    "\n",
    "print(\"\\n=== Bunge: data row 9 (has merged spans) ===\\n\")\n",
    "entries = sorted(layout.rows[9])\n",
    "for col, text in entries:\n",
    "    if col >= 190:\n",
    "        marker = \" <-- MERGED\" if len(text) > 12 else \"\"\n",
    "        print(f\"  row 9  col={col:4d}  \\\"{text}\\\"{marker}\")\n",
    "\n",
    "print()\n",
    "print(\"Header col 219 = EXPORTER, col 245 = COMMODITY\")\n",
    "print(\"Data   col 208 = \\\"7:00:00 PM BUNGE\\\" (16 chars, ends at 224)\")\n",
    "print(\"Split position: 219 - 208 = char index 11 → between \\\"PM\\\" and \\\"BUNGE\\\"\")\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27940bd285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "print(f\"{'File':<55} {'Lines':>6} {'Width':>6} {'Chars':>7}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    text = pdf_to_spatial_text(path)\n",
    "    lines = text.split(\"\\n\")\n",
    "    max_width = max((len(l) for l in lines), default=0)\n",
    "    print(f\"{fname:<55} {len(lines):>6} {max_width:>6} {len(text):>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bb966166e",
   "metadata": {},
   "source": [
    "## 4. Compressed Spatial Text\n",
    "\n",
    "`compress_spatial_text()` works from the same raw span data but produces a **token-efficient structured representation**. It:\n",
    "\n",
    "1. **Splits merged spans** — uses column boundaries from header rows to split data spans that the PDF fused (e.g., `\"7:00:00 PM BUNGE\"` → `\"7:00:00 PM\"` + `\"BUNGE\"`)\n",
    "2. **Classifies page regions** — tables, headings, text blocks, key-value pairs, scattered text\n",
    "3. **Detects multi-row records** — merges repeating row patterns (e.g., 3-row shipping records) into single logical rows\n",
    "4. **Renders each region** using its natural format — markdown pipe tables, flowing paragraphs, `key: value` lines\n",
    "\n",
    "When `refine_headers=True` (the default), a lightweight LLM call (GPT-4o-mini) further improves results:\n",
    "\n",
    "- **Header refinement**: Cleans stacked/multiline column headers, handles hierarchical spanning headers\n",
    "- **Table detection fallback**: When heuristics find no table on a page, the LLM detects and extracts tabular data\n",
    "\n",
    "Setting `refine_headers=False` gives pure heuristic output with no LLM calls (no API key required). Even without the LLM, span splitting and pipe table rendering work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f3693ae367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== compress_spatial_text(refine_headers=False) — heuristic only ===\n",
      "Characters: 2086\n",
      "Pipe tables: Yes\n",
      "\n",
      "Shipping Stem Report\n",
      "\n",
      "Date Generated: 15/09/2025\n",
      "\n",
      "Date of Quantity: Date of\n",
      "\n",
      "|Port|Ship Name|Ref #|Exporter|Commodity||Nomination|Nomination|ETA|ETB|ETS|Load Status|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "||||||(tonnes)|||||||\n",
      "|||||||Received|Accepted|||||\n",
      "|Newcastle|ADAGIO|NT25084|ARROW COMMODITIES|Wheat|26,914|10/07/2025 11:45 AM|10/07/2025 2:25 PM|06/08/2025 8:06 AM|06/08/2025 8:06 AM|09/08/2025 11:15 PM|Completed|\n",
      "|Newcastle|QC ISABELLA|NT25082|QUBE GRAINS|Wheat|20,333|10/07/2025 7:59 AM|10/07/2025 5:40 PM|17/08/2025 4:06 PM|17/08/2025 4:06 PM|19/08/2025 11:55 AM|Completed|\n",
      "|Newcastle|BRIGHTEN TRADER|NT25085|ARROW COMMODITIES|Wheat|33,000|01/08/2025 7:06 PM|04/08/2025 10:39 AM|20/08/2024 8:00 AM|20/08/2024 8:00 AM|23/08/2025 5:30 PM|Completed|\n",
      "|Newcastle|ARUNA NAZIK|NT25083|CHS BROADBENT|Wheat|55,000|25/07/2025 12:59 PM|25/07/2025 3:09 PM|28/08/2025 6:35 AM|28/08/2025 6:35 AM|31/08/2025 11:12 AM|Completed|\n",
      "|Newcastle|GREAT FORTUNE|NT25086|QUBE GRAINS|Wheat|54,998|04/08/2025 7:26 AM|04/08/2025 9:11 AM|03/09/2025 1:06 PM|03/09/2025 1:06 PM|11/09/2025 5:00 AM|Completed|\n",
      "|Newcastle|ASL ROSE|NT25092|CHS BROADBENT|Wheat|24,000|06/08/2025 5:03 PM|07/08/2025 12:43 PM|12/09/2025 8:36 PM|12/09/2025 8:36 PM|14/09/2025 8:30 PM|Completed|\n",
      "|Newcastle|MAGIC CELESTE|NT25094|ARROW COMMODITIES|Wheat|30,000|16/08/2025 6:09 PM|17/08/2025 11:56 AM|14/09/2025 6:00 AM|14/09/2025 6:00 AM|18/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|ANNE|NT25091|QUBE GRAINS|Wheat|50,000|01/09/2025 10:28 AM|01/09/2025 10:48 AM|22/09/2025 6:00 AM|22/09/2025 6:00 AM|26/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|OCEAN ACE|NT25093|CHS BROADBENT|Wheat|50,000|03/09/2025 10:04 AM|03/09/2025 12:32 PM|25/09/2025 6:00 AM|25/09/2025 6:00 AM|29/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|OCEAN EMERALD|NT25103|QUBE GRAINS|Wheat|20,000|28/08/2025 2:17 PM|29/08/2025 10:30 PM|01/10/2025 6:00 AM|01/10/2025 6:00 AM|05/10/2025 6:00 PM|Nominated|\n",
      "|Newcastle|PAOLO TOPIC|NT25105|ARROW COMMODITIES|Wheat|24,000|08/09/2025 9:49 AM|08/09/2025 12:10 PM|10/10/2025 6:00 AM|10/10/2025 6:00 AM|14/10/2025 6:00 PM|Nominated|\n"
     ]
    }
   ],
   "source": [
    "from pdf_ocr import compress_spatial_text\n",
    "\n",
    "# Pure heuristic compression — no LLM, no API key needed\n",
    "compressed_heuristic = compress_spatial_text(PDF_PATH, refine_headers=False)\n",
    "\n",
    "print(\"=== compress_spatial_text(refine_headers=False) — heuristic only ===\")\n",
    "print(f\"Characters: {len(compressed_heuristic)}\")\n",
    "print(f\"Pipe tables: {'Yes' if '|---|' in compressed_heuristic else 'No'}\\n\")\n",
    "print(compressed_heuristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21f748f8df",
   "metadata": {},
   "source": [
    "For `2857439.pdf`, heuristic compression produces structured output (headings, key-value pairs, tab-separated data) but **no pipe table** — the multi-row record layout doesn't meet the heuristic's strict requirements (2+ spans, 2+ shared column anchors, 3+ qualifying rows).\n",
    "\n",
    "With `refine_headers=True`, the LLM fallback detects the table structure and produces a clean pipe table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e58a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_nc = compress_spatial_text(PDF_PATH, refine_headers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a4e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping Stem Report\n",
      "\n",
      "Date Generated: 15/09/2025\n",
      "\n",
      "|Port|Ship Name|Ref #|Exporter|Commodity|Quantity (tonnes)|Date of Nomination Received|Date of Nomination Accepted|ETA|ETB|ETS|Load Status|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|Newcastle|ADAGIO|NT25084|ARROW COMMODITIES|Wheat|26,914|10/07/2025 11:45 AM|10/07/2025 2:25 PM|06/08/2025 8:06 AM|06/08/2025 8:06 AM|09/08/2025 11:15 PM|Completed|\n",
      "|Newcastle|QC ISABELLA|NT25082|QUBE GRAINS|Wheat|20,333|10/07/2025 7:59 AM|10/07/2025 5:40 PM|17/08/2025 4:06 PM|17/08/2025 4:06 PM|19/08/2025 11:55 AM|Completed|\n",
      "|Newcastle|BRIGHTEN TRADER|NT25085|ARROW COMMODITIES|Wheat|33,000|01/08/2025 7:06 PM|04/08/2025 10:39 AM|20/08/2024 8:00 AM|20/08/2024 8:00 AM|23/08/2025 5:30 PM|Completed|\n",
      "|Newcastle|ARUNA NAZIK|NT25083|CHS BROADBENT|Wheat|55,000|25/07/2025 12:59 PM|25/07/2025 3:09 PM|28/08/2025 6:35 AM|28/08/2025 6:35 AM|31/08/2025 11:12 AM|Completed|\n",
      "|Newcastle|GREAT FORTUNE|NT25086|QUBE GRAINS|Wheat|54,998|04/08/2025 7:26 AM|04/08/2025 9:11 AM|03/09/2025 1:06 PM|03/09/2025 1:06 PM|11/09/2025 5:00 AM|Completed|\n",
      "|Newcastle|ASL ROSE|NT25092|CHS BROADBENT|Wheat|24,000|06/08/2025 5:03 PM|07/08/2025 12:43 PM|12/09/2025 8:36 PM|12/09/2025 8:36 PM|14/09/2025 8:30 PM|Completed|\n",
      "|Newcastle|MAGIC CELESTE|NT25094|ARROW COMMODITIES|Wheat|30,000|16/08/2025 6:09 PM|17/08/2025 11:56 AM|14/09/2025 6:00 AM|14/09/2025 6:00 AM|18/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|ANNE|NT25091|QUBE GRAINS|Wheat|50,000|01/09/2025 10:28 AM|01/09/2025 10:48 AM|22/09/2025 6:00 AM|22/09/2025 6:00 AM|26/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|OCEAN ACE|NT25093|CHS BROADBENT|Wheat|50,000|03/09/2025 10:04 AM|03/09/2025 12:32 PM|25/09/2025 6:00 AM|25/09/2025 6:00 AM|29/09/2025 6:00 PM|Nominated|\n",
      "|Newcastle|OCEAN EMERALD|NT25103|QUBE GRAINS|Wheat|20,000|28/08/2025 2:17 PM|29/08/2025 10:30 PM|01/10/2025 6:00 AM|01/10/2025 6:00 AM|05/10/2025 6:00 PM|Nominated|\n",
      "|Newcastle|PAOLO TOPIC|NT25105|ARROW COMMODITIES|Wheat|24,000|08/09/2025 9:49 AM|08/09/2025 12:10 PM|10/10/2025 6:00 AM|10/10/2025 6:00 AM|14/10/2025 6:00 PM|Nominated|\n"
     ]
    }
   ],
   "source": [
    "print(compressed_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b70bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pdf_to_spatial_text() ===\n",
      "                                                                                                                                                                    DATE ETA OF   TIME ETA OF\n",
      "                                                                                                                                                                                                                                                                   LOADING\n",
      "UNIQUE SLOT                      DATE AT WHICH  TIME AT WHICH   DATE AT WHICH  TIME AT WHICH                                                                        GRAIN         GRAIN\n",
      "                                                                                                           DATE ETA OF   TIME ETA SHIP  DATE ETA OF   TIME ETA OF                              DATE ETD OF   TIME ETD OF                 QUANTITY                  \"COMMENCED\"    DATE LOADING     TIME LOADING\n",
      "REFERENCE     NAME OF SHIP        NOMINATION     NOMINATION      NOMINATION     NOMINATION     PORT                                                                 LOADING       LOADING                                  EXPORTER                  COMMODITY                                                  NOTES\n",
      "                                                                                                           SHIP FROM     FROM           SHIP TO       SHIP TO                                  SHIP          SHIP                        (TONNES)                  OR             COMPLETED          COMPLETED\n",
      "NUMBER                           WAS RECEIVED    WAS RECEIVED   WAS ACCEPTED    WAS ACCEPTED                                                                        COMMENCEM     COMMENCEM\n",
      "                                                                                                                                                                                                                                                                   \"COMPLETED\"\n",
      "                                                                                                                                                                    ENT           ENT\n",
      "BG20250025    AFRICAN DOVE         05/06/2025      4:04:00 PM     05/06/2025      4:53:00 PM   BUNBURY        01/07/2025     9:00:00 AM    10/07/2025   12:00:00 PM    07/07/2025    9:00:00 AM   17/07/2025    7:00:00 PM BUNGE               33020 WHEAT         COMPLETED           16/07/2025     8:29:00 AM\n",
      "BG20250026    THE ETERNAL          25/06/2025      7:31:00 PM     26/06/2025     10:48:00 AM   BUNBURY        10/07/2025     9:00:00 AM    30/07/2025   12:00:00 PM    15/07/2025    9:00:00 AM   25/07/2025    7:00:00 PM BUNGE               62666 BARLEY        COMPLETED           26/07/2025     1:35:00 PM\n",
      "BG20250027    OCEAN BEAUTY         14/07/2025      5:17:00 PM     14/07/2025      5:29:00 PM   BUNBURY        25/07/2025     9:00:00 AM    03/07/2025   12:00:00 PM    31/07/2025    9:00:00 AM   03/08/2025    7:00:00 PM BUNGE               29700 CANOLA        COMPLETED           05/08/2025    11:00:00 PM\n",
      "BG20250028    FELICIA K            08/08/2025     11:49:00 AM     08/08/2025     12:25:00 PM   BUNBURY        25/08/2025     9:00:00 AM    05/09/2025   12:00:00 PM    27/08/2025    9:00:00 AM   30/08/2025    7:00:00 PM BUNGE               26250 CANOLA        COMPLETED           28/08/2025     3:14:00 AM\n",
      "BG20250029    STR VIGOR            04/09/2025     10:04:00 AM     04/09/2025     11:42:00 AM   BUNBURY        01/10/2025     9:00:00 AM    10/10/2025   12:00:00 PM    02/10/2025    9:00:00 AM   12/10/2025    7:00:00 PM BOORTMALT           12500 MALT                                                       STR Vigo replaces Ever Brilliant\n",
      "BG20250030    BULK MANARA          03/09/2025     12:24:00 PM     03/09/2025      1:00:00 PM   BUNBURY        20/09/2025     9:00:00 AM    30/09/2025   12:00:00 PM    20/09/2025    9:00:00 AM   30/09/2025    7:00:00 PM BUNGE               50000 WHEAT         LOADING\n",
      "BG20250031    NORDSCHELDE          22/09/2025      3:49:00 PM     23/09/2025     11:00:00 AM   BUNBURY        05/10/2025     9:00:00 AM    20/10/2025   12:00:00 PM    10/10/2025    9:00:00 AM   15/10/2025    7:00:00 PM BUNGE               30000 CANOLA\n",
      "                                                                                                                                                        Last Updated :  AB 25/09/2025\n"
     ]
    }
   ],
   "source": [
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "spatial = pdf_to_spatial_text(BUNGE_PATH)\n",
    "print(\"=== pdf_to_spatial_text() ===\")\n",
    "print(spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67260450796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"COMPLETED\"\n",
      "\n",
      "|UNIQUE SLOT REFERENCE NUMBER|NAME OF SHIP|DATE AT WHICH NOMINATION WAS RECEIVED|TIME AT WHICH NOMINATION WAS RECEIVED|DATE AT WHICH NOMINATION WAS ACCEPTED|TIME AT WHICH NOMINATION WAS ACCEPTED|PORT|DATE ETA OF SHIP FROM|TIME ETA SHIP FROM|DATE ETA OF SHIP TO|TIME ETA OF SHIP TO|DATE ETA OF GRAIN LOADING COMMENCEM ENT|TIME ETA OF GRAIN LOADING COMMENCEM|DATE ETD OF SHIP|TIME ETD OF SHIP|EXPORTER|QUANTITY (TONNES)|COMMODITY|LOADING \"COMMENCED\" OR \"COMPLETED\"|DATE LOADING COMPLETED|TIME LOADING COMPLETED|NOTES|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|BG20250025|AFRICAN DOVE|05/06/2025|4:04:00 PM|05/06/2025|4:53:00 PM|BUNBURY|01/07/2025|9:00:00 AM|10/07/2025|12:00:00 PM|07/07/2025|9:00:00 AM|17/07/2025|7:00:00 PM|BUNGE|33020|WHEAT|COMPLETED|16/07/2025|8:29:00 AM||\n",
      "|BG20250026|THE ETERNAL|25/06/2025|7:31:00 PM|26/06/2025|10:48:00 AM|BUNBURY|10/07/2025|9:00:00 AM|30/07/2025|12:00:00 PM|15/07/2025|9:00:00 AM|25/07/2025|7:00:00 PM|BUNGE|62666|BARLEY|COMPLETED|26/07/2025|1:35:00 PM||\n",
      "|BG20250027|OCEAN BEAUTY|14/07/2025|5:17:00 PM|14/07/2025|5:29:00 PM|BUNBURY|25/07/2025|9:00:00 AM|03/07/2025|12:00:00 PM|31/07/2025|9:00:00 AM|03/08/2025|7:00:00 PM|BUNGE|29700|CANOLA|COMPLETED|05/08/2025|11:00:00 PM||\n",
      "|BG20250028|FELICIA K|08/08/2025|11:49:00 AM|08/08/2025|12:25:00 PM|BUNBURY|25/08/2025|9:00:00 AM|05/09/2025|12:00:00 PM|27/08/2025|9:00:00 AM|30/08/2025|7:00:00 PM|BUNGE|26250|CANOLA|COMPLETED|28/08/2025|3:14:00 AM||\n",
      "|BG20250029|STR VIGOR|04/09/2025|10:04:00 AM|04/09/2025|11:42:00 AM|BUNBURY|01/10/2025|9:00:00 AM|10/10/2025|12:00:00 PM|02/10/2025|9:00:00 AM|12/10/2025|7:00:00 PM|BOORTMALT|12500|MALT||||STR Vigo replaces Ever Brilliant|\n",
      "|BG20250030|BULK MANARA|03/09/2025|12:24:00 PM|03/09/2025|1:00:00 PM|BUNBURY|20/09/2025|9:00:00 AM|30/09/2025|12:00:00 PM|20/09/2025|9:00:00 AM|30/09/2025|7:00:00 PM|BUNGE|50000|WHEAT|LOADING||||\n",
      "|BG20250031|NORDSCHELDE|22/09/2025|3:49:00 PM|23/09/2025|11:00:00 AM|BUNBURY|05/10/2025|9:00:00 AM|20/10/2025|12:00:00 PM|10/10/2025|9:00:00 AM|15/10/2025|7:00:00 PM|BUNGE|30000|CANOLA|||||\n",
      "|||||||||||Last Updated|: AB 25/09/2025|||||||||||\n"
     ]
    }
   ],
   "source": [
    "# Bunge: the merged-span PDF — now with clean pipe tables\n",
    "compressed_bunge = compress_spatial_text(BUNGE_PATH, refine_headers=True)\n",
    "print(compressed_bunge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf233a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_cbh = compress_spatial_text(CBH_PATH, refine_headers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a17855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compressed_cbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b393e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_queensland = compress_spatial_text(QUEENSLAND_PATH, refine_headers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4309b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compressed_queensland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4a5e78b4f",
   "metadata": {},
   "source": [
    "Every cell contains exactly one logical value. The span splitting fixed what PyMuPDF fused:\n",
    "\n",
    "| Before | After |\n",
    "|---|---|\n",
    "| `\\|7:00:00 PM BUNGE\\|` | `\\|7:00:00 PM\\|BUNGE\\|` |\n",
    "| `\\|33020 WHEAT\\|` | `\\|33020\\|WHEAT\\|` |\n",
    "\n",
    "This structural integrity is the foundation for reliable downstream processing. Each pipe table row has a consistent cell count matching the header — making it a **natural chunking key** for parallel interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60643dc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-assisted compression — refines headers + catches missed tables\n",
    "compressed_llm = compress_spatial_text(BUNGE_PATH, refine_headers=True)\n",
    "\n",
    "print(\"=== compress_spatial_text(refine_headers=True) — with LLM ===\")\n",
    "print(f\"Characters: {len(compressed_llm)}\")\n",
    "print(f\"Pipe tables: {'Yes' if '|---|' in compressed_llm else 'No'}\\n\")\n",
    "print(compressed_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compressed_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26119f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_to_spatial_text(BUNGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ee1435690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf_ocr import pdf_to_spatial_text, compress_spatial_text\n",
    "\n",
    "print(f\"{'File':<55} {'Spatial':>8} {'Compress':>9} {'Reduc.':>7} {'Pipes':>6}\")\n",
    "print(\"-\" * 89)\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    s = pdf_to_spatial_text(path)\n",
    "    c = compress_spatial_text(path, refine_headers=False)\n",
    "    reduction = (1 - len(c) / len(s)) * 100 if len(s) > 0 else 0\n",
    "    pipes = c.count(\"|---|\")\n",
    "    print(f\"{fname:<55} {len(s):>8} {len(c):>9} {reduction:>6.0f}% {pipes:>6}\")\n",
    "\n",
    "print()\n",
    "print(\"'Pipes' counts pipe-table separator rows (|---|).\")\n",
    "print(\"Files with 0 pipes have tables that heuristics missed —\")\n",
    "print(\"refine_headers=True (the default) catches these via an LLM fallback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovuvmoji2f",
   "metadata": {},
   "source": [
    "## Table Interpretation\n",
    "\n",
    "`interpret_table()` takes compressed text and a **canonical schema** describing the columns your application expects, then uses an LLM pipeline to extract structured records.\n",
    "\n",
    "The schema maps inconsistent PDF column names (e.g. \"Ship Name\", \"Vessel\", \"Vessel Name\") to stable canonical names via aliases. Two modes are available:\n",
    "\n",
    "- **2-step** (`interpret_table`) — parse table structure first, then map to schema. Step 2 is **batched**: each page's parsed rows are split into chunks (default 20 rows) so the LLM produces complete output without truncation. All batches across all pages run concurrently.\n",
    "- **Single-shot** (`interpret_table_single_shot`) — one LLM call per page. Faster for simple flat tables, but cannot batch and may truncate on dense pages (50+ rows).\n",
    "\n",
    "Both modes **auto-split** multi-page input (pages joined by `\\f`) and process all pages **concurrently** via `asyncio.gather()`. The return value is a `dict[int, MappedTable]` keyed by 1-indexed page number — each page gets its own complete result (records, unmapped columns, mapping notes, metadata). Records contain only canonical schema fields.\n",
    "\n",
    "Use `to_records(result)` to flatten all pages into a single `list[dict]`, or `to_records_by_page(result)` for `{page: [dicts]}`.\n",
    "\n",
    "### Vision-based schema inference (optional)\n",
    "\n",
    "Some PDFs have dense tables with stacked/multi-line headers where text extraction produces **garbled or concatenated column names** (e.g. `\"7:00:00 PM BUNGE\"` or `\"33020 WHEAT\"` as single text runs). For these cases, pass `pdf_path=` to `interpret_table()` to enable a vision pre-step:\n",
    "\n",
    "```\n",
    "Step 0 (vision):  page image + compressed text → InferredTableSchema\n",
    "Step 1 (guided):  compressed text + InferredTableSchema → ParsedTable\n",
    "Step 2 (unchanged): ParsedTable → MappedTable\n",
    "```\n",
    "\n",
    "The vision step renders each PDF page as an image and uses a vision-capable LLM to read the correct column headers from the visual layout, then step 1 uses that schema to correctly split compound values. When `pdf_path` is omitted, the pipeline behaves exactly as before (no vision overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pvubmndte5j",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the canonical schema as a plain dict (e.g. loaded from a JSON file).\n",
    "# CanonicalSchema.from_dict() converts it into the typed dataclass.\n",
    "#\n",
    "# Note: \"port\" has no aliases — it will be inferred from context (section headers,\n",
    "# document title, or repeated contextual values) rather than matched to a column name.\n",
    "schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"Name of Ship\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"Unique Slot Reference Number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"Exporter\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"Quantity(tonnes)\"], \"format\": \"#,###\"},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"Date ETA of Ship To\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Load Status\"], \"format\": \"titlecase\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "schema = CanonicalSchema.from_dict(schema_dict)\n",
    "\n",
    "print(f\"Schema: {schema.description}\")\n",
    "print(f\"Columns ({len(schema.columns)}):\")\n",
    "for col in schema.columns:\n",
    "    print(f\"  {col.name:20s}  {col.type:6s}  format={col.format or 'None':20s}  aliases={col.aliases}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naakfdlktr",
   "metadata": {},
   "source": [
    "## Multi-page auto-split with batching\n",
    "\n",
    "`compress_spatial_text()` joins pages with `\\f` (form-feed). When `interpret_table()` receives multi-page input, it splits on `\\f` and processes all pages **concurrently**.\n",
    "\n",
    "Step 2 (schema mapping) is **batched** — each page's parsed rows are split into chunks of `batch_size` rows (default 20) before calling the LLM. This prevents truncation on dense pages with many data rows. All batches across all pages run concurrently via `asyncio.gather()`.\n",
    "\n",
    "The result is a `dict[int, MappedTable]` keyed by 1-indexed page number. Each page has its own `records`, `unmapped_columns`, `mapping_notes`, and `metadata`. Use `to_records()` to flatten or `to_records_by_page()` for page-grouped dicts.\n",
    "\n",
    "Below we run the full pipeline on `shipping-stem-2025-11-13.pdf` (3 pages, 180+ records) — no manual splitting needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txfb2oacg6",
   "metadata": {},
   "source": [
    "## Vision-based interpretation (garbled-header PDFs)\n",
    "\n",
    "The Bunge loading statement has dense stacked headers where text extraction produces concatenated spans. Passing `pdf_path=` enables the vision pipeline: each page is rendered as an image, a vision LLM infers the correct column structure, and the guided parser uses that schema to split compound values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63zvg58zso",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from pdf_ocr import (\n",
    "    # Core functions\n",
    "    compress_spatial_text,\n",
    "    pdf_to_spatial_text,\n",
    "    # Table interpretation\n",
    "    interpret_table,\n",
    "    interpret_table_single_shot,\n",
    "    CanonicalSchema,\n",
    "    ColumnDef,\n",
    "    to_records,\n",
    "    to_records_by_page,\n",
    "    # PDF filtering\n",
    "    filter_pdf_by_table_titles,\n",
    "    extract_table_titles,\n",
    "    FilterMatch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on a garbled-header PDF.\n",
    "# The only difference from normal usage is pdf_path= which enables step 0 (vision).\n",
    "\n",
    "newcastle_pdf = \"inputs/2857439.pdf\"\n",
    "compressed_bunge = compress_spatial_text(newcastle_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_bunge)}\")\n",
    "print(compressed_bunge[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Newcastle loading statements\n",
    "newcastle_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"port\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"ship name\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"unique slot reference number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"exporter\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"commodity\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"quantity(tonnes)\"], \"format\": \"#,###\"},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"eta\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"load status\"], \"format\": \"titlecase\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "newcastle_schema = CanonicalSchema.from_dict(newcastle_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_newcastle = interpret_table(\n",
    "    compressed_bunge,\n",
    "    newcastle_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=newcastle_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_newcastle = to_records(result_newcastle)\n",
    "print(f\"Records extracted (vision): {len(records_newcastle)}\")\n",
    "for page, mt in sorted(result_newcastle.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_newcastle[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_newcastle.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngyzjdom4tm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the Bunge PDF (garbled stacked headers).\n",
    "\n",
    "bunge_pdf = \"inputs/Bunge_loadingstatement_2025-09-25.pdf\"\n",
    "compressed_bunge = compress_spatial_text(bunge_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_bunge)}\")\n",
    "print(compressed_bunge[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Bunge loading statements\n",
    "bunge_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"Name of Ship\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"Unique Slot Reference Number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"Exporter\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"Quantity(tonnes)\"], \"format\": \"#,###\"},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"Date ETA of Ship To\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Load Status\"], \"format\": \"titlecase\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "bunge_schema = CanonicalSchema.from_dict(bunge_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_bunge = interpret_table(\n",
    "    compressed_bunge,\n",
    "    bunge_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=bunge_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_bunge = to_records(result_bunge)\n",
    "print(f\"Records extracted (vision): {len(records_bunge)}\")\n",
    "for page, mt in sorted(result_bunge.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_bunge[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_bunge.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the CBH PDF.\n",
    "\n",
    "cbh_pdf = \"inputs/CBH Shipping Stem 26092025.pdf\"\n",
    "compressed_cbh = compress_spatial_text(cbh_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_cbh)}\")\n",
    "print(compressed_cbh[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for CBH loading statements\n",
    "cbh_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name, not captured in the original schema but present inside the header\", \"aliases\": [], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"vessel name\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"vna #\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"client\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"volume\"], \"format\": \"#,###\"},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"ETA\"], \"format\": \"YYYY-MM-DD\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Loading Status\"], \"format\": \"titlecase\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "cbh_schema = CanonicalSchema.from_dict(cbh_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_cbh = interpret_table(\n",
    "    compressed_cbh,\n",
    "    cbh_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=cbh_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_cbh = to_records(result_cbh)\n",
    "print(f\"Records extracted (vision): {len(records_cbh)}\")\n",
    "for page, mt in sorted(result_cbh.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_cbh[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_cbh.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fahgdot7id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the Queensland PDF.\n",
    "\n",
    "queensland_pdf = \"inputs/document (1).pdf\"\n",
    "compressed_queensland = compress_spatial_text(queensland_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_queensland)}\")\n",
    "print(compressed_queensland[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Queensland loading statements\n",
    "queensland_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"port\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"name of ship\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"unique slot reference number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"exporter\"], \"format\": \"uppercase\"},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"commodity\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"quantity(tonnes)\"], \"format\": \"#,###\"},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"date of eta of ship to\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"loading ' commenced' or ' completed'\"], \"format\": \"titlecase\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "queensland_schema = CanonicalSchema.from_dict(queensland_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_queensland = interpret_table(\n",
    "    compressed_queensland,\n",
    "    queensland_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=queensland_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_queensland = to_records(result_queensland)\n",
    "print(f\"Records extracted (vision): {len(records_queensland)}\")\n",
    "for page, mt in sorted(result_queensland.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_queensland[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_queensland.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "acea_pdf_filtered = filter_pdf_by_table_titles(\n",
    "    acea_pdf,\n",
    "    [\"new car registrations by market and power source, monthly\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the ACEA car registrations PDF.\n",
    "\n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "acea_pdf_filtered, matches = filter_pdf_by_table_titles(\n",
    "    acea_pdf,\n",
    "    [\"new car registrations by market and power source, monthly\"],\n",
    ")\n",
    "compressed_acea = compress_spatial_text(acea_pdf_filtered)\n",
    "print(f\"Compressed chars: {len(compressed_acea)}\")\n",
    "print(compressed_acea[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema for ACEA car registrations\n",
    "# Note: car_motorization aliases match header parts to trigger unpivot\n",
    "# Note: date has empty aliases - the LLM infers year values from headers\n",
    "acea_schema_dict = {\n",
    "    \"description\": \"ACEA new car registrations by market and power source, monthly\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"country\", \"type\": \"string\", \"description\": \"Country of registration\", \"aliases\": [], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"car_motorization\", \"type\": \"string\", \"description\": \"Car motorization type\", \"aliases\": [\"battery electric\", \"plug-in hybrid\", \"hybrid electric\", \"others\", \"petrol\", \"diesel\"], \"format\": \"titlecase\"},\n",
    "        {\"name\": \"new_car_registration\", \"type\": \"int\", \"description\": \"Number of new car registrations\", \"aliases\": [], \"format\": \"#,###\"},\n",
    "        {\"name\": \"date\", \"type\": \"string\", \"description\": \"Registration period (year from column header, month from document context)\", \"aliases\": [], \"format\": \"YYYY-MM\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "acea_schema = CanonicalSchema.from_dict(acea_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_acea = interpret_table(\n",
    "    compressed_acea,\n",
    "    acea_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=acea_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_acea = to_records(result_acea)\n",
    "print(f\"Records extracted (vision): {len(records_acea)}\")\n",
    "for page, mt in sorted(result_acea.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_acea[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_acea.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "{page: mt.model_dump() for page, mt in result_acea.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1obb135qme3",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "After interpreting tables, export results to CSV, TSV, Parquet, pandas or polars DataFrames using the `serialize` module. All functions validate records against the schema and coerce OCR artifacts (e.g., `\"1,234\"` → `1234`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s53jfqaky0h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize interpretation results to various formats\n",
    "from pdf_ocr import to_csv, to_tsv, to_pandas\n",
    "\n",
    "# Export to CSV string\n",
    "csv_str = to_csv(result_acea, acea_schema)\n",
    "print(\"=== CSV (first 500 chars) ===\")\n",
    "print(csv_str[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Export to CSV file with page column\n",
    "to_csv(result_acea, acea_schema, path=\"/tmp/acea_output.csv\", include_page=True)\n",
    "print(\"\\nWrote /tmp/acea_output.csv\")\n",
    "\n",
    "# Export to pandas DataFrame with proper nullable dtypes\n",
    "df = to_pandas(result_acea, acea_schema, include_page=True)\n",
    "print(\"\\n=== pandas DataFrame ===\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nDtypes:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr.interpret import analyze_and_parse                                                                                                                                                                              \n",
    "from pdf_ocr import compress_spatial_text, filter_pdf_by_table_titles                                                                                                                                                        \n",
    "                                                                                                                                                                                                                            \n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"                                                                                                                                                        \n",
    "filtered, _ = filter_pdf_by_table_titles(acea_pdf, pages=[2])                                                                                                                                                                \n",
    "compressed = compress_spatial_text(filtered)                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                                            \n",
    "# Check what Step 1 outputs                                                                                                                                                                                                  \n",
    "parsed = analyze_and_parse(compressed, model=\"openai/gpt-4o\")                                                                                                                                                                \n",
    "print(f\"table_type: {parsed.table_type}\")                                                                                                                                                                                    \n",
    "print(f\"headers: {parsed.headers}\")                                                                                                                                                                                          \n",
    "print(f\"notes: {parsed.notes}\")                                                                                                                                                                                              \n",
    "print(f\"data_rows count: {len(parsed.data_rows)}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
