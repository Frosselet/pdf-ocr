{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "path = \"/Volumes/WD Green/dev/git/pdf_ocr/pdf-ocr/inputs/2857439.pdf\"\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(path)\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def reconstruct_markdown(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_md = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Récupère les blocs de texte triés (haut-gauche vers bas-droite)\n",
    "        # 'sort=True' gère souvent déjà l'ordre de lecture naturel\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        \n",
    "        for b in blocks:\n",
    "            x0, y0, x1, y1, text, block_no, block_type = b\n",
    "            \n",
    "            if block_type == 0:  # C'est un bloc de texte\n",
    "                clean_text = text.strip()\n",
    "                if not clean_text: continue\n",
    "                \n",
    "                # Exemple de logique de style simple :\n",
    "                # Si le bloc est très à gauche, c'est peut-être un titre ou une puce\n",
    "                if x0 < 100 and len(clean_text) < 50:\n",
    "                    full_md.append(f\"### {clean_text}\\n\")\n",
    "                else:\n",
    "                    full_md.append(f\"{clean_text}\\n\")\n",
    "        \n",
    "        full_md.append(\"\\n---\\n\") # Séparateur de page\n",
    "    \n",
    "    return \"\\n\".join(full_md)\n",
    "\n",
    "md_output = reconstruct_markdown(path)\n",
    "print(md_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moi32xu0ihr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "output = pdf_to_spatial_text(\"inputs/2857439.pdf\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i3qj1yp7qc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    text = pdf_to_spatial_text(path)\n",
    "    lines = text.split(\"\\n\")\n",
    "    max_width = max((len(l) for l in lines), default=0)\n",
    "    print(f\"{fname:50s}  lines={len(lines):4d}  max_width={max_width:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ja98ihg0ty",
   "metadata": {},
   "source": [
    "## Compressed Spatial Text\n",
    "\n",
    "`compress_spatial_text()` produces a token-efficient representation by classifying page regions (tables, headings, text blocks, key-value pairs) and rendering them as markdown tables and flowing text instead of whitespace-heavy grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "idb5df8kpmf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import compress_spatial_text, pdf_to_spatial_text\n",
    "\n",
    "pdf = \"inputs/2857439.pdf\"\n",
    "\n",
    "spatial = pdf_to_spatial_text(pdf)\n",
    "compressed = compress_spatial_text(pdf)\n",
    "\n",
    "print(spatial)\n",
    "print(compress,ed)\n",
    "print(f\"\\n--- Compression: {len(spatial)} chars → {len(compressed)} chars ({(1 - len(compressed)/len(spatial))*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fl46cmb0yjk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"{'File':<50} {'Spatial':>8} {'Compressed':>10} {'Reduction':>10}\")\n",
    "print(\"-\" * 82)\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    s = pdf_to_spatial_text(path)\n",
    "    c = compress_spatial_text(path)\n",
    "    reduction = (1 - len(c) / len(s)) * 100 if len(s) > 0 else 0\n",
    "    print(f\"{fname:<50} {len(s):>8} {len(c):>10} {reduction:>9.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovuvmoji2f",
   "metadata": {},
   "source": [
    "## Table Interpretation\n",
    "\n",
    "`interpret_table()` takes compressed text and a **canonical schema** describing the columns your application expects, then uses an LLM pipeline to extract structured records.\n",
    "\n",
    "The schema maps inconsistent PDF column names (e.g. \"Ship Name\", \"Vessel\", \"Vessel Name\") to stable canonical names via aliases. Two modes are available:\n",
    "\n",
    "- **2-step** (`interpret_table`) — parse table structure first, then map to schema. Step 2 is **batched**: each page's parsed rows are split into chunks (default 20 rows) so the LLM produces complete output without truncation. All batches across all pages run concurrently.\n",
    "- **Single-shot** (`interpret_table_single_shot`) — one LLM call per page. Faster for simple flat tables, but cannot batch and may truncate on dense pages (50+ rows).\n",
    "\n",
    "Both modes **auto-split** multi-page input (pages joined by `\\f`) and process all pages **concurrently** via `asyncio.gather()`. The return value is a `dict[int, MappedTable]` keyed by 1-indexed page number — each page gets its own complete result (records, unmapped columns, mapping notes, metadata). Records contain only canonical schema fields.\n",
    "\n",
    "Use `to_records(result)` to flatten all pages into a single `list[dict]`, or `to_records_by_page(result)` for `{page: [dicts]}`.\n",
    "\n",
    "### Vision-based schema inference (optional)\n",
    "\n",
    "Some PDFs have dense tables with stacked/multi-line headers where text extraction produces **garbled or concatenated column names** (e.g. `\"7:00:00 PM BUNGE\"` or `\"33020 WHEAT\"` as single text runs). For these cases, pass `pdf_path=` to `interpret_table()` to enable a vision pre-step:\n",
    "\n",
    "```\n",
    "Step 0 (vision):  page image + compressed text → InferredTableSchema\n",
    "Step 1 (guided):  compressed text + InferredTableSchema → ParsedTable\n",
    "Step 2 (unchanged): ParsedTable → MappedTable\n",
    "```\n",
    "\n",
    "The vision step renders each PDF page as an image and uses a vision-capable LLM to read the correct column headers from the visual layout, then step 1 uses that schema to correctly split compound values. When `pdf_path` is omitted, the pipeline behaves exactly as before (no vision overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pvubmndte5j",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the canonical schema as a plain dict (e.g. loaded from a JSON file).\n",
    "# CanonicalSchema.from_dict() converts it into the typed dataclass.\n",
    "#\n",
    "# Note: \"port\" has no aliases — it will be inferred from context (section headers,\n",
    "# document title, or repeated contextual values) rather than matched to a column name.\n",
    "schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\",            \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"]},\n",
    "        {\"name\": \"vessel_name\",     \"type\": \"string\", \"description\": \"Name of the vessel\",             \"aliases\": [\"Name of Ship\"]},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\",               \"aliases\": [\"Unique Slot Reference Number\"]},\n",
    "        {\"name\": \"shipper\",        \"type\": \"string\", \"description\": \"Exporting company\",              \"aliases\": [\"Exporter\"]},\n",
    "        {\"name\": \"commodity\",       \"type\": \"string\", \"description\": \"Type of commodity\",              \"aliases\": [\"Commodity\"]},\n",
    "        {\"name\": \"tons\", \"type\": \"int\",    \"description\": \"Quantity in metric tonnes\",      \"aliases\": [\"Quantity(tonnes)\"]},\n",
    "        {\"name\": \"eta\",             \"type\": \"string\", \"description\": \"Estimated time of arrival\",      \"aliases\": [\"Date ETA of Ship To\"]},\n",
    "        {\"name\": \"status\",          \"type\": \"string\", \"description\": \"Loading status\",                 \"aliases\": [\"Status\", \"Load Status\"]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "schema = CanonicalSchema.from_dict(schema_dict)\n",
    "\n",
    "print(f\"Schema: {schema.description}\")\n",
    "print(f\"Columns ({len(schema.columns)}):\")\n",
    "for col in schema.columns:\n",
    "    print(f\"  {col.name:20s}  {col.type:6s}  aliases={col.aliases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naakfdlktr",
   "metadata": {},
   "source": [
    "## Multi-page auto-split with batching\n",
    "\n",
    "`compress_spatial_text()` joins pages with `\\f` (form-feed). When `interpret_table()` receives multi-page input, it splits on `\\f` and processes all pages **concurrently**.\n",
    "\n",
    "Step 2 (schema mapping) is **batched** — each page's parsed rows are split into chunks of `batch_size` rows (default 20) before calling the LLM. This prevents truncation on dense pages with many data rows. All batches across all pages run concurrently via `asyncio.gather()`.\n",
    "\n",
    "The result is a `dict[int, MappedTable]` keyed by 1-indexed page number. Each page has its own `records`, `unmapped_columns`, `mapping_notes`, and `metadata`. Use `to_records()` to flatten or `to_records_by_page()` for page-grouped dicts.\n",
    "\n",
    "Below we run the full pipeline on `shipping-stem-2025-11-13.pdf` (3 pages, 180+ records) — no manual splitting needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txfb2oacg6",
   "metadata": {},
   "source": [
    "## Vision-based interpretation (garbled-header PDFs)\n",
    "\n",
    "The Bunge loading statement has dense stacked headers where text extraction produces concatenated spans. Passing `pdf_path=` enables the vision pipeline: each page is rendered as an image, a vision LLM infers the correct column structure, and the guided parser uses that schema to split compound values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63zvg58zso",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from pdf_ocr import (\n",
    "    compress_spatial_text,\n",
    "    interpret_table,\n",
    "    interpret_table_single_shot,\n",
    "    CanonicalSchema,\n",
    "    ColumnDef,\n",
    "    to_records,\n",
    "    to_records_by_page,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on a garbled-header PDF.\n",
    "# The only difference from normal usage is pdf_path= which enables step 0 (vision).\n",
    "\n",
    "newcastle_pdf = \"inputs/2857439.pdf\"\n",
    "compressed_bunge = compress_spatial_text(newcastle_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_bunge)}\")\n",
    "print(compressed_bunge[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Newcastle loading statements\n",
    "newcastle_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"port\"]},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"ship name\"]},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"unique slot reference number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"exporter\"]},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"commodity\"]},\n",
    "        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"quantity(tonnes)\"]},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"eta\"]},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"load status\"]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "newcastle_schema = CanonicalSchema.from_dict(newcastle_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_newcastle = interpret_table(\n",
    "    compressed_bunge,\n",
    "    newcastle_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=newcastle_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_newcastle = to_records(result_newcastle)\n",
    "print(f\"Records extracted (vision): {len(records_newcastle)}\")\n",
    "for page, mt in sorted(result_newcastle.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_newcastle[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_newcastle.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngyzjdom4tm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the Bunge PDF (garbled stacked headers).\n",
    "\n",
    "bunge_pdf = \"inputs/Bunge_loadingstatement_2025-09-25.pdf\"\n",
    "compressed_bunge = compress_spatial_text(bunge_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_bunge)}\")\n",
    "print(compressed_bunge[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Bunge loading statements\n",
    "bunge_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"]},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"Name of Ship\"]},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"Unique Slot Reference Number\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"Exporter\"]},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"]},\n",
    "        {\"name\": \"tons\", \"type\": \"int\",    \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"Quantity(tonnes)\"]},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"Date ETA of Ship To\"]},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Load Status\"]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "bunge_schema = CanonicalSchema.from_dict(bunge_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_bunge = interpret_table(\n",
    "    compressed_bunge,\n",
    "    bunge_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=bunge_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_bunge = to_records(result_bunge)\n",
    "print(f\"Records extracted (vision): {len(records_bunge)}\")\n",
    "for page, mt in sorted(result_bunge.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_bunge[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_bunge.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-enabled pipeline on the Bunge PDF (garbled stacked headers).\n",
    "\n",
    "cbh_pdf = \"inputs/CBH Shipping Stem 26092025.pdf\"\n",
    "compressed_cbh = compress_spatial_text(cbh_pdf)\n",
    "print(f\"Compressed chars: {len(compressed_cbh)}\")\n",
    "print(compressed_cbh[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Define a schema suitable for Bunge loading statements\n",
    "cbh_schema_dict = {\n",
    "    \"description\": \"Shipping stem vessel loading records\",\n",
    "    \"columns\": [\n",
    "        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name, not captured in the original schema but present inside the header\", \"aliases\": []},\n",
    "        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"vessel name\"]},\n",
    "        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"vna #\"]},\n",
    "        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"client\"]},\n",
    "        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"]},\n",
    "        {\"name\": \"tons\", \"type\": \"int\",    \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"volume\"]},\n",
    "        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"ETA\"]},\n",
    "        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Loading Status\"]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "cbh_schema = CanonicalSchema.from_dict(cbh_schema_dict)\n",
    "\n",
    "# Run WITH vision (pdf_path= enables step 0)\n",
    "result_cbh = interpret_table(\n",
    "    compressed_cbh,\n",
    "    cbh_schema,\n",
    "    model=\"openai/gpt-4o\",\n",
    "    pdf_path=cbh_pdf,\n",
    ")\n",
    "\n",
    "# Result is dict[int, MappedTable] — one entry per page\n",
    "records_cbh = to_records(result_cbh)\n",
    "print(f\"Records extracted (vision): {len(records_cbh)}\")\n",
    "for page, mt in sorted(result_cbh.items()):\n",
    "    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\n",
    "print(f\"\\n--- First 5 records ---\\n\")\n",
    "for i, rec in enumerate(records_cbh[:5], 1):\n",
    "    print(f\"[{i}] {rec}\")\n",
    "\n",
    "# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n",
    "{page: mt.model_dump() for page, mt in result_cbh.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
