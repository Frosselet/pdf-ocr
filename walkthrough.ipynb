{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "path = \"/Volumes/WD Green/dev/git/pdf_ocr/pdf-ocr/inputs/2857439.pdf\"\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(path)\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def reconstruct_markdown(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_md = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Récupère les blocs de texte triés (haut-gauche vers bas-droite)\n",
    "        # 'sort=True' gère souvent déjà l'ordre de lecture naturel\n",
    "        blocks = page.get_text(\"blocks\", sort=True)\n",
    "        \n",
    "        for b in blocks:\n",
    "            x0, y0, x1, y1, text, block_no, block_type = b\n",
    "            \n",
    "            if block_type == 0:  # C'est un bloc de texte\n",
    "                clean_text = text.strip()\n",
    "                if not clean_text: continue\n",
    "                \n",
    "                # Exemple de logique de style simple :\n",
    "                # Si le bloc est très à gauche, c'est peut-être un titre ou une puce\n",
    "                if x0 < 100 and len(clean_text) < 50:\n",
    "                    full_md.append(f\"### {clean_text}\\n\")\n",
    "                else:\n",
    "                    full_md.append(f\"{clean_text}\\n\")\n",
    "        \n",
    "        full_md.append(\"\\n---\\n\") # Séparateur de page\n",
    "    \n",
    "    return \"\\n\".join(full_md)\n",
    "\n",
    "md_output = reconstruct_markdown(path)\n",
    "print(md_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moi32xu0ihr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import pdf_to_spatial_text\n",
    "\n",
    "output = pdf_to_spatial_text(\"inputs/2857439.pdf\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i3qj1yp7qc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    text = pdf_to_spatial_text(path)\n",
    "    lines = text.split(\"\\n\")\n",
    "    max_width = max((len(l) for l in lines), default=0)\n",
    "    print(f\"{fname:50s}  lines={len(lines):4d}  max_width={max_width:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ja98ihg0ty",
   "metadata": {},
   "source": [
    "## Compressed Spatial Text\n",
    "\n",
    "`compress_spatial_text()` produces a token-efficient representation by classifying page regions (tables, headings, text blocks, key-value pairs) and rendering them as markdown tables and flowing text instead of whitespace-heavy grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "idb5df8kpmf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import compress_spatial_text, pdf_to_spatial_text\n",
    "\n",
    "pdf = \"inputs/2857439.pdf\"\n",
    "\n",
    "spatial = pdf_to_spatial_text(pdf)\n",
    "compressed = compress_spatial_text(pdf)\n",
    "\n",
    "print(spatial)\n",
    "print(compress,ed)\n",
    "print(f\"\\n--- Compression: {len(spatial)} chars → {len(compressed)} chars ({(1 - len(compressed)/len(spatial))*100:.0f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fl46cmb0yjk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"{'File':<50} {'Spatial':>8} {'Compressed':>10} {'Reduction':>10}\")\n",
    "print(\"-\" * 82)\n",
    "\n",
    "for fname in sorted(os.listdir(\"inputs\")):\n",
    "    if not fname.endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(\"inputs\", fname)\n",
    "    s = pdf_to_spatial_text(path)\n",
    "    c = compress_spatial_text(path)\n",
    "    reduction = (1 - len(c) / len(s)) * 100 if len(s) > 0 else 0\n",
    "    print(f\"{fname:<50} {len(s):>8} {len(c):>10} {reduction:>9.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovuvmoji2f",
   "metadata": {},
   "source": [
    "## Table Interpretation\n",
    "\n",
    "`interpret_table()` takes compressed text and a **canonical schema** describing the columns your application expects, then uses an LLM pipeline to extract structured records.\n",
    "\n",
    "The schema maps inconsistent PDF column names (e.g. \"Ship Name\", \"Vessel\", \"Vessel Name\") to stable canonical names via aliases. Two modes are available:\n",
    "\n",
    "- **2-step** (`interpret_table`) — parse table structure first, then map to schema. Step 2 is **batched**: each page's parsed rows are split into chunks (default 20 rows) so the LLM produces complete output without truncation. All batches across all pages run concurrently.\n",
    "- **Single-shot** (`interpret_table_single_shot`) — one LLM call per page. Faster for simple flat tables, but cannot batch and may truncate on dense pages (50+ rows).\n",
    "\n",
    "Both modes **auto-split** multi-page input (pages joined by `\\f`) and process all pages **concurrently** via `asyncio.gather()`. The return value is a `dict[int, MappedTable]` keyed by 1-indexed page number — each page gets its own complete result (records, unmapped columns, mapping notes, metadata). Records contain only canonical schema fields.\n",
    "\n",
    "Use `to_records(result)` to flatten all pages into a single `list[dict]`, or `to_records_by_page(result)` for `{page: [dicts]}`.\n",
    "\n",
    "### Vision-based schema inference (optional)\n",
    "\n",
    "Some PDFs have dense tables with stacked/multi-line headers where text extraction produces **garbled or concatenated column names** (e.g. `\"7:00:00 PM BUNGE\"` or `\"33020 WHEAT\"` as single text runs). For these cases, pass `pdf_path=` to `interpret_table()` to enable a vision pre-step:\n",
    "\n",
    "```\n",
    "Step 0 (vision):  page image + compressed text → InferredTableSchema\n",
    "Step 1 (guided):  compressed text + InferredTableSchema → ParsedTable\n",
    "Step 2 (unchanged): ParsedTable → MappedTable\n",
    "```\n",
    "\n",
    "The vision step renders each PDF page as an image and uses a vision-capable LLM to read the correct column headers from the visual layout, then step 1 uses that schema to correctly split compound values. When `pdf_path` is omitted, the pipeline behaves exactly as before (no vision overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pvubmndte5j",
   "metadata": {},
   "outputs": [],
   "source": "\n# Define the canonical schema as a plain dict (e.g. loaded from a JSON file).\n# CanonicalSchema.from_dict() converts it into the typed dataclass.\n#\n# Note: \"port\" has no aliases — it will be inferred from context (section headers,\n# document title, or repeated contextual values) rather than matched to a column name.\nschema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"], \"format\": \"uppercase\"},\n        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"Name of Ship\"], \"format\": \"uppercase\"},\n        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"Unique Slot Reference Number\"]},\n        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"Exporter\"], \"format\": \"uppercase\"},\n        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"Quantity(tonnes)\"], \"format\": \"#,###\"},\n        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"Date ETA of Ship To\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Load Status\"], \"format\": \"titlecase\"},\n    ],\n}\n\nschema = CanonicalSchema.from_dict(schema_dict)\n\nprint(f\"Schema: {schema.description}\")\nprint(f\"Columns ({len(schema.columns)}):\")\nfor col in schema.columns:\n    print(f\"  {col.name:20s}  {col.type:6s}  format={col.format or 'None':20s}  aliases={col.aliases}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "naakfdlktr",
   "metadata": {},
   "source": [
    "## Multi-page auto-split with batching\n",
    "\n",
    "`compress_spatial_text()` joins pages with `\\f` (form-feed). When `interpret_table()` receives multi-page input, it splits on `\\f` and processes all pages **concurrently**.\n",
    "\n",
    "Step 2 (schema mapping) is **batched** — each page's parsed rows are split into chunks of `batch_size` rows (default 20) before calling the LLM. This prevents truncation on dense pages with many data rows. All batches across all pages run concurrently via `asyncio.gather()`.\n",
    "\n",
    "The result is a `dict[int, MappedTable]` keyed by 1-indexed page number. Each page has its own `records`, `unmapped_columns`, `mapping_notes`, and `metadata`. Use `to_records()` to flatten or `to_records_by_page()` for page-grouped dicts.\n",
    "\n",
    "Below we run the full pipeline on `shipping-stem-2025-11-13.pdf` (3 pages, 180+ records) — no manual splitting needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txfb2oacg6",
   "metadata": {},
   "source": [
    "## Vision-based interpretation (garbled-header PDFs)\n",
    "\n",
    "The Bunge loading statement has dense stacked headers where text extraction produces concatenated spans. Passing `pdf_path=` enables the vision pipeline: each page is rendered as an image, a vision LLM infers the correct column structure, and the guided parser uses that schema to split compound values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63zvg58zso",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from pdf_ocr import (\n",
    "    # Core functions\n",
    "    compress_spatial_text,\n",
    "    pdf_to_spatial_text,\n",
    "    # Table interpretation\n",
    "    interpret_table,\n",
    "    interpret_table_single_shot,\n",
    "    CanonicalSchema,\n",
    "    ColumnDef,\n",
    "    to_records,\n",
    "    to_records_by_page,\n",
    "    # PDF filtering\n",
    "    filter_pdf_by_table_titles,\n",
    "    extract_table_titles,\n",
    "    FilterMatch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184ed26",
   "metadata": {},
   "outputs": [],
   "source": "# Vision-enabled pipeline on a garbled-header PDF.\n# The only difference from normal usage is pdf_path= which enables step 0 (vision).\n\nnewcastle_pdf = \"inputs/2857439.pdf\"\ncompressed_bunge = compress_spatial_text(newcastle_pdf)\nprint(f\"Compressed chars: {len(compressed_bunge)}\")\nprint(compressed_bunge[:500])\nprint(\"...\")\n\n# Define a schema suitable for Newcastle loading statements\nnewcastle_schema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"port\"], \"format\": \"uppercase\"},\n        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"ship name\"], \"format\": \"uppercase\"},\n        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"unique slot reference number\"]},\n        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"exporter\"], \"format\": \"uppercase\"},\n        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"commodity\"], \"format\": \"titlecase\"},\n        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"quantity(tonnes)\"], \"format\": \"#,###\"},\n        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"eta\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"load status\"], \"format\": \"titlecase\"},\n    ],\n}\n\nnewcastle_schema = CanonicalSchema.from_dict(newcastle_schema_dict)\n\n# Run WITH vision (pdf_path= enables step 0)\nresult_newcastle = interpret_table(\n    compressed_bunge,\n    newcastle_schema,\n    model=\"openai/gpt-4o\",\n    pdf_path=newcastle_pdf,\n)\n\n# Result is dict[int, MappedTable] — one entry per page\nrecords_newcastle = to_records(result_newcastle)\nprint(f\"Records extracted (vision): {len(records_newcastle)}\")\nfor page, mt in sorted(result_newcastle.items()):\n    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\nprint(f\"\\n--- First 5 records ---\\n\")\nfor i, rec in enumerate(records_newcastle[:5], 1):\n    print(f\"[{i}] {rec}\")\n\n# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n{page: mt.model_dump() for page, mt in result_newcastle.items()}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngyzjdom4tm",
   "metadata": {},
   "outputs": [],
   "source": "# Vision-enabled pipeline on the Bunge PDF (garbled stacked headers).\n\nbunge_pdf = \"inputs/Bunge_loadingstatement_2025-09-25.pdf\"\ncompressed_bunge = compress_spatial_text(bunge_pdf)\nprint(f\"Compressed chars: {len(compressed_bunge)}\")\nprint(compressed_bunge[:500])\nprint(\"...\")\n\n# Define a schema suitable for Bunge loading statements\nbunge_schema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"Port\"], \"format\": \"uppercase\"},\n        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"Name of Ship\"], \"format\": \"uppercase\"},\n        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"Unique Slot Reference Number\"]},\n        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"Exporter\"], \"format\": \"uppercase\"},\n        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"Quantity(tonnes)\"], \"format\": \"#,###\"},\n        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"Date ETA of Ship To\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Load Status\"], \"format\": \"titlecase\"},\n    ],\n}\n\nbunge_schema = CanonicalSchema.from_dict(bunge_schema_dict)\n\n# Run WITH vision (pdf_path= enables step 0)\nresult_bunge = interpret_table(\n    compressed_bunge,\n    bunge_schema,\n    model=\"openai/gpt-4o\",\n    pdf_path=bunge_pdf,\n)\n\n# Result is dict[int, MappedTable] — one entry per page\nrecords_bunge = to_records(result_bunge)\nprint(f\"Records extracted (vision): {len(records_bunge)}\")\nfor page, mt in sorted(result_bunge.items()):\n    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\nprint(f\"\\n--- First 5 records ---\\n\")\nfor i, rec in enumerate(records_bunge[:5], 1):\n    print(f\"[{i}] {rec}\")\n\n# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n{page: mt.model_dump() for page, mt in result_bunge.items()}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441b92b",
   "metadata": {},
   "outputs": [],
   "source": "# Vision-enabled pipeline on the CBH PDF.\n\ncbh_pdf = \"inputs/CBH Shipping Stem 26092025.pdf\"\ncompressed_cbh = compress_spatial_text(cbh_pdf)\nprint(f\"Compressed chars: {len(compressed_cbh)}\")\nprint(compressed_cbh[:500])\nprint(\"...\")\n\n# Define a schema suitable for CBH loading statements\ncbh_schema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name, not captured in the original schema but present inside the header\", \"aliases\": [], \"format\": \"uppercase\"},\n        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"vessel name\"], \"format\": \"uppercase\"},\n        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"vna #\"]},\n        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"client\"], \"format\": \"uppercase\"},\n        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"Commodity\"], \"format\": \"titlecase\"},\n        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"volume\"], \"format\": \"#,###\"},\n        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"ETA\"], \"format\": \"YYYY-MM-DD\"},\n        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"Loading Status\"], \"format\": \"titlecase\"},\n    ],\n}\n\ncbh_schema = CanonicalSchema.from_dict(cbh_schema_dict)\n\n# Run WITH vision (pdf_path= enables step 0)\nresult_cbh = interpret_table(\n    compressed_cbh,\n    cbh_schema,\n    model=\"openai/gpt-4o\",\n    pdf_path=cbh_pdf,\n)\n\n# Result is dict[int, MappedTable] — one entry per page\nrecords_cbh = to_records(result_cbh)\nprint(f\"Records extracted (vision): {len(records_cbh)}\")\nfor page, mt in sorted(result_cbh.items()):\n    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\nprint(f\"\\n--- First 5 records ---\\n\")\nfor i, rec in enumerate(records_cbh[:5], 1):\n    print(f\"[{i}] {rec}\")\n\n# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n{page: mt.model_dump() for page, mt in result_cbh.items()}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fahgdot7id",
   "metadata": {},
   "outputs": [],
   "source": "# Vision-enabled pipeline on the Queensland PDF.\n\nqueensland_pdf = \"inputs/document (1).pdf\"\ncompressed_queensland = compress_spatial_text(queensland_pdf)\nprint(f\"Compressed chars: {len(compressed_queensland)}\")\nprint(compressed_queensland[:500])\nprint(\"...\")\n\n# Define a schema suitable for Queensland loading statements\nqueensland_schema_dict = {\n    \"description\": \"Shipping stem vessel loading records\",\n    \"columns\": [\n        {\"name\": \"load_port\", \"type\": \"string\", \"description\": \"Loading port name\", \"aliases\": [\"port\"], \"format\": \"uppercase\"},\n        {\"name\": \"vessel_name\", \"type\": \"string\", \"description\": \"Name of the vessel\", \"aliases\": [\"name of ship\"], \"format\": \"uppercase\"},\n        {\"name\": \"unique_shipping_slot_id\", \"type\": \"string\", \"description\": \"Reference number\", \"aliases\": [\"unique slot reference number\"]},\n        {\"name\": \"shipper\", \"type\": \"string\", \"description\": \"Exporting company\", \"aliases\": [\"exporter\"], \"format\": \"uppercase\"},\n        {\"name\": \"commodity\", \"type\": \"string\", \"description\": \"Type of commodity\", \"aliases\": [\"commodity\"], \"format\": \"titlecase\"},\n        {\"name\": \"tons\", \"type\": \"int\", \"description\": \"Quantity in metric tonnes\", \"aliases\": [\"quantity(tonnes)\"], \"format\": \"#,###\"},\n        {\"name\": \"eta\", \"type\": \"string\", \"description\": \"Estimated time of arrival\", \"aliases\": [\"date of eta of ship to\"], \"format\": \"YYYY-MM-DD HH:mm\"},\n        {\"name\": \"status\", \"type\": \"string\", \"description\": \"Loading status\", \"aliases\": [\"Status\", \"loading ' commenced' or ' completed'\"], \"format\": \"titlecase\"},\n    ],\n}\n\nqueensland_schema = CanonicalSchema.from_dict(queensland_schema_dict)\n\n# Run WITH vision (pdf_path= enables step 0)\nresult_queensland = interpret_table(\n    compressed_queensland,\n    queensland_schema,\n    model=\"openai/gpt-4o\",\n    pdf_path=queensland_pdf,\n)\n\n# Result is dict[int, MappedTable] — one entry per page\nrecords_queensland = to_records(result_queensland)\nprint(f\"Records extracted (vision): {len(records_queensland)}\")\nfor page, mt in sorted(result_queensland.items()):\n    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\nprint(f\"\\n--- First 5 records ---\\n\")\nfor i, rec in enumerate(records_queensland[:5], 1):\n    print(f\"[{i}] {rec}\")\n\n# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n{page: mt.model_dump() for page, mt in result_queensland.items()}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\n",
    "acea_pdf_filtered = filter_pdf_by_table_titles(\n",
    "    acea_pdf,\n",
    "    [\"new car registrations by market and power source, monthly\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639174",
   "metadata": {},
   "outputs": [],
   "source": "# Vision-enabled pipeline on the ACEA car registrations PDF.\n\nacea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"\nacea_pdf_filtered, matches = filter_pdf_by_table_titles(\n    acea_pdf,\n    [\"new car registrations by market and power source, monthly\"],\n)\ncompressed_acea = compress_spatial_text(acea_pdf_filtered)\nprint(f\"Compressed chars: {len(compressed_acea)}\")\nprint(compressed_acea[:500])\nprint(\"...\")\n\n# Define a schema for ACEA car registrations\n# Note: car_motorization aliases match header parts to trigger unpivot\n# Note: date has empty aliases - the LLM infers year values from headers\nacea_schema_dict = {\n    \"description\": \"ACEA new car registrations by market and power source, monthly\",\n    \"columns\": [\n        {\"name\": \"country\", \"type\": \"string\", \"description\": \"Country of registration\", \"aliases\": [], \"format\": \"titlecase\"},\n        {\"name\": \"car_motorization\", \"type\": \"string\", \"description\": \"Car motorization type\", \"aliases\": [\"battery electric\", \"plug-in hybrid\", \"hybrid electric\", \"others\", \"petrol\", \"diesel\"], \"format\": \"titlecase\"},\n        {\"name\": \"new_car_registration\", \"type\": \"int\", \"description\": \"Number of new car registrations\", \"aliases\": [], \"format\": \"#,###\"},\n        {\"name\": \"date\", \"type\": \"string\", \"description\": \"Registration period (year from column header, month from document context)\", \"aliases\": [], \"format\": \"YYYY-MM\"},\n    ],\n}\n\nacea_schema = CanonicalSchema.from_dict(acea_schema_dict)\n\n# Run WITH vision (pdf_path= enables step 0)\nresult_acea = interpret_table(\n    compressed_acea,\n    acea_schema,\n    model=\"openai/gpt-4o\",\n    pdf_path=acea_pdf,\n)\n\n# Result is dict[int, MappedTable] — one entry per page\nrecords_acea = to_records(result_acea)\nprint(f\"Records extracted (vision): {len(records_acea)}\")\nfor page, mt in sorted(result_acea.items()):\n    print(f\"Page {page}: {len(mt.records)} records, unmapped={mt.unmapped_columns}\")\nprint(f\"\\n--- First 5 records ---\\n\")\nfor i, rec in enumerate(records_acea[:5], 1):\n    print(f\"[{i}] {rec}\")\n\n# Inspect per-page structure: each page has its own records, unmapped_columns, metadata\n{page: mt.model_dump() for page, mt in result_acea.items()}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "{page: mt.model_dump() for page, mt in result_acea.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1obb135qme3",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "After interpreting tables, export results to CSV, TSV, Parquet, pandas or polars DataFrames using the `serialize` module. All functions validate records against the schema and coerce OCR artifacts (e.g., `\"1,234\"` → `1234`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "s53jfqaky0h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV (first 500 chars) ===\n",
      "country,car_motorization,new_car_registration,date\n",
      "Austria,Battery Electric,4621,2025-12\n",
      "Austria,Plug-in Hybrid,2776,2025-12\n",
      "Austria,Hybrid Electric,7253,2025-12\n",
      "Austria,Others,0,2025-12\n",
      "Austria,Petrol,5750,2025-12\n",
      "Austria,Diesel,1976,2025-12\n",
      "Belgium,Battery Electric,11333,2025-12\n",
      "Belgium,Plug-in Hybrid,3345,2025-12\n",
      "Belgium,Hybrid Electric,3591,2025-12\n",
      "Belgium,Others,114,2025-12\n",
      "Belgium,Petrol,9900,2025-12\n",
      "Belgium,Diesel,594,2025-12\n",
      "Bulgaria,Battery Electric,236,2025-12\n",
      "Bulgaria,Pl\n",
      "...\n",
      "\n",
      "Wrote /tmp/acea_output.csv\n",
      "\n",
      "=== pandas DataFrame ===\n",
      "   page  country  car_motorization new_car_registration     date\n",
      "0     1  Austria  Battery Electric                 4621  2025-12\n",
      "1     1  Austria    Plug-in Hybrid                 2776  2025-12\n",
      "2     1  Austria   Hybrid Electric                 7253  2025-12\n",
      "3     1  Austria            Others                    0  2025-12\n",
      "4     1  Austria            Petrol                 5750  2025-12\n",
      "5     1  Austria            Diesel                 1976  2025-12\n",
      "6     1  Belgium  Battery Electric                11333  2025-12\n",
      "7     1  Belgium    Plug-in Hybrid                 3345  2025-12\n",
      "8     1  Belgium   Hybrid Electric                 3591  2025-12\n",
      "9     1  Belgium            Others                  114  2025-12\n",
      "\n",
      "Shape: (160, 5)\n",
      "\n",
      "Dtypes:\n",
      "page                     Int64\n",
      "country                 string\n",
      "car_motorization        string\n",
      "new_car_registration    string\n",
      "date                    string\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Serialize interpretation results to various formats\n",
    "from pdf_ocr import to_csv, to_tsv, to_pandas\n",
    "\n",
    "# Export to CSV string\n",
    "csv_str = to_csv(result_acea, acea_schema)\n",
    "print(\"=== CSV (first 500 chars) ===\")\n",
    "print(csv_str[:500])\n",
    "print(\"...\")\n",
    "\n",
    "# Export to CSV file with page column\n",
    "to_csv(result_acea, acea_schema, path=\"/tmp/acea_output.csv\", include_page=True)\n",
    "print(\"\\nWrote /tmp/acea_output.csv\")\n",
    "\n",
    "# Export to pandas DataFrame with proper nullable dtypes\n",
    "df = to_pandas(result_acea, acea_schema, include_page=True)\n",
    "print(\"\\n=== pandas DataFrame ===\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nDtypes:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr.interpret import analyze_and_parse                                                                                                                                                                              \n",
    "from pdf_ocr import compress_spatial_text, filter_pdf_by_table_titles                                                                                                                                                        \n",
    "                                                                                                                                                                                                                            \n",
    "acea_pdf = \"inputs/Press_release_car_registrations_December_2025.pdf\"                                                                                                                                                        \n",
    "filtered, _ = filter_pdf_by_table_titles(acea_pdf, pages=[2])                                                                                                                                                                \n",
    "compressed = compress_spatial_text(filtered)                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                                            \n",
    "# Check what Step 1 outputs                                                                                                                                                                                                  \n",
    "parsed = analyze_and_parse(compressed, model=\"openai/gpt-4o\")                                                                                                                                                                \n",
    "print(f\"table_type: {parsed.table_type}\")                                                                                                                                                                                    \n",
    "print(f\"headers: {parsed.headers}\")                                                                                                                                                                                          \n",
    "print(f\"notes: {parsed.notes}\")                                                                                                                                                                                              \n",
    "print(f\"data_rows count: {len(parsed.data_rows)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493c893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}