# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-1-20250805\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-5-haiku-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.218.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "interpret.baml": "// Table interpretation: extract structured tabular data from compressed PDF text\n// and map it to a user-defined canonical schema.\n\n// ─── Enums ───────────────────────────────────────────────────────────────────\n\nenum TableType {\n  FlatHeader\n  HierarchicalHeader\n  PivotedTable\n  TransposedTable\n  Unknown\n}\n\nenum AggregationType {\n  Total\n  Sum\n  Min\n  Max\n  Average\n  Count\n  NoAggregation @alias(\"None\")\n}\n\nenum Confidence {\n  High   @description(\"Direct column match by name or alias\")\n  Medium @description(\"Inferred from context with reasonable certainty (section header, title, etc.)\")\n  Low    @description(\"Best guess — ambiguous source or weak contextual signal\")\n}\n\n// ─── Intermediate pipeline types ─────────────────────────────────────────────\n\nclass HeaderInfo {\n  levels int @description(\"Number of header levels (1 for flat, >1 for hierarchical)\")\n  names string[][] @description(\"Header names per level, outer = level, inner = columns\")\n}\n\nclass AggregationInfo {\n  type AggregationType\n  axis string @description(\"'row' or 'column' — the axis along which aggregation runs\")\n  labels string[] @description(\"Labels identifying aggregation rows/columns, e.g. ['Total', 'Grand Total']\")\n}\n\nclass ParsedTable {\n  table_type TableType\n  headers HeaderInfo\n  aggregations AggregationInfo? @description(\"Present only if the table contains aggregation rows/columns\")\n  data_rows string[][] @description(\"All data rows (excluding headers and aggregation rows). Each inner array is one row of cell values as strings.\")\n  notes string? @description(\"Any caveats or observations about the table structure\")\n}\n\n// ─── Canonical schema + output types ─────────────────────────────────────────\n\nclass ColumnDef {\n  name string @description(\"Canonical column name\")\n  type string @description(\"Expected type: string, int, float, bool, date\")\n  description string @description(\"What this column represents\")\n  aliases string[] @description(\"Alternative names this column may appear as in source tables\")\n}\n\nclass CanonicalSchema {\n  columns ColumnDef[]\n  description string? @description(\"Optional description of what this schema represents\")\n}\n\nclass MappedRecord {\n  @@dynamic\n}\n\nclass FieldMapping {\n  column_name string  @description(\"Canonical column name from the schema\")\n  source string       @description(\"Where the value came from, e.g. 'column: Vessel Name', 'section header', 'document title', 'inferred from context'\")\n  rationale string    @description(\"Brief explanation of why this mapping was chosen\")\n  confidence Confidence\n}\n\nclass InterpretationMetadata {\n  model string                  @description(\"Model that produced this result, e.g. 'openai/gpt-4o'\")\n  field_mappings FieldMapping[] @description(\"One entry per canonical column, explaining how it was resolved\")\n  sections_detected string[]?   @description(\"Section/group labels found in the text, if any (e.g. ['GERALDTON', 'KWINANA'])\")\n}\n\nclass MappedTable {\n  records MappedRecord[] @description(\"Mapped data records conforming to the canonical schema\")\n  unmapped_columns string[] @description(\"Source columns that could not be mapped to any canonical column\")\n  mapping_notes string? @description(\"Notes about the mapping process, e.g. ambiguous matches or type coercion issues\")\n  metadata InterpretationMetadata @description(\"Metadata about the interpretation: model used, per-field mapping rationale, detected sections\")\n}\n\n// ─── Functions ───────────────────────────────────────────────────────────────\n\nfunction AnalyzeAndParseTable(compressed_text: string) -> ParsedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a table-structure analyst. Given compressed text extracted from a PDF,\n    identify the table structure and parse all data rows.\n\n    Steps:\n    1. Determine the table type (FlatHeader, HierarchicalHeader, PivotedTable, TransposedTable, or Unknown).\n    2. Extract headers — for hierarchical tables, capture each level separately.\n    3. Identify any aggregation rows/columns (totals, subtotals, averages, etc.) and separate them.\n    4. Parse every data row into an array of string cell values, preserving the original order.\n    5. If the text contains multiple table sections separated by labels or headers\n       (e.g. data grouped by category, region, time period, or any other dimension),\n       record each section label and which data rows belong to it in the notes field.\n       Format: \"Sections: <label1> (rows 0-N), <label2> (rows N+1-M), ...\"\n\n    Important:\n    - Keep cell values as strings exactly as they appear (do not reformat numbers or dates).\n    - Exclude header rows and aggregation rows from data_rows.\n    - Include ALL other rows, even if they have empty cells, dashes, \"N/A\", zero or\n      missing values, or unusual-looking content. If a row occupies a data position in\n      the table it is a data row — do not filter based on cell content.\n    - If the table has merged cells or spans, repeat the value for each spanned column.\n\n    Compressed text:\n    ---\n    {{ compressed_text }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction MapToCanonicalSchema(parsed_table: ParsedTable, schema: CanonicalSchema, model_name: string) -> MappedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a data mapper. Given a parsed table and a canonical schema, map each data row\n    to the canonical schema columns.\n\n    Mapping rules:\n    1. Match source columns to canonical columns using name, aliases, and description.\n    2. If a source column matches multiple canonical columns, pick the best match and note the ambiguity.\n    3. Coerce values to the expected type where possible (e.g. \"1,234\" -> 1234 for int columns).\n    4. For date columns, normalize to ISO 8601 format (YYYY-MM-DD) when the date is unambiguous.\n    5. If a canonical column has no matching source column, omit it from the record (it will be null).\n    6. List any source columns that could not be mapped in unmapped_columns.\n    7. CONTEXT INFERENCE: If a canonical column cannot be matched to any source column\n       (especially when its aliases list is empty), look for its value in the\n       surrounding context:\n       - Section headers or group labels that appear above or between data groups\n       - Document title, subtitle, or metadata lines\n       - Repeated contextual values that apply to all rows in a group\n       When a value is inferred from context, apply it to ALL rows in that section.\n       The parsed_table's notes field may contain section boundary information.\n    8. For EVERY canonical column in the schema, produce a FieldMapping in the\n       metadata explaining:\n       - Where the value came from (source)\n       - Why this mapping was chosen (rationale)\n       - How confident you are (High = direct name/alias match,\n         Medium = inferred from context with reasonable certainty,\n         Low = best guess or ambiguous)\n    9. Set metadata.model to \"{{ model_name }}\" (it will be provided).\n    10. If the text had section labels, list them in metadata.sections_detected.\n\n    Parsed table:\n    {{ parsed_table }}\n\n    Canonical schema:\n    {{ schema }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction InterpretTable(compressed_text: string, schema: CanonicalSchema, model_name: string) -> MappedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a table interpreter. Given compressed text from a PDF and a canonical schema,\n    extract and map tabular data in a single pass.\n\n    Steps:\n    1. Identify the table structure (flat, hierarchical, pivoted, transposed).\n    2. Extract all data rows (exclude headers and aggregation rows like totals).\n    3. Map each row's columns to the canonical schema using column names, aliases, and descriptions.\n    4. Coerce values to expected types (e.g. \"1,234\" -> 1234 for int, dates to ISO 8601).\n    5. List any source columns that could not be mapped.\n    6. CONTEXT INFERENCE: If a canonical column cannot be matched to any source column\n       (especially when its aliases list is empty), look for its value in the\n       surrounding context:\n       - Section headers or group labels that appear above or between data groups\n       - Document title, subtitle, or metadata lines\n       - Repeated contextual values that apply to all rows in a group\n       When a value is inferred from context, apply it to ALL rows in that section.\n    7. For EVERY canonical column in the schema, produce a FieldMapping in the\n       metadata explaining:\n       - Where the value came from (source)\n       - Why this mapping was chosen (rationale)\n       - How confident you are (High = direct name/alias match,\n         Medium = inferred from context with reasonable certainty,\n         Low = best guess or ambiguous)\n    8. Set metadata.model to \"{{ model_name }}\" (it will be provided).\n    9. If the text had section labels, list them in metadata.sections_detected.\n\n    Important:\n    - Keep values faithful to the source; only reformat for type coercion.\n    - If a canonical column has no match, omit it from the record.\n    - Note any ambiguities or issues in mapping_notes.\n\n    Compressed text:\n    ---\n    {{ compressed_text }}\n    ---\n\n    Canonical schema:\n    {{ schema }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// ─── Test cases ──────────────────────────────────────────────────────────────\n\ntest flat_shipping_stem {\n  functions [AnalyzeAndParseTable]\n  args {\n    compressed_text #\"\n      | Port | Vessel | Commodity | Quantity (MT) | ETA | Status |\n      |------|--------|-----------|---------------|-----|--------|\n      | Portland | MV Ocean Star | Wheat | 52,000 | 2025-10-15 | Loading |\n      | Geelong | MV Pacific Tide | Barley | 38,500 | 2025-10-18 | Scheduled |\n      | Kwinana | MV Iron Wind | Canola | 41,200 | 2025-10-20 | Scheduled |\n      | Total | | | 131,700 | | |\n    \"#\n  }\n}\n\ntest hierarchical_header {\n  functions [AnalyzeAndParseTable]\n  args {\n    compressed_text #\"\n      | | Q1 2025 | Q1 2025 | Q2 2025 | Q2 2025 |\n      | Region | Revenue | Costs | Revenue | Costs |\n      |--------|---------|-------|---------|-------|\n      | North | 1,200 | 800 | 1,350 | 850 |\n      | South | 980 | 650 | 1,100 | 700 |\n      | East | 1,500 | 950 | 1,600 | 1,000 |\n    \"#\n  }\n}\n\ntest pivoted_with_totals {\n  functions [InterpretTable]\n  args {\n    compressed_text #\"\n      | Commodity | Oct 2025 | Nov 2025 | Dec 2025 | Total |\n      |-----------|----------|----------|----------|-------|\n      | Wheat | 52,000 | 48,000 | 55,000 | 155,000 |\n      | Barley | 38,500 | 42,000 | 39,500 | 120,000 |\n      | Canola | 41,200 | 37,800 | 43,000 | 122,000 |\n      | Grand Total | 131,700 | 127,800 | 137,500 | 397,000 |\n    \"#\n    schema {\n      description \"Monthly commodity shipment volumes\"\n      columns [\n        {\n          name \"commodity\"\n          type \"string\"\n          description \"Type of commodity\"\n          aliases [\"Commodity\", \"Product\"]\n        },\n        {\n          name \"month\"\n          type \"string\"\n          description \"Month and year\"\n          aliases [\"Period\", \"Month\"]\n        },\n        {\n          name \"volume_mt\"\n          type \"int\"\n          description \"Volume in metric tonnes\"\n          aliases [\"Quantity\", \"Volume\", \"MT\"]\n        }\n      ]\n    }\n    model_name \"openai/gpt-4o\"\n  }\n}\n\ntest sectioned_table_context_inference {\n  functions [InterpretTable]\n  args {\n    compressed_text #\"\n      ## WAREHOUSE A\n\n      | Item | Qty | Unit Price |\n      |------|-----|------------|\n      | Widget Alpha | 150 | 12.50 |\n      | Widget Beta  | 300 | 8.75  |\n\n      ## WAREHOUSE B\n\n      | Item | Qty | Unit Price |\n      |------|-----|------------|\n      | Widget Alpha | 80  | 12.50 |\n      | Gadget Gamma | 200 | 22.00 |\n    \"#\n    schema {\n      description \"Inventory records by warehouse\"\n      columns [\n        {\n          name \"warehouse\"\n          type \"string\"\n          description \"Warehouse or location where the inventory is stored — may appear as a section header rather than a table column\"\n          aliases []\n        },\n        {\n          name \"item_name\"\n          type \"string\"\n          description \"Name of the inventory item\"\n          aliases [\"Item\", \"Product\", \"SKU\"]\n        },\n        {\n          name \"quantity\"\n          type \"int\"\n          description \"Number of units in stock\"\n          aliases [\"Qty\", \"Quantity\", \"Count\"]\n        },\n        {\n          name \"unit_price\"\n          type \"float\"\n          description \"Price per unit\"\n          aliases [\"Unit Price\", \"Price\"]\n        }\n      ]\n    }\n    model_name \"openai/gpt-4o\"\n  }\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return _file_map