# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model \"claude-opus-4-1-20250805\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-5-haiku-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example Google AI client (uncomment to use)\n// client<llm> CustomGemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     api_key env.GOOGLE_API_KEY\n//   }\n// }\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.218.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "interpret.baml": "// Table interpretation: extract structured tabular data from compressed PDF text\n// and map it to a user-defined canonical schema.\n\n// ─── Enums ───────────────────────────────────────────────────────────────────\n\nenum TableType {\n  FlatHeader @description(\"Standard table: headers in top row(s), data in subsequent rows. Each column is independent (no spanning parent cells). Header text may wrap across multiple rows vertically but each column stands alone. 1:1 row-to-record mapping. Key sign: first column contains different values per row (IDs, names, categories).\")\n  HierarchicalHeader @description(\"Tree-structured headers where parent cells span horizontally across multiple child columns (e.g. 'Q1 2025' spanning 'Revenue' and 'Costs'). Column names become compound paths (e.g. 'Q1 2025 / Revenue'). 1:1 row-to-record mapping. Key sign: header row has FEWER cells than data rows because parents span multiple children. Note: stacked/multiline headers with no horizontal spanning are FlatHeader, not this.\")\n  PivotedTable @description(\"Cross-tabulation: row labels are categories, column headers are time periods or attributes, cells are values. Requires unpivoting: each source row produces multiple output records (one per value column). Key sign: column headers are dates, periods, or repeated attribute names; first column contains category labels; cells contain numeric values.\")\n  TransposedTable @description(\"Property sheet layout: field NAMES as rows (left column), field VALUES as columns. Each column (after the label column) represents a complete record. Requires transposition: each column becomes one output record, each row provides one field. Key signs: (1) first column contains field names like 'Name', 'Date', 'Status', 'Total'; (2) subsequent columns contain the corresponding values; (3) rows are heterogeneous (mix of dates, numbers, text) because each row is a different field type.\")\n  Unknown @description(\"Ambiguous structure — use best judgment based on content\")\n}\n\nenum AggregationType {\n  Total\n  Sum\n  Min\n  Max\n  Average\n  Count\n  NoAggregation @alias(\"None\")\n}\n\nenum Confidence {\n  High   @description(\"Direct column match by name or alias\")\n  Medium @description(\"Inferred from context with reasonable certainty (section header, title, etc.)\")\n  Low    @description(\"Best guess — ambiguous source or weak contextual signal\")\n}\n\n// ─── Intermediate pipeline types ─────────────────────────────────────────────\n\nclass HeaderInfo {\n  levels int @description(\"Number of header rows. For FlatHeader with stacked/multiline labels, this counts the text rows but names[][] should contain the combined column names. For HierarchicalHeader, this counts the hierarchy depth.\")\n  names string[][] @description(\"Header names per level, outer = level, inner = columns. For FlatHeader (even with multiline labels), use a single level with combined names. For HierarchicalHeader, each level represents a tier in the tree.\")\n}\n\nclass AggregationInfo {\n  type AggregationType\n  axis string @description(\"'row' or 'column' — the axis along which aggregation runs\")\n  labels string[] @description(\"Labels identifying aggregation rows/columns, e.g. ['Total', 'Grand Total']\")\n}\n\nclass ParsedTable {\n  table_type TableType\n  headers HeaderInfo\n  aggregations AggregationInfo? @description(\"Present only if the table contains aggregation rows/columns\")\n  data_rows string[][] @description(\"All data rows (excluding headers and aggregation rows). Each inner array is one row of cell values as strings.\")\n  notes string? @description(\"Any caveats or observations about the table structure\")\n}\n\n// ─── Canonical schema + output types ─────────────────────────────────────────\n\nclass ColumnDef {\n  name string @description(\"Canonical column name\")\n  type string @description(\"Expected type: string, int, float, bool, date\")\n  description string @description(\"What this column represents\")\n  aliases string[] @description(\"Alternative names this column may appear as in source tables\")\n}\n\nclass CanonicalSchema {\n  columns ColumnDef[]\n  description string? @description(\"Optional description of what this schema represents\")\n}\n\nclass MappedRecord {\n  @@dynamic\n}\n\nclass FieldMapping {\n  column_name string  @description(\"Canonical column name from the schema\")\n  source string       @description(\"Where the value came from, e.g. 'column: Vessel Name', 'section header', 'document title', 'inferred from context'\")\n  rationale string    @description(\"Brief explanation of why this mapping was chosen\")\n  confidence Confidence\n}\n\nclass TableTypeInference {\n  table_type TableType          @description(\"The inferred table structure type\")\n  rationale string              @description(\"Brief explanation of why this table type was chosen, citing specific structural evidence\")\n  confidence Confidence         @description(\"How confident the inference is based on structural clarity\")\n}\n\nclass InterpretationMetadata {\n  model string                  @description(\"Model that produced this result, e.g. 'openai/gpt-4o'\")\n  table_type_inference TableTypeInference @description(\"The inferred table type with rationale explaining the classification\")\n  field_mappings FieldMapping[] @description(\"One entry per canonical column, explaining how it was resolved\")\n  sections_detected string[]?   @description(\"Section/group labels found in the text, if any (e.g. ['GERALDTON', 'KWINANA'])\")\n  section_role string?          @description(\"'context' if a single section label applies to all rows, 'grouping' if multiple labels partition rows into groups\")\n}\n\nclass MappedTable {\n  records MappedRecord[] @description(\"Mapped data records conforming to the canonical schema\")\n  unmapped_columns string[] @description(\"Source columns that could not be mapped to any canonical column\")\n  mapping_notes string? @description(\"Notes about the mapping process, e.g. ambiguous matches or type coercion issues\")\n  metadata InterpretationMetadata @description(\"Metadata about the interpretation: model used, per-field mapping rationale, detected sections\")\n}\n\n// ─── Vision-based schema inference ──────────────────────────────────────────\n\nclass InferredTableSchema {\n  column_names string[] @description(\"Column headers as they visually appear in the table, left to right. For stacked/multiline headers, combine the vertically stacked labels into a single name per column (e.g. 'Unique Slot Reference Number'). For hierarchical headers with spanning parents, use compound paths (e.g. 'Q1 2025 / Revenue').\")\n  column_count int @description(\"Total number of data columns\")\n  header_levels int @description(\"Number of header rows (1 for single-row, >1 for stacked/multiline or hierarchical)\")\n  has_spanning_headers bool @description(\"True if parent cells span multiple child columns (hierarchical). False if headers are just stacked text with no spanning (multiline flat).\")\n  notes string? @description(\"Observations about the header structure: merged cells, stacked labels, spanning parent cells, visual groupings, etc.\")\n}\n\n// ─── Functions ───────────────────────────────────────────────────────────────\n\nfunction AnalyzeAndParseTable(compressed_text: string) -> ParsedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a table-structure analyst. Given compressed text extracted from a PDF,\n    identify the table structure and parse all data rows.\n\n    === DATA-FIRST APPROACH ===\n\n    The data rows are the ground truth for column structure. Analyze them FIRST:\n\n    STEP 1: COUNT DATA COLUMNS\n    - Find the pipe table (rows with | delimiters)\n    - Count cells in the FIRST data row (after |---|---| separator)\n    - This is your AUTHORITATIVE column count\n\n    STEP 2: IDENTIFY DATA TYPES PER COLUMN\n    - Look at values in each column position across multiple data rows\n    - Identify: dates, times, numbers, text, codes, status words, etc.\n    - Note any concatenated values (e.g. \"1500 Category A\" = number + text = 2 columns)\n\n    STEP 3: MAP HEADERS TO DATA COLUMNS\n    - Look at the header lines ABOVE the pipe table\n    - For stacked/multiline headers, combine vertically aligned text:\n      \"Unique Slot\" / \"Reference\" / \"Number\" → \"Unique Slot Reference Number\"\n    - You MUST produce exactly as many header names as data columns\n\n    === TABLE TYPE CLASSIFICATION ===\n\n    - FlatHeader: Each column independent, even with stacked/multiline header text.\n      Header text may span multiple rows vertically but no cell spans horizontally.\n    - HierarchicalHeader: Parent cells SPAN HORIZONTALLY across multiple child columns.\n      Key sign: A header row has FEWER cells than data rows because parents span.\n    - PivotedTable: Row labels are categories, columns are time periods/attributes.\n    - TransposedTable: Field names as rows, records as columns.\n\n    === PARSING RULES ===\n\n    1. headers.names MUST have exactly column_count entries (one per data column)\n    2. Each data_row MUST have exactly column_count cells\n    3. If a pipe cell contains concatenated values that span multiple columns,\n       SPLIT them to maintain consistent column count\n    4. Exclude aggregation rows (totals, subtotals) from data_rows\n    5. Keep cell values as strings exactly as they appear\n    6. Include ALL data rows even with empty cells, dashes, \"N/A\", or zeros\n\n    SECTION LABELS:\n    If the text has sections (e.g. data grouped by region, category, etc.),\n    record in notes: \"Sections: <label1> (rows 0-N), <label2> (rows N+1-M), ...\"\n\n    Compressed text:\n    ---\n    {{ compressed_text }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction InferTableSchemaFromImage(page_img: image, compressed_text: string) -> InferredTableSchema {\n  client CustomGPT4o\n  prompt #\"\n    {{ _.role(\"system\") }}\n    You are a table structure analyst. You are given:\n    1. An image of a PDF page containing a table\n    2. Compressed text extracted from the same page (which may have parsing errors)\n\n    Your task: determine the COMPLETE table column structure. Use a DATA-FIRST\n    approach — the data rows are the ground truth for column count.\n\n    === STEP 1: ANALYZE DATA ROWS (Ground Truth) ===\n\n    Look at the DATA ROWS in the image (below the headers):\n    1. Pick a complete data row and count the distinct columns visually\n    2. Note column boundaries — where values end and new values begin\n    3. Identify the DATA TYPE in each column:\n       - Dates (DD/MM/YYYY, YYYY-MM-DD, etc.)\n       - Times (HH:MM, HH:MM:SS AM/PM)\n       - Numbers (quantities, IDs, codes)\n       - Text (names, descriptions, status words)\n       - Mixed (e.g. \"1500 Category A\" = number + text = likely 2 columns concatenated)\n\n    The number of distinct data columns is your AUTHORITATIVE column_count.\n\n    === STEP 2: VALIDATE WITH COMPRESSED TEXT ===\n\n    Count pipe-separated cells in the FIRST data row of the compressed text\n    (the row after |---|---|). Compare to your visual count:\n    - If text has FEWER cells → some values are concatenated in text\n    - If text has MORE cells → you may have missed narrow columns visually\n    - Note any concatenations (e.g. \"7:00 PM Company X\" = time + text = 2 columns)\n\n    === STEP 3: MAP HEADERS TO DATA COLUMNS ===\n\n    Now look at the header area and assign a name to EACH data column:\n    1. For stacked/multiline headers, combine vertically aligned text into one name\n       (e.g. \"Unique Slot\" / \"Reference\" / \"Number\" → \"Unique Slot Reference Number\")\n    2. For hierarchical headers with horizontal spanning, use compound paths\n       (e.g. \"Date\" spanning \"Nominated\" and \"Accepted\" → \"Date / Nominated\", \"Date / Accepted\")\n    3. If a header is unclear, infer from the data type (e.g. column with dates\n       and no clear header might be \"Date\" or \"ETA\")\n\n    CRITICAL: column_names array length MUST equal column_count from Step 1.\n\n    === STEP 4: DETERMINE HEADER STRUCTURE ===\n\n    Set has_spanning_headers based on visual header structure:\n    - FALSE (stacked/multiline): Column dividers run continuously top-to-bottom,\n      each column independent even if header text spans multiple rows vertically\n    - TRUE (hierarchical): Parent cells span horizontally across multiple child\n      columns, column dividers only appear in lower rows under the spanning cell\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }}\n    {{ page_img }}\n\n    Compressed text (for reference — may contain errors):\n    ---\n    {{ compressed_text }}\n    ---\n  \"#\n}\n\nfunction AnalyzeAndParseTableGuided(compressed_text: string, visual_schema: InferredTableSchema) -> ParsedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a table-structure analyst. Given compressed text extracted from a PDF\n    AND a visual schema describing the correct column structure (inferred from the\n    page image), parse the table data rows.\n\n    VISUAL SCHEMA (from image analysis — treat as ground truth for column count\n    and header names):\n    {{ visual_schema }}\n\n    The compressed text below may have parsing artifacts: column headers may be\n    garbled or values from adjacent columns may be concatenated into a single\n    cell. Use the visual schema's column_names and column_count to:\n    - Correctly identify which part of the text corresponds to which column\n    - Split concatenated values when the column count in a row doesn't match\n      the expected count (e.g. \"1500 Category A\" should become two cells: \"1500\"\n      and \"Category A\" if they map to separate columns)\n    - Use the visual schema's column names as the authoritative header names\n\n    Steps:\n    1. Use the visual schema to set the correct headers (column_names →\n       headers.names, header_levels → headers.levels).\n    2. Determine the table type based on has_spanning_headers:\n       - If has_spanning_headers is true → HierarchicalHeader (parent cells span\n         multiple child columns in a tree structure)\n       - If has_spanning_headers is false → FlatHeader (even with stacked/multiline\n         headers, each column is independent)\n    3. Identify any aggregation rows/columns and separate them.\n    4. Parse every data row into an array of string cell values — the array\n       length MUST equal visual_schema.column_count for each row.\n    5. If the text contains multiple table sections separated by labels or headers\n       (e.g. data grouped by category, region, time period, or any other dimension),\n       record each section label and which data rows belong to it in the notes field.\n       Format: \"Sections: <label1> (rows 0-N), <label2> (rows N+1-M), ...\"\n\n    Important:\n    - Keep cell values as strings exactly as they appear (do not reformat).\n    - Exclude header rows and aggregation rows from data_rows.\n    - Include ALL data rows regardless of cell content.\n    - If a row has fewer tokens than expected columns, split concatenated\n      values to reach the correct column count.\n    - If the table has merged cells or spans, repeat the value for each\n      spanned column.\n\n    Compressed text:\n    ---\n    {{ compressed_text }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction MapToCanonicalSchema(parsed_table: ParsedTable, schema: CanonicalSchema, model_name: string) -> MappedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a data mapper. Given a parsed table and a canonical schema, map data rows\n    to the canonical schema columns using a strategy appropriate for the table type.\n\n    TABLE TYPE: {{ parsed_table.table_type }}\n\n    Mapping strategy by table type:\n    - FlatHeader: Map each data row directly to one output record. Columns align 1:1.\n    - HierarchicalHeader: Column names may be compound from merged headers (e.g. \"Q1 / Revenue\").\n      Match the full compound name against aliases. Each data row produces one record.\n    - PivotedTable: Categories are rows, time periods or attributes are columns. You must UNPIVOT:\n      for each data row, produce ONE OUTPUT RECORD PER VALUE COLUMN. The column header becomes\n      a field value (e.g. column \"Period A\" → the \"period\" field gets value \"Period A\").\n      Example: a row \"Item X | 100 | 200\" with columns \"Category | Period A | Period B\"\n      becomes TWO records: {category: \"Item X\", period: \"Period A\", value: 100} and\n      {category: \"Item X\", period: \"Period B\", value: 200}.\n    - TransposedTable: Field names are in the FIRST COLUMN, values are in subsequent columns.\n      You must TRANSPOSE before mapping:\n      1. The first column contains field names — these become your \"headers\" for alias matching\n      2. Each subsequent column is ONE COMPLETE RECORD\n      3. Match each row's first-column label against canonical column aliases\n      4. Example: If rows are \"Name | Alice | Bob\" and \"Status | Active | Inactive\",\n         produce TWO records: {name: \"Alice\", status: \"Active\"} and {name: \"Bob\", status: \"Inactive\"}\n      5. EVERY row label must be checked against ALL canonical column aliases\n    - Unknown: Use best judgment based on the data structure.\n\n    General mapping rules:\n    1. Match source columns to canonical columns using name, aliases, and description.\n    2. If a source column matches multiple canonical columns, pick the best match and note the ambiguity.\n    3. Coerce values to the expected type where possible (e.g. \"1,234\" -> 1234 for int columns).\n    4. For date columns, normalize to ISO 8601 format (YYYY-MM-DD) when the date is unambiguous.\n    5. If a canonical column has no matching source column, omit it from the record (it will be null).\n    6. List any source columns that could not be mapped in unmapped_columns.\n    7. CONTEXT INFERENCE: If a canonical column cannot be matched to any source column\n       (especially when its aliases list is empty), look for its value in the\n       surrounding context:\n       - Section headers or group labels that appear above or between data groups\n       - Document title, subtitle, or metadata lines\n       - Repeated contextual values that apply to all rows in a group\n       When a value is inferred from context, apply it to ALL rows in that section.\n       The parsed_table's notes field may contain section boundary information.\n    8. For EVERY canonical column in the schema, produce a FieldMapping in the\n       metadata explaining:\n       - Where the value came from (source)\n       - Why this mapping was chosen (rationale)\n       - How confident you are (High = direct name/alias match,\n         Medium = inferred from context with reasonable certainty,\n         Low = best guess or ambiguous)\n    9. Set metadata.model to \"{{ model_name }}\" (it will be provided).\n    10. If the text had section labels, list them in metadata.sections_detected and set\n        metadata.section_role to:\n        - \"context\" if there is only ONE section label that applies to all rows\n        - \"grouping\" if there are MULTIPLE section labels that partition rows into groups\n    11. Populate metadata.table_type_inference with:\n        - table_type: Use the table type from parsed_table ({{ parsed_table.table_type }})\n        - rationale: Explain why this classification applies based on the table structure.\n          For FlatHeader vs HierarchicalHeader, explicitly state whether parent cells span\n          horizontally across multiple child columns or not.\n        - confidence: High if structure is unambiguous, Medium if some irregularities, Low if uncertain\n\n    Parsed table:\n    {{ parsed_table }}\n\n    Canonical schema:\n    {{ schema }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction InterpretTable(compressed_text: string, schema: CanonicalSchema, model_name: string) -> MappedTable {\n  client CustomGPT4o\n  prompt #\"\n    You are a table interpreter. Given compressed text from a PDF and a canonical schema,\n    extract and map tabular data in a single pass.\n\n    DETECTING HEADER STRUCTURE:\n\n    Before classifying, examine the header rows carefully:\n\n    1. STACKED/MULTILINE (→ FlatHeader):\n       - Each column has text that wraps across multiple rows vertically\n       - BUT each column is independent — no cell spans multiple columns horizontally\n       - Example: \"Unique Slot\" / \"Reference\" / \"Number\" stacked vertically = one column\n         named \"Unique Slot Reference Number\"\n       - Key sign: The number of text fragments per row equals the number of columns\n\n    2. GROUPED/SPANNING (→ HierarchicalHeader):\n       - A parent header cell spans HORIZONTALLY across multiple child columns\n       - Children are sibling columns that share a common parent cell above them\n       - Example: \"Date\" spanning over \"Nominated\" and \"Accepted\" = compound names\n         \"Date / Nominated\" and \"Date / Accepted\"\n       - Key sign: A header row has FEWER cells than the data rows because parents span\n\n    KEY TEST: Do sibling columns share a common parent cell above them that spans\n    horizontally? If YES → HierarchicalHeader. If NO → FlatHeader (even with multiline text).\n\n    Steps:\n    1. Identify the table structure using the header structure tests above and apply the\n       appropriate mapping strategy:\n       - FlatHeader: Each column has its own independent header. This includes tables with\n         stacked/multiline headers where text is split across multiple rows but each column\n         stands alone — combine the stacked text into one column name. Map each row to one record.\n       - HierarchicalHeader: Parent header cells SPAN HORIZONTALLY across multiple child columns\n         in a tree structure (e.g. \"Q1 2025\" spanning over \"Revenue\" and \"Costs\" columns beneath it).\n         Column names are compound paths (e.g. \"Q1 2025 / Revenue\"). Map each row to one record.\n         NOTE: Multiline/stacked headers with no horizontal spanning are FlatHeader, not this.\n       - PivotedTable: Categories as rows, time periods or attributes as columns. UNPIVOT:\n         for each data row, produce ONE RECORD PER VALUE COLUMN. The column header becomes\n         a field value. Example: \"Item X | 100 | 200\" with columns \"Category | Period A | Period B\"\n         becomes TWO records: {category: \"Item X\", period: \"Period A\", value: 100} and\n         {category: \"Item X\", period: \"Period B\", value: 200}.\n       - TransposedTable: Field names in FIRST COLUMN, values in subsequent columns. TRANSPOSE:\n         (1) First column labels become \"headers\" for alias matching\n         (2) Each subsequent column is ONE COMPLETE RECORD\n         (3) Match each row label against canonical column aliases\n         (4) Example: rows \"Name | Alice | Bob\" and \"Status | Active | Inactive\" →\n             TWO records: {name: \"Alice\", status: \"Active\"}, {name: \"Bob\", status: \"Inactive\"}\n    2. Extract all data rows (exclude headers and aggregation rows like totals).\n    3. Match source columns to canonical columns using names, aliases, and descriptions.\n    4. Coerce values to expected types (e.g. \"1,234\" -> 1234 for int, dates to ISO 8601).\n    5. List any source columns that could not be mapped.\n    6. CONTEXT INFERENCE: If a canonical column cannot be matched to any source column\n       (especially when its aliases list is empty), look for its value in the\n       surrounding context:\n       - Section headers or group labels that appear above or between data groups\n       - Document title, subtitle, or metadata lines\n       - Repeated contextual values that apply to all rows in a group\n       When a value is inferred from context, apply it to ALL rows in that section.\n    7. For EVERY canonical column in the schema, produce a FieldMapping in the\n       metadata explaining:\n       - Where the value came from (source)\n       - Why this mapping was chosen (rationale)\n       - How confident you are (High = direct name/alias match,\n         Medium = inferred from context with reasonable certainty,\n         Low = best guess or ambiguous)\n    8. Set metadata.model to \"{{ model_name }}\" (it will be provided).\n    9. If the text had section labels, list them in metadata.sections_detected and set\n       metadata.section_role to:\n       - \"context\" if there is only ONE section label that applies to all rows\n       - \"grouping\" if there are MULTIPLE section labels that partition rows into groups\n    10. Populate metadata.table_type_inference with:\n        - table_type: The table structure type you identified in step 1\n        - rationale: Explain why this classification applies, citing specific structural evidence.\n          For FlatHeader vs HierarchicalHeader, explicitly state whether parent cells span\n          horizontally across multiple child columns or not.\n        - confidence: High if structure is unambiguous, Medium if some irregularities, Low if uncertain\n\n    Important:\n    - Keep values faithful to the source; only reformat for type coercion.\n    - If a canonical column has no match, omit it from the record.\n    - Note any ambiguities or issues in mapping_notes.\n\n    Compressed text:\n    ---\n    {{ compressed_text }}\n    ---\n\n    Canonical schema:\n    {{ schema }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// ─── Test cases ──────────────────────────────────────────────────────────────\n\ntest flat_shipping_stem {\n  functions [AnalyzeAndParseTable]\n  args {\n    compressed_text #\"\n      | Port | Vessel | Commodity | Quantity (MT) | ETA | Status |\n      |------|--------|-----------|---------------|-----|--------|\n      | Portland | MV Ocean Star | Wheat | 52,000 | 2025-10-15 | Loading |\n      | Geelong | MV Pacific Tide | Barley | 38,500 | 2025-10-18 | Scheduled |\n      | Kwinana | MV Iron Wind | Canola | 41,200 | 2025-10-20 | Scheduled |\n      | Total | | | 131,700 | | |\n    \"#\n  }\n}\n\ntest hierarchical_header {\n  functions [AnalyzeAndParseTable]\n  args {\n    compressed_text #\"\n      | | Q1 2025 | Q1 2025 | Q2 2025 | Q2 2025 |\n      | Region | Revenue | Costs | Revenue | Costs |\n      |--------|---------|-------|---------|-------|\n      | North | 1,200 | 800 | 1,350 | 850 |\n      | South | 980 | 650 | 1,100 | 700 |\n      | East | 1,500 | 950 | 1,600 | 1,000 |\n    \"#\n  }\n}\n\ntest pivoted_with_totals {\n  functions [InterpretTable]\n  args {\n    compressed_text #\"\n      | Commodity | Oct 2025 | Nov 2025 | Dec 2025 | Total |\n      |-----------|----------|----------|----------|-------|\n      | Wheat | 52,000 | 48,000 | 55,000 | 155,000 |\n      | Barley | 38,500 | 42,000 | 39,500 | 120,000 |\n      | Canola | 41,200 | 37,800 | 43,000 | 122,000 |\n      | Grand Total | 131,700 | 127,800 | 137,500 | 397,000 |\n    \"#\n    schema {\n      description \"Monthly commodity shipment volumes\"\n      columns [\n        {\n          name \"commodity\"\n          type \"string\"\n          description \"Type of commodity\"\n          aliases [\"Commodity\", \"Product\"]\n        },\n        {\n          name \"month\"\n          type \"string\"\n          description \"Month and year\"\n          aliases [\"Period\", \"Month\"]\n        },\n        {\n          name \"volume_mt\"\n          type \"int\"\n          description \"Volume in metric tonnes\"\n          aliases [\"Quantity\", \"Volume\", \"MT\"]\n        }\n      ]\n    }\n    model_name \"openai/gpt-4o\"\n  }\n}\n\ntest sectioned_table_context_inference {\n  functions [InterpretTable]\n  args {\n    compressed_text #\"\n      ## WAREHOUSE A\n\n      | Item | Qty | Unit Price |\n      |------|-----|------------|\n      | Widget Alpha | 150 | 12.50 |\n      | Widget Beta  | 300 | 8.75  |\n\n      ## WAREHOUSE B\n\n      | Item | Qty | Unit Price |\n      |------|-----|------------|\n      | Widget Alpha | 80  | 12.50 |\n      | Gadget Gamma | 200 | 22.00 |\n    \"#\n    schema {\n      description \"Inventory records by warehouse\"\n      columns [\n        {\n          name \"warehouse\"\n          type \"string\"\n          description \"Warehouse or location where the inventory is stored — may appear as a section header rather than a table column\"\n          aliases []\n        },\n        {\n          name \"item_name\"\n          type \"string\"\n          description \"Name of the inventory item\"\n          aliases [\"Item\", \"Product\", \"SKU\"]\n        },\n        {\n          name \"quantity\"\n          type \"int\"\n          description \"Number of units in stock\"\n          aliases [\"Qty\", \"Quantity\", \"Count\"]\n        },\n        {\n          name \"unit_price\"\n          type \"float\"\n          description \"Price per unit\"\n          aliases [\"Unit Price\", \"Price\"]\n        }\n      ]\n    }\n    model_name \"openai/gpt-4o\"\n  }\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return _file_map