{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# DOCX Table Extraction Walkthrough\n",
    "\n",
    "This notebook demonstrates extracting structured tables from Word documents (.docx).\n",
    "\n",
    "Key capabilities:\n",
    "- **Merged cell handling**: python-docx returns duplicate `_tc` references for horizontally merged cells — the extractor deduplicates them\n",
    "- **Compound headers**: hierarchical headers (metric labels spanning 2+ columns) are stacked with \" / \" separators\n",
    "- **User-defined classification**: `classify_docx_tables()` accepts caller-supplied categories and keywords\n",
    "- **Dynamic pivot values**: `extract_pivot_values()` reads years from headers instead of hardcoding\n",
    "\n",
    "The pipeline: DOCX → extract raw grids → detect header rows (merge-based) → build compound headers → render pipe-table markdown → `interpret_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-md",
   "metadata": {},
   "source": [
    "## 1. Synthetic Files — The Agnostic API\n",
    "\n",
    "8 synthetic DOCX files in `inputs/docx/synthetic/` cover different table structures.\n",
    "No domain knowledge required — the API works on any DOCX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-list",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "synth_files = sorted(glob.glob(\"inputs/docx/synthetic/*.docx\"))\n",
    "print(f\"{len(synth_files)} synthetic DOCX files:\")\n",
    "for f in synth_files:\n",
    "    print(f\"  {f.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import extract_tables_from_docx, compress_docx_tables\n",
    "\n",
    "# Basic extraction from a flat table (no merges)\n",
    "tables = extract_tables_from_docx(\"inputs/docx/synthetic/flat.docx\")\n",
    "print(f\"flat.docx: {len(tables)} table, {len(tables[0].column_names)} cols, {len(tables[0].data)} data rows\")\n",
    "print(f\"Source format: {tables[0].source_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-md",
   "metadata": {},
   "source": [
    "## 2. Merged Cells and Compound Headers\n",
    "\n",
    "When headers span multiple rows (horizontal merge for metric labels, vertical merge for label columns),\n",
    "`compress_docx_tables()` builds compound column names joined by \" / \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-hspan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hspan.docx: 2 header rows with horizontal spans\n",
    "# Row 0: Category | Revenue (span 2) | Cost (span 2)\n",
    "# Row 1:          | Q1 | Q2          | Q1 | Q2\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/hspan.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-deep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_hierarchy.docx: 3 header rows (Group / Sub / Year)\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/deep_hierarchy.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_hspan.docx: title row (\"QUARTERLY REPORT\") + hierarchical headers\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/title_hspan.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Title: {meta['title']!r}\")\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-md",
   "metadata": {},
   "source": [
    "## 3. The Merged Cell Bug Fix\n",
    "\n",
    "python-docx returns **duplicate `_tc` references** for horizontally merged cells. A cell with `gridSpan=2`\n",
    "appears twice in `row.cells`, both pointing to the same XML element. Without deduplication, headers shift right.\n",
    "\n",
    "The fix: track `id(cell._tc)` in a `seen_tcs` set and skip duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from pdf_ocr.docx_extractor import _build_grid_from_table, _detect_header_rows_from_merges\n",
    "\n",
    "# Show raw grid from hspan.docx — verify no duplicate text\n",
    "doc = Document(\"inputs/docx/synthetic/hspan.docx\")\n",
    "grid, _ = _build_grid_from_table(doc.tables[0])\n",
    "hc = _detect_header_rows_from_merges(doc.tables[0])\n",
    "\n",
    "print(f\"Grid: {len(grid)} rows x {len(grid[0])} cols, {hc} header rows\")\n",
    "for i, row in enumerate(grid):\n",
    "    tag = \"[header]\" if i < hc else \"[data]  \"\n",
    "    print(f\"  row {i} {tag}: {row}\")\n",
    "\n",
    "print(f\"\\nRow 0 col 1 = {grid[0][1]!r}  (not duplicated)\")\n",
    "print(f\"Row 0 col 2 = {grid[0][2]!r}  (empty span continuation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-md",
   "metadata": {},
   "source": [
    "## 4. Classification with User-Defined Categories\n",
    "\n",
    "`classify_docx_tables(path, categories)` matches header text against caller-supplied keywords.\n",
    "No hardcoded domain knowledge — you define your own categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-synth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import classify_docx_tables\n",
    "\n",
    "# multi_category.docx has 4 tables: shipping, HR, financial, inventory\n",
    "categories = {\n",
    "    \"shipping\": [\"cargo\", \"port\", \"vessel\"],\n",
    "    \"hr\": [\"employee\", \"salary\", \"department\"],\n",
    "    \"finance\": [\"revenue\", \"expenses\"],\n",
    "    \"inventory\": [\"stock\", \"warehouse\"],\n",
    "}\n",
    "\n",
    "classes = classify_docx_tables(\"inputs/docx/synthetic/multi_category.docx\", categories)\n",
    "for c in classes:\n",
    "    print(f\"  Table {c['index']}: {c['category']:12s} title={c['title'] or '(none)':25s} {c['rows']}r x {c['cols']}c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classification to filter and compress only financial tables\n",
    "finance_idx = [c[\"index\"] for c in classes if c[\"category\"] == \"finance\"]\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/multi_category.docx\", table_indices=finance_idx)\n",
    "\n",
    "for md, meta in results:\n",
    "    print(f\"Title: {meta['title']!r}\")\n",
    "    print(md)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-md",
   "metadata": {},
   "source": [
    "## 5. Dynamic Pivot Values\n",
    "\n",
    "`extract_pivot_values()` reads years from the compressed markdown headers.\n",
    "This avoids hardcoding years that change every release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5-pivot",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_ocr import extract_pivot_values\n",
    "\n",
    "# Extract years from each synthetic file that has them\n",
    "for name in [\"hspan\", \"title_hspan\", \"deep_hierarchy\", \"unicode\"]:\n",
    "    results = compress_docx_tables(f\"inputs/docx/synthetic/{name}.docx\")\n",
    "    md, _ = results[0]\n",
    "    years = extract_pivot_values(md)\n",
    "    print(f\"{name:20s} → years={years}  last 2: {years[-2:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6-md",
   "metadata": {},
   "source": [
    "## 6. Real-World Example — Russian Agricultural Reports\n",
    "\n",
    "6 weekly DOCX reports from the Russian Ministry of Agriculture.\n",
    "Each contains multiple tables (export summaries, harvest progress, planting progress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCX_DIR = \"inputs/docx/input\"\n",
    "docx_files = sorted(glob.glob(f\"{DOCX_DIR}/*.docx\"))\n",
    "\n",
    "labels = {\n",
    "    \"Apr 21-22\": [f for f in docx_files if \"Apr 21\" in f][0],\n",
    "    \"Apr 25-26\": [f for f in docx_files if \"April 25\" in f][0],\n",
    "    \"May 9-10\": [f for f in docx_files if \"May\" in f][0],\n",
    "    \"Jun 20-21\": [f for f in docx_files if \"June\" in f][0],\n",
    "    \"Jul 11-12\": [f for f in docx_files if \"July\" in f][0],\n",
    "    \"Sep 2-3\": [f for f in docx_files if \"September\" in f][0],\n",
    "}\n",
    "\n",
    "print(f\"{len(docx_files)} DOCX files:\")\n",
    "for label, path in labels.items():\n",
    "    print(f\"  {label}: {path.split('/')[-1][:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-classify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain-specific categories for Russian agricultural reports\n",
    "ag_categories = {\n",
    "    \"harvest\": [\"area harvested\", \"yield\", \"collected\", \"bunker\", \"centner\",\n",
    "                \"harvested area\", \"crop harvested\"],\n",
    "    \"planting\": [\"spring crops\", \"moa target\", \"spring wheat\", \"spring barley\",\n",
    "                 \"sown area\", \"planting\", \"sowing\", \"planted\"],\n",
    "    \"export\": [\"export\", \"shipment\", \"ports\", \"fob\", \"vessel\", \"cargo\"],\n",
    "}\n",
    "\n",
    "for label, path in labels.items():\n",
    "    classes = classify_docx_tables(path, ag_categories)\n",
    "    counts = {}\n",
    "    for c in classes:\n",
    "        counts[c[\"category\"]] = counts.get(c[\"category\"], 0) + 1\n",
    "    print(f\"{label:12s} ({len(classes):2d} tables): {dict(sorted(counts.items()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the WHEAT table from the July file\n",
    "july_path = labels[\"Jul 11-12\"]\n",
    "july_classes = classify_docx_tables(july_path, ag_categories)\n",
    "harvest_idx = [c[\"index\"] for c in july_classes if c[\"category\"] == \"harvest\"]\n",
    "\n",
    "july_results = compress_docx_tables(july_path, table_indices=harvest_idx)\n",
    "wheat_md, wheat_meta = [(md, m) for md, m in july_results if m[\"title\"] == \"WHEAT\"][0]\n",
    "\n",
    "print(f\"WHEAT: {wheat_meta['row_count']} data rows, {wheat_meta['col_count']} cols\")\n",
    "print()\n",
    "# Show header + first 5 data rows\n",
    "for line in wheat_md.split(\"\\n\")[:8]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7-md",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline: DOCX to Structured Records\n",
    "\n",
    "Run `interpret_table()` on the compressed markdown to extract structured records.\n",
    "Years are read dynamically from headers via `extract_pivot_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-schema",
   "metadata": {},
   "outputs": [],
   "source": "from pdf_ocr import interpret_table, interpret_tables, CanonicalSchema, to_records, to_pandas\n\n# Dynamic year aliases from the actual headers\nyears = extract_pivot_values(wheat_md)\nprint(f\"Years in headers: {years}\")\nyear_aliases = years[-2:]  # last 2 years\nprint(f\"Using aliases: {year_aliases}\")\n\nharvest_schema = CanonicalSchema.from_dict({\n    \"description\": \"Harvest progress by region, metric, and year\",\n    \"columns\": [\n        {\"name\": \"crop\", \"type\": \"string\", \"aliases\": []},\n        {\"name\": \"region\", \"type\": \"string\", \"aliases\": [\"Region\"]},\n        {\"name\": \"metric\", \"type\": \"string\",\n         \"aliases\": [\"Area harvested\", \"collected\", \"Yield\"]},\n        {\"name\": \"year\", \"type\": \"int\", \"aliases\": year_aliases},\n        {\"name\": \"value\", \"type\": \"float\", \"aliases\": []},\n    ],\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-interpret",
   "metadata": {},
   "outputs": [],
   "source": "result = interpret_table(wheat_md, harvest_schema, model=\"openai/gpt-4o\")\n\ndf_wheat = to_pandas(result, harvest_schema)\ndf_wheat.insert(0, \"crop\", \"WHEAT\")\nprint(f\"WHEAT: {len(df_wheat)} records\")\ndf_wheat.head(12)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-all-july",
   "metadata": {},
   "outputs": [],
   "source": "import time, pandas as pd\n\n# All 4 July harvest tables — interpreted concurrently\ntexts = [md for md, _ in july_results]\ntitles = [meta[\"title\"] or \"Unknown\" for _, meta in july_results]\n\nt0 = time.perf_counter()\nmapped_tables = interpret_tables(texts, harvest_schema, model=\"openai/gpt-4o\")\nelapsed = time.perf_counter() - t0\n\n# Build combined DataFrame via to_pandas (typed columns, OCR coercion)\nframes = []\nfor title, mapped in zip(titles, mapped_tables):\n    df = to_pandas(mapped, harvest_schema)\n    df.insert(0, \"crop\", title)\n    print(f\"  {title}: {len(df)} records\")\n    frames.append(df)\n\ndf_july = pd.concat(frames, ignore_index=True)\nprint(f\"\\nTotal July harvest records: {len(df_july)}  ({elapsed:.1f}s concurrent)\")\ndf_july"
  },
  {
   "cell_type": "markdown",
   "id": "t75x8detqoh",
   "source": "### FROM → TO: Source Table vs Structured DataFrame\n\nSide-by-side comparison of the raw DOCX pipe-table (as the LLM sees it) and the normalized pandas output.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "saqhc0zzl1",
   "source": "from IPython.display import display, HTML\n\ndef _md_table_to_html(md_text: str, title: str) -> str:\n    \"\"\"Convert a pipe-table markdown string to an HTML table.\"\"\"\n    lines = [l.strip() for l in md_text.strip().split(\"\\n\") if l.strip()]\n    # Skip separator lines (e.g. |---|---|)\n    rows = [l for l in lines if not all(c in \"-| \" for c in l)]\n    html = f\"<b>{title}</b><table style='font-size:11px; border-collapse:collapse; margin:4px 0'>\"\n    for i, row in enumerate(rows):\n        cells = [c.strip() for c in row.strip(\"|\").split(\"|\")]\n        tag = \"th\" if i == 0 else \"td\"\n        style = \"border:1px solid #ccc; padding:2px 6px; white-space:nowrap\"\n        html += \"<tr>\" + \"\".join(f\"<{tag} style='{style}'>{c}</{tag}>\" for c in cells) + \"</tr>\"\n    html += \"</table>\"\n    return html\n\n# Show WHEAT: original pipe-table (FROM) alongside interpreted DataFrame (TO)\nsource_html = _md_table_to_html(wheat_md, \"FROM: Raw DOCX pipe-table (WHEAT)\")\ndf_display = df_july[df_july[\"crop\"] == \"WHEAT\"].head(15)\nto_html = f\"<b>TO: Normalized DataFrame (WHEAT, first 15 rows)</b>{df_display.to_html(index=False)}\"\n\ndisplay(HTML(\n    \"<div style='display:flex; gap:24px; align-items:flex-start'>\"\n    f\"<div>{source_html}</div>\"\n    f\"<div style='font-size:12px'>{to_html}</div>\"\n    \"</div>\"\n))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s8-md",
   "metadata": {},
   "source": [
    "## 8. September Harvest Tables\n",
    "\n",
    "The September file has 8 harvest tables. The same schema handles them without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-sep",
   "metadata": {},
   "outputs": [],
   "source": "sep_path = labels[\"Sep 2-3\"]\nsep_classes = classify_docx_tables(sep_path, ag_categories)\nsep_harvest = [c[\"index\"] for c in sep_classes if c[\"category\"] == \"harvest\"]\nsep_results = compress_docx_tables(sep_path, table_indices=sep_harvest)\n\nprint(f\"September: {len(sep_results)} harvest tables\")\nfor md, meta in sep_results:\n    print(f\"  Table {meta['table_index']}: {meta['title']!r} ({meta['row_count']} rows)\")\n\n# Interpret all 8 tables concurrently\nsep_texts = [md for md, _ in sep_results]\nsep_titles = [meta[\"title\"] or \"Unknown\" for _, meta in sep_results]\n\nt0 = time.perf_counter()\nsep_mapped = interpret_tables(sep_texts, harvest_schema, model=\"openai/gpt-4o\")\nelapsed = time.perf_counter() - t0\n\nsep_frames = []\nfor title, mapped in zip(sep_titles, sep_mapped):\n    df = to_pandas(mapped, harvest_schema)\n    df.insert(0, \"crop\", title)\n    sep_frames.append(df)\n\ndf_sep = pd.concat(sep_frames, ignore_index=True)\nprint(f\"\\nTotal September harvest records: {len(df_sep)}  ({elapsed:.1f}s concurrent)\")\ndf_sep"
  },
  {
   "cell_type": "markdown",
   "id": "s9-md",
   "metadata": {},
   "source": [
    "## 9. Compare to Expected Parquet\n",
    "\n",
    "Load reference parquet files from `inputs/docx/output/` to compare structure and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    harvest_pq = \"inputs/docx/output/ru_ag_min_raw_harvest_progress_20250724_08_56_15.parquet\"\n",
    "    planting_pq = \"inputs/docx/output/ru_ag_min_raw_planting_progress_20250509_17_11_25.parquet\"\n",
    "\n",
    "    df_harvest = pd.read_parquet(harvest_pq)\n",
    "    df_planting = pd.read_parquet(planting_pq)\n",
    "\n",
    "    print(\"=== Harvest reference ===\")\n",
    "    print(f\"Shape: {df_harvest.shape}\")\n",
    "    print(f\"Columns: {list(df_harvest.columns)}\")\n",
    "    print(df_harvest.head())\n",
    "\n",
    "    print(f\"\\n=== Planting reference ===\")\n",
    "    print(f\"Shape: {df_planting.shape}\")\n",
    "    print(f\"Columns: {list(df_planting.columns)}\")\n",
    "    print(df_planting.head())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"pandas not available. Install with: pip install pandas pyarrow\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Reference file not found: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}