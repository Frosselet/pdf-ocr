{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# DOCX Table Extraction Walkthrough\n",
    "\n",
    "This notebook demonstrates extracting structured tables from Word documents (.docx).\n",
    "\n",
    "Key capabilities:\n",
    "- **Merged cell handling**: python-docx returns duplicate `_tc` references for horizontally merged cells — the extractor deduplicates them\n",
    "- **Compound headers**: hierarchical headers (metric labels spanning 2+ columns) are stacked with \" / \" separators\n",
    "- **User-defined classification**: `classify_docx_tables()` accepts caller-supplied categories and keywords\n",
    "- **Dynamic pivot values**: `extract_pivot_values()` reads years from headers instead of hardcoding\n",
    "\n",
    "The pipeline: DOCX → extract raw grids → detect header rows (merge-based) → build compound headers → render pipe-table markdown → `interpret_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-md",
   "metadata": {},
   "source": [
    "## 1. Synthetic Files — The Agnostic API\n",
    "\n",
    "8 synthetic DOCX files in `inputs/docx/synthetic/` cover different table structures.\n",
    "No domain knowledge required — the API works on any DOCX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-list",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "synth_files = sorted(glob.glob(\"inputs/docx/synthetic/*.docx\"))\n",
    "print(f\"{len(synth_files)} synthetic DOCX files:\")\n",
    "for f in synth_files:\n",
    "    print(f\"  {f.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-extract",
   "metadata": {},
   "outputs": [],
   "source": "from docpact import extract_tables_from_docx, compress_docx_tables\n\n# Basic extraction from a flat table (no merges)\ntables = extract_tables_from_docx(\"inputs/docx/synthetic/flat.docx\")\nprint(f\"flat.docx: {len(tables)} table, {len(tables[0].column_names)} cols, {len(tables[0].data)} data rows\")\nprint(f\"Source format: {tables[0].source_format}\")"
  },
  {
   "cell_type": "markdown",
   "id": "s2-md",
   "metadata": {},
   "source": [
    "## 2. Merged Cells and Compound Headers\n",
    "\n",
    "When headers span multiple rows (horizontal merge for metric labels, vertical merge for label columns),\n",
    "`compress_docx_tables()` builds compound column names joined by \" / \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-hspan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hspan.docx: 2 header rows with horizontal spans\n",
    "# Row 0: Category | Revenue (span 2) | Cost (span 2)\n",
    "# Row 1:          | Q1 | Q2          | Q1 | Q2\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/hspan.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-deep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_hierarchy.docx: 3 header rows (Group / Sub / Year)\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/deep_hierarchy.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_hspan.docx: title row (\"QUARTERLY REPORT\") + hierarchical headers\n",
    "\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/title_hspan.docx\")\n",
    "md, meta = results[0]\n",
    "print(f\"Title: {meta['title']!r}\")\n",
    "print(f\"Rows: {meta['row_count']}, Cols: {meta['col_count']}\")\n",
    "print()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-md",
   "metadata": {},
   "source": [
    "## 3. The Merged Cell Bug Fix\n",
    "\n",
    "python-docx returns **duplicate `_tc` references** for horizontally merged cells. A cell with `gridSpan=2`\n",
    "appears twice in `row.cells`, both pointing to the same XML element. Without deduplication, headers shift right.\n",
    "\n",
    "The fix: track `id(cell._tc)` in a `seen_tcs` set and skip duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-grid",
   "metadata": {},
   "outputs": [],
   "source": "from docx import Document\nfrom docpact.docx_extractor import _build_grid_from_table, _detect_header_rows_from_merges\n\n# Show raw grid from hspan.docx — verify no duplicate text\ndoc = Document(\"inputs/docx/synthetic/hspan.docx\")\ngrid, _ = _build_grid_from_table(doc.tables[0])\nhc = _detect_header_rows_from_merges(doc.tables[0])\n\nprint(f\"Grid: {len(grid)} rows x {len(grid[0])} cols, {hc} header rows\")\nfor i, row in enumerate(grid):\n    tag = \"[header]\" if i < hc else \"[data]  \"\n    print(f\"  row {i} {tag}: {row}\")\n\nprint(f\"\\nRow 0 col 1 = {grid[0][1]!r}  (not duplicated)\")\nprint(f\"Row 0 col 2 = {grid[0][2]!r}  (empty span continuation)\")"
  },
  {
   "cell_type": "markdown",
   "id": "s4-md",
   "metadata": {},
   "source": [
    "## 4. Classification with User-Defined Categories\n",
    "\n",
    "`classify_docx_tables(path, categories)` matches header text against caller-supplied keywords.\n",
    "No hardcoded domain knowledge — you define your own categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-synth",
   "metadata": {},
   "outputs": [],
   "source": "from docpact import classify_docx_tables\n\n# multi_category.docx has 4 tables: shipping, HR, financial, inventory\ncategories = {\n    \"shipping\": [\"cargo\", \"port\", \"vessel\"],\n    \"hr\": [\"employee\", \"salary\", \"department\"],\n    \"finance\": [\"revenue\", \"expenses\"],\n    \"inventory\": [\"stock\", \"warehouse\"],\n}\n\nclasses = classify_docx_tables(\"inputs/docx/synthetic/multi_category.docx\", categories)\nfor c in classes:\n    print(f\"  Table {c['index']}: {c['category']:12s} title={c['title'] or '(none)':25s} {c['rows']}r x {c['cols']}c\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s4-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classification to filter and compress only financial tables\n",
    "finance_idx = [c[\"index\"] for c in classes if c[\"category\"] == \"finance\"]\n",
    "results = compress_docx_tables(\"inputs/docx/synthetic/multi_category.docx\", table_indices=finance_idx)\n",
    "\n",
    "for md, meta in results:\n",
    "    print(f\"Title: {meta['title']!r}\")\n",
    "    print(md)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-md",
   "metadata": {},
   "source": [
    "## 5. Dynamic Pivot Values\n",
    "\n",
    "`extract_pivot_values()` reads years from the compressed markdown headers.\n",
    "This avoids hardcoding years that change every release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5-pivot",
   "metadata": {},
   "outputs": [],
   "source": "from docpact import extract_pivot_values\n\n# Extract years from each synthetic file that has them\nfor name in [\"hspan\", \"title_hspan\", \"deep_hierarchy\", \"unicode\"]:\n    results = compress_docx_tables(f\"inputs/docx/synthetic/{name}.docx\")\n    md, _ = results[0]\n    years = extract_pivot_values(md)\n    print(f\"{name:20s} → years={years}  last 2: {years[-2:]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "s6-md",
   "metadata": {},
   "source": [
    "## 6. Real-World Example — Russian Agricultural Reports\n",
    "\n",
    "6 weekly DOCX reports from the Russian Ministry of Agriculture.\n",
    "Each contains multiple tables (export summaries, harvest progress, planting progress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCX_DIR = \"inputs/docx/input\"\n",
    "docx_files = sorted(glob.glob(f\"{DOCX_DIR}/*.docx\"))\n",
    "\n",
    "labels = {\n",
    "    \"Apr 21-22\": [f for f in docx_files if \"Apr 21\" in f][0],\n",
    "    \"Apr 25-26\": [f for f in docx_files if \"April 25\" in f][0],\n",
    "    \"May 9-10\": [f for f in docx_files if \"May\" in f][0],\n",
    "    \"Jun 20-21\": [f for f in docx_files if \"June\" in f][0],\n",
    "    \"Jul 11-12\": [f for f in docx_files if \"July\" in f][0],\n",
    "    \"Sep 2-3\": [f for f in docx_files if \"September\" in f][0],\n",
    "}\n",
    "\n",
    "print(f\"{len(docx_files)} DOCX files:\")\n",
    "for label, path in labels.items():\n",
    "    print(f\"  {label}: {path.split('/')[-1][:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-classify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain-specific categories for Russian agricultural reports\n",
    "ag_categories = {\n",
    "    \"harvest\": [\"area harvested\", \"yield\", \"collected\", \"bunker\", \"centner\",\n",
    "                \"harvested area\", \"crop harvested\"],\n",
    "    \"planting\": [\"spring crops\", \"moa target\", \"spring wheat\", \"spring barley\",\n",
    "                 \"sown area\", \"planting\", \"sowing\", \"planted\"],\n",
    "    \"export\": [\"export\", \"shipment\", \"ports\", \"fob\", \"vessel\", \"cargo\"],\n",
    "}\n",
    "\n",
    "for label, path in labels.items():\n",
    "    classes = classify_docx_tables(path, ag_categories)\n",
    "    counts = {}\n",
    "    for c in classes:\n",
    "        counts[c[\"category\"]] = counts.get(c[\"category\"], 0) + 1\n",
    "    print(f\"{label:12s} ({len(classes):2d} tables): {dict(sorted(counts.items()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the WHEAT table from the July file\n",
    "july_path = labels[\"Jul 11-12\"]\n",
    "july_classes = classify_docx_tables(july_path, ag_categories)\n",
    "harvest_idx = [c[\"index\"] for c in july_classes if c[\"category\"] == \"harvest\"]\n",
    "\n",
    "july_results = compress_docx_tables(july_path, table_indices=harvest_idx)\n",
    "wheat_md, wheat_meta = [(md, m) for md, m in july_results if m[\"title\"] == \"WHEAT\"][0]\n",
    "\n",
    "print(f\"WHEAT: {wheat_meta['row_count']} data rows, {wheat_meta['col_count']} cols\")\n",
    "print()\n",
    "# Show header + first 5 data rows\n",
    "for line in wheat_md.split(\"\\n\")[:8]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7-md",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline: DOCX to Structured Records\n",
    "\n",
    "Run `interpret_table()` on the compressed markdown to extract structured records.\n",
    "Years are read dynamically from headers via `extract_pivot_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-schema",
   "metadata": {},
   "outputs": [],
   "source": "from docpact import interpret_table, interpret_tables, CanonicalSchema, to_records, to_pandas\n\n# Dynamic year aliases from the actual headers\nyears = extract_pivot_values(wheat_md)\nprint(f\"Years in headers: {years}\")\nyear_aliases = years[-2:]  # last 2 years\nprint(f\"Using aliases: {year_aliases}\")\n\nharvest_schema = CanonicalSchema.from_dict({\n    \"description\": \"Harvest progress by region, metric, and year\",\n    \"columns\": [\n        {\"name\": \"crop\", \"type\": \"string\", \"aliases\": []},\n        {\"name\": \"region\", \"type\": \"string\", \"aliases\": [\"Region\"]},\n        {\"name\": \"metric\", \"type\": \"string\",\n         \"aliases\": [\"Area harvested\", \"collected\", \"Yield\"]},\n        {\"name\": \"year\", \"type\": \"int\", \"aliases\": year_aliases},\n        {\"name\": \"value\", \"type\": \"float\", \"aliases\": []},\n    ],\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-interpret",
   "metadata": {},
   "outputs": [],
   "source": "result = interpret_table(wheat_md, harvest_schema, model=\"openai/gpt-4o\")\n\ndf_wheat = to_pandas(result, harvest_schema)\ndf_wheat[\"crop\"] = \"WHEAT\"\nprint(f\"WHEAT: {len(df_wheat)} records\")\ndf_wheat.head(12)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-all-july",
   "metadata": {},
   "outputs": [],
   "source": "import time, pandas as pd\n\n# All 4 July harvest tables — interpreted concurrently\ntexts = [md for md, _ in july_results]\ntitles = [meta[\"title\"] or \"Unknown\" for _, meta in july_results]\n\nt0 = time.perf_counter()\nmapped_tables = interpret_tables(texts, harvest_schema, model=\"openai/gpt-4o\")\nelapsed = time.perf_counter() - t0\n\n# Build combined DataFrame via to_pandas (typed columns, OCR coercion)\nframes = []\nfor title, mapped in zip(titles, mapped_tables):\n    df = to_pandas(mapped, harvest_schema)\n    df[\"crop\"] = title\n    print(f\"  {title}: {len(df)} records\")\n    frames.append(df)\n\ndf_july = pd.concat(frames, ignore_index=True)\nprint(f\"\\nTotal July harvest records: {len(df_july)}  ({elapsed:.1f}s concurrent)\")\ndf_july"
  },
  {
   "cell_type": "markdown",
   "id": "t75x8detqoh",
   "source": "### FROM → TO: Source Table vs Structured DataFrame\n\nSide-by-side comparison of the raw DOCX pipe-table (as the LLM sees it) and the normalized pandas output.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "saqhc0zzl1",
   "source": "from IPython.display import display, HTML\n\ndef _md_table_to_html(md_text: str, title: str) -> str:\n    \"\"\"Convert a pipe-table markdown string to an HTML table.\"\"\"\n    lines = [l.strip() for l in md_text.strip().split(\"\\n\") if l.strip()]\n    # Skip separator lines (e.g. |---|---|)\n    rows = [l for l in lines if not all(c in \"-| \" for c in l)]\n    html = f\"<b>{title}</b><table style='font-size:11px; border-collapse:collapse; margin:4px 0'>\"\n    for i, row in enumerate(rows):\n        cells = [c.strip() for c in row.strip(\"|\").split(\"|\")]\n        tag = \"th\" if i == 0 else \"td\"\n        style = \"border:1px solid #ccc; padding:2px 6px; white-space:nowrap\"\n        html += \"<tr>\" + \"\".join(f\"<{tag} style='{style}'>{c}</{tag}>\" for c in cells) + \"</tr>\"\n    html += \"</table>\"\n    return html\n\n# Show WHEAT: original pipe-table (FROM) alongside interpreted DataFrame (TO)\nsource_html = _md_table_to_html(wheat_md, \"FROM: Raw DOCX pipe-table (WHEAT)\")\ndf_display = df_july[df_july[\"crop\"] == \"WHEAT\"].head(15)\nto_html = f\"<b>TO: Normalized DataFrame (WHEAT, first 15 rows)</b>{df_display.to_html(index=False)}\"\n\ndisplay(HTML(\n    \"<div style='display:flex; gap:24px; align-items:flex-start'>\"\n    f\"<div>{source_html}</div>\"\n    f\"<div style='font-size:12px'>{to_html}</div>\"\n    \"</div>\"\n))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s8-md",
   "metadata": {},
   "source": [
    "## 8. September Harvest Tables\n",
    "\n",
    "The September file has 8 harvest tables. The same schema handles them without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-sep",
   "metadata": {},
   "outputs": [],
   "source": "sep_path = labels[\"Sep 2-3\"]\nsep_classes = classify_docx_tables(sep_path, ag_categories)\nsep_harvest = [c[\"index\"] for c in sep_classes if c[\"category\"] == \"harvest\"]\nsep_results = compress_docx_tables(sep_path, table_indices=sep_harvest)\n\nprint(f\"September: {len(sep_results)} harvest tables\")\nfor md, meta in sep_results:\n    print(f\"  Table {meta['table_index']}: {meta['title']!r} ({meta['row_count']} rows)\")\n\n# Interpret all 8 tables concurrently\nsep_texts = [md for md, _ in sep_results]\nsep_titles = [meta[\"title\"] or \"Unknown\" for _, meta in sep_results]\n\nt0 = time.perf_counter()\nsep_mapped = interpret_tables(sep_texts, harvest_schema, model=\"openai/gpt-4o\")\nelapsed = time.perf_counter() - t0\n\nsep_frames = []\nfor title, mapped in zip(sep_titles, sep_mapped):\n    df = to_pandas(mapped, harvest_schema)\n    df[\"crop\"] = title\n    sep_frames.append(df)\n\ndf_sep = pd.concat(sep_frames, ignore_index=True)\nprint(f\"\\nTotal September harvest records: {len(df_sep)}  ({elapsed:.1f}s concurrent)\")\ndf_sep"
  },
  {
   "cell_type": "markdown",
   "id": "s9-md",
   "metadata": {},
   "source": [
    "## 9. Compare to Expected Parquet\n",
    "\n",
    "Load reference parquet files from `inputs/docx/output/` to compare structure and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    harvest_pq = \"inputs/docx/output/ru_ag_min_raw_harvest_progress_20250724_08_56_15.parquet\"\n",
    "    planting_pq = \"inputs/docx/output/ru_ag_min_raw_planting_progress_20250509_17_11_25.parquet\"\n",
    "\n",
    "    df_harvest = pd.read_parquet(harvest_pq)\n",
    "    df_planting = pd.read_parquet(planting_pq)\n",
    "\n",
    "    print(\"=== Harvest reference ===\")\n",
    "    print(f\"Shape: {df_harvest.shape}\")\n",
    "    print(f\"Columns: {list(df_harvest.columns)}\")\n",
    "    print(df_harvest.head())\n",
    "\n",
    "    print(f\"\\n=== Planting reference ===\")\n",
    "    print(f\"Shape: {df_planting.shape}\")\n",
    "    print(f\"Columns: {list(df_planting.columns)}\")\n",
    "    print(df_planting.head())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"pandas not available. Install with: pip install pandas pyarrow\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Reference file not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hm4b9q3vz0f",
   "source": "## 10. Data Contract Pipeline\n\nA generic 4-step pipeline driven by an inline **JSON data contract**.\n\n```\nctx = prepare(ctx)   # parse contract → schemas, categories, enrichment rules\nctx = fetch(ctx)     # classify & extract tables from DOCX\nctx = transform(ctx) # concurrent LLM interpretation + enrichment\nctx = save(ctx)      # write parquet outputs\n```\n\nAll pipeline logic is standard. The contract is the only differentiator:\nnew providers, new schemas, new outputs — change the contract, not the code.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t24wgnxzpkn",
   "source": "import re, time\nimport pandas as pd\nfrom pathlib import Path\nfrom docpact import (\n    classify_docx_tables, compress_docx_tables, extract_pivot_values,\n    interpret_tables, CanonicalSchema, ColumnDef, to_pandas,\n)\n\n# ── Data Contract ────────────────────────────────────────────────────────────\n# One dict per provider. The pipeline code below is 100% generic.\n\nCONTRACT = {\n    \"provider\": \"ru_ag_ministry\",\n    \"description\": \"Russian Ministry of Agriculture weekly grain reports\",\n    \"model\": \"openai/gpt-4o\",\n\n    # regex to pull report date from filename\n    # e.g. \"...EOW June 20-21 2025-1.docx\" → \"June 20-21\"\n    \"report_date_pattern\": r\"(?:EOW\\s+)?([A-Z][a-z]+ \\d+-\\d+)\",\n\n    # table classification rules (case-insensitive keyword match on headers)\n    # Scored matching: each table is assigned to the category with the most\n    # keyword hits. Title text is included in scoring, so export tables whose\n    # titles say \"export\" beat a single crop-name hit in their headers.\n    \"categories\": {\n        \"harvest\": {\n            \"keywords\": [\n                \"area harvested\", \"yield\", \"collected\", \"bunker\",\n                \"centner\", \"harvested area\", \"crop harvested\",\n            ],\n        },\n        \"planting\": {\n            \"keywords\": [\n                \"spring crops\", \"moa target\", \"spring wheat\", \"spring barley\",\n                \"sown area\", \"planting\", \"sowing\", \"planted\",\n                \"corn\", \"rice\", \"sunflower\", \"soya\", \"rape\", \"buckwheat\",\n            ],\n        },\n        \"export\": {\n            \"keywords\": [\"export\", \"shipment\", \"ports\", \"fob\", \"vessel\", \"cargo\"],\n        },\n    },\n\n    # output table definitions\n    # column source types:\n    #   (no \"source\")          → LLM-extracted via CanonicalSchema + aliases\n    #   \"source\": \"title\"      → filled from the DOCX table title\n    #   \"source\": \"report_date\"→ filled from filename-derived report date\n    #   \"source\": \"constant\"   → filled with a fixed \"value\"\n    #   \"dynamic_aliases\": \"pivot\" → Year aliases auto-detected from headers\n    #   \"filter\": \"latest\"     → keep only rows with max value in this column\n    #   \"filter\": \"earliest\"   → keep only rows with min value in this column\n    #   \"suffix\": \" text\"      → appended to enrichment value (e.g. report_date)\n    #   \"format\": \"lowercase\"  → string case transformation on output\n    \"outputs\": {\n        \"harvest\": {\n            \"category\": \"harvest\",\n            \"filename\": \"harvest.parquet\",\n            \"schema\": {\n                \"description\": \"Harvest progress by region, metric, and year\",\n                \"columns\": [\n                    {\"name\": \"Region\",      \"type\": \"string\", \"description\": \"Geographic region\",     \"aliases\": [\"Region\"]},\n                    {\"name\": \"Report_date\", \"type\": \"string\", \"description\": \"Reporting period label\", \"source\": \"report_date\"},\n                    {\"name\": \"Crop\",        \"type\": \"string\", \"description\": \"Crop name\",              \"source\": \"title\"},\n                    {\"name\": \"Campaign\",    \"type\": \"string\", \"description\": \"Campaign type\",          \"source\": \"constant\", \"value\": \"HARVESTING\"},\n                    {\"name\": \"Metric\",      \"type\": \"string\", \"description\": \"Measurement type\",       \"aliases\": [\"Area harvested\", \"collected\", \"Yield\"]},\n                    {\"name\": \"Value\",       \"type\": \"float\",  \"description\": \"Numeric value\"},\n                    {\"name\": \"Year\",        \"type\": \"int\",    \"description\": \"Reporting year\",          \"dynamic_aliases\": \"pivot\"},\n                ],\n            },\n        },\n        \"planting\": {\n            \"category\": \"planting\",\n            \"filename\": \"planting.parquet\",\n            \"schema\": {\n                \"description\": \"Planting/sowing progress by region and crop\",\n                \"columns\": [\n                    {\"name\": \"Region\",      \"type\": \"string\", \"description\": \"Geographic region\",                          \"aliases\": [\"Region\"]},\n                    {\"name\": \"Area\",        \"type\": \"float\",  \"description\": \"MoA target area — the 'MOA Target YYYY' column in the source table\",\n                                                                                                                           \"aliases\": [\"MOA Target\"]},\n                    {\"name\": \"Value\",       \"type\": \"float\",  \"description\": \"Actual sown area — the plain year column (e.g. '2025') without any prefix like 'MOA Target' or 'Final'\"},\n                    {\"name\": \"Crop\",        \"type\": \"string\", \"description\": \"Crop name — extracted from section headers or compound header prefixes (e.g. 'SUNFLOWER / ...' → crop is SUNFLOWER)\",\n                                                                                                                           \"format\": \"lowercase\"},\n                    {\"name\": \"Report_date\", \"type\": \"string\", \"description\": \"Reporting period label\",                     \"source\": \"report_date\", \"suffix\": \" interim report\"},\n                    {\"name\": \"Year\",        \"type\": \"int\",    \"description\": \"Reporting year\",                              \"dynamic_aliases\": \"pivot\", \"filter\": \"latest\"},\n                ],\n            },\n        },\n    },\n}\n\nprint(f\"Contract loaded: {CONTRACT['provider']}\")\nprint(f\"Outputs: {list(CONTRACT['outputs'].keys())}\")\nfor name, spec in CONTRACT[\"outputs\"].items():\n    cols = [c[\"name\"] for c in spec[\"schema\"][\"columns\"]]\n    print(f\"  {name}: {cols}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9ebbsvznj5d",
   "source": "# ── Generic Pipeline Functions ────────────────────────────────────────────────\n# These functions are contract-agnostic. Swap the CONTRACT dict above\n# and re-run to process a completely different provider / document type.\n\ndef prepare(ctx: dict) -> dict:\n    \"\"\"Parse data contract → schemas, categories, enrichment rules.\"\"\"\n    contract = ctx[\"contract\"]\n    ctx[\"model\"] = contract.get(\"model\", \"openai/gpt-4o\")\n    ctx[\"categories\"] = {\n        name: cat[\"keywords\"]\n        for name, cat in contract[\"categories\"].items()\n    }\n    ctx[\"output_dir\"] = Path(ctx.get(\"output_dir\", \"outputs\"))\n\n    # Extract report_date from filename\n    pattern = contract.get(\"report_date_pattern\", r\"([A-Z][a-z]+ \\d+-\\d+)\")\n    m = re.search(pattern, Path(ctx[\"input_path\"]).name)\n    ctx[\"report_date\"] = m.group(1) if m else \"\"\n\n    # Build CanonicalSchema per output (LLM-extracted columns only)\n    ctx[\"schemas\"] = {}\n    ctx[\"enrichment\"] = {}\n    for out_name, spec in contract[\"outputs\"].items():\n        llm_cols = []\n        enrich = {}\n        for col in spec[\"schema\"][\"columns\"]:\n            if \"source\" in col:\n                enrich[col[\"name\"]] = col\n            else:\n                llm_cols.append(ColumnDef(\n                    name=col[\"name\"],\n                    type=col.get(\"type\", \"string\"),\n                    description=col.get(\"description\", \"\"),\n                    aliases=col.get(\"aliases\", []),\n                ))\n        ctx[\"schemas\"][out_name] = CanonicalSchema(\n            description=spec[\"schema\"][\"description\"],\n            columns=llm_cols,\n        )\n        ctx[\"enrichment\"][out_name] = enrich\n\n    print(f\"  Contract: {contract['provider']}\")\n    print(f\"  Report date: {ctx['report_date']!r}\")\n    print(f\"  Outputs: {list(contract['outputs'].keys())}\")\n    return ctx\n\n\ndef fetch(ctx: dict) -> dict:\n    \"\"\"Classify and extract tables from the DOCX file.\"\"\"\n    path = ctx[\"input_path\"]\n    classes = classify_docx_tables(path, ctx[\"categories\"])\n    ctx[\"classifications\"] = classes\n\n    counts = {}\n    for c in classes:\n        counts[c[\"category\"]] = counts.get(c[\"category\"], 0) + 1\n    print(f\"  Classified {len(classes)} tables: {dict(sorted(counts.items()))}\")\n\n    ctx[\"tables\"] = {}\n    for out_name, spec in ctx[\"contract\"][\"outputs\"].items():\n        category = spec[\"category\"]\n        indices = [c[\"index\"] for c in classes if c[\"category\"] == category]\n        if indices:\n            ctx[\"tables\"][out_name] = compress_docx_tables(path, table_indices=indices)\n            print(f\"  {out_name}: {len(indices)} tables extracted\")\n\n    return ctx\n\n\ndef transform(ctx: dict) -> dict:\n    \"\"\"Interpret tables concurrently and apply enrichment.\"\"\"\n    ctx[\"dataframes\"] = {}\n\n    for out_name, results in ctx[\"tables\"].items():\n        schema = ctx[\"schemas\"][out_name]\n\n        # Resolve dynamic pivot aliases (Year columns)\n        pivot_vals = extract_pivot_values(results[0][0])\n        for col in schema.columns:\n            col_spec = next(\n                (c for c in ctx[\"contract\"][\"outputs\"][out_name][\"schema\"][\"columns\"]\n                 if c[\"name\"] == col.name), None\n            )\n            if col_spec and col_spec.get(\"dynamic_aliases\") == \"pivot\":\n                col.aliases = pivot_vals[-2:]\n\n        # Concurrent LLM interpretation\n        texts = [md for md, _ in results]\n        t0 = time.perf_counter()\n        mapped = interpret_tables(texts, schema, model=ctx[\"model\"])\n        elapsed = time.perf_counter() - t0\n\n        # Build DataFrames with enrichment\n        frames = []\n        for (md, meta), mapped_table in zip(results, mapped):\n            df = to_pandas(mapped_table, schema)\n            for col_name, spec in ctx[\"enrichment\"][out_name].items():\n                if spec[\"source\"] == \"title\":\n                    df[col_name] = meta.get(\"title\") or \"Unknown\"\n                elif spec[\"source\"] == \"report_date\":\n                    val = ctx[\"report_date\"]\n                    if \"suffix\" in spec:\n                        val = val + spec[\"suffix\"]\n                    df[col_name] = val\n                elif spec[\"source\"] == \"constant\":\n                    df[col_name] = spec[\"value\"]\n            frames.append(df)\n\n        # Reorder columns to match contract definition\n        col_order = [c[\"name\"] for c in ctx[\"contract\"][\"outputs\"][out_name][\"schema\"][\"columns\"]]\n        df = pd.concat(frames, ignore_index=True)[col_order]\n\n        # Apply column-level format transformations from contract\n        for col_spec in ctx[\"contract\"][\"outputs\"][out_name][\"schema\"][\"columns\"]:\n            fmt = col_spec.get(\"format\")\n            col_name = col_spec[\"name\"]\n            if not fmt or col_name not in df.columns:\n                continue\n            if fmt == \"lowercase\":\n                df[col_name] = df[col_name].astype(str).str.lower()\n            elif fmt == \"uppercase\":\n                df[col_name] = df[col_name].astype(str).str.upper()\n            elif fmt == \"titlecase\":\n                df[col_name] = df[col_name].astype(str).str.title()\n\n        # Apply column filters from contract\n        for col_spec in ctx[\"contract\"][\"outputs\"][out_name][\"schema\"][\"columns\"]:\n            filt = col_spec.get(\"filter\")\n            col_name = col_spec[\"name\"]\n            if not filt or filt == \"all\" or col_name not in df.columns:\n                continue\n            if filt == \"latest\":\n                df = df[df[col_name] == df[col_name].max()]\n            elif filt == \"earliest\":\n                df = df[df[col_name] == df[col_name].min()]\n\n        ctx[\"dataframes\"][out_name] = df\n        print(f\"  {out_name}: {len(df)} records ({elapsed:.1f}s)\")\n\n    return ctx\n\n\ndef save(ctx: dict) -> dict:\n    \"\"\"Write output DataFrames to Parquet.\"\"\"\n    ctx[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n    ctx[\"output_paths\"] = {}\n\n    for out_name, spec in ctx[\"contract\"][\"outputs\"].items():\n        if out_name not in ctx[\"dataframes\"]:\n            print(f\"  {out_name}: skipped (no tables for category '{spec['category']}')\")\n            continue\n        path = ctx[\"output_dir\"] / spec[\"filename\"]\n        ctx[\"dataframes\"][out_name].to_parquet(path, index=False)\n        ctx[\"output_paths\"][out_name] = path\n        print(f\"  {out_name}: {path} ({len(ctx['dataframes'][out_name])} rows)\")\n\n    return ctx\n\nprint(\"Pipeline functions defined: prepare → fetch → transform → save\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "oklvys8lpna",
   "source": "# Run the pipeline — only the ctx seed changes per document\nctx = {\n    \"contract\": CONTRACT,\n    \"input_path\": \"inputs/docx/input/2025-06-24_11-58-45.Russian weekly grain EOW June 20-21 2025-1.docx\",\n    \"output_dir\": \"outputs/june\",\n}\n\nprint(\"PREPARE\"); ctx = prepare(ctx)\nprint(\"\\nFETCH\");   ctx = fetch(ctx)\nprint(\"\\nTRANSFORM\"); ctx = transform(ctx)\nprint(\"\\nSAVE\");    ctx = save(ctx)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fsy9gpx965i",
   "source": "# Inspect outputs — schema must match reference parquet files\nfor name, df in ctx[\"dataframes\"].items():\n    print(f\"=== {name.upper()} ({len(df)} rows) ===\")\n    print(f\"Columns: {list(df.columns)}\")\n    print(f\"Dtypes:  {dict(df.dtypes)}\")\n    display(df.head(15))\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "msxhh1idnv",
   "source": "# Validate schema matches reference parquet files\nrefs = {\n    \"harvest\": \"inputs/docx/output/ru_ag_min_raw_harvest_progress_20250724_08_56_15.parquet\",\n    \"planting\": \"inputs/docx/output/ru_ag_min_raw_planting_progress_20250509_17_11_25.parquet\",\n}\n\nfor name, ref_path in refs.items():\n    ref = pd.read_parquet(ref_path)\n    if name in ctx[\"dataframes\"]:\n        out = ctx[\"dataframes\"][name]\n        cols_match = list(out.columns) == list(ref.columns)\n        print(f\"{name}: columns {'MATCH' if cols_match else 'MISMATCH'}\")\n        if not cols_match:\n            print(f\"  expected: {list(ref.columns)}\")\n            print(f\"  got:      {list(out.columns)}\")\n    else:\n        print(f\"{name}: not produced (June file has no {name} tables — expected)\")\n\nprint(\"\\nDone. Same contract, different document → different outputs, same schema.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}